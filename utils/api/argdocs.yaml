space.init.obs: |1+
    Observable of the space.

space.init.limits: |1+
    A tuple-like object of the limits of the space.
                These are the lower and upper limits.

space.init.binning: |1+
    Binning of the space.
                Currently, only regular and variable binning *with a name* is supported.
                If an integer or a list of integers is given with lengths equal to the number of observables,
                it is interpreted as the number of bins and
                a regular binning is automatically created using the limits as the
                start and end points.

space.init.name: |1+
    Human-readable name of the space.

model.init.name: |1+
    Human-readable name
                or label of
                the PDF for better identification.
                Has no programmatical functional purpose as identification.

pdf.init.name: |1+
    Human-readable name
                or label of
                the PDF for better identification.
                Has no programmatical functional purpose as identification.
model.init.obs: |1+
    Observables of the
                model. This will be used as the default space of the PDF and,
                if not given explicitly, as the normalization range.

                The default space is used for example in the sample method: if no
                sampling limits are given, the default space is used.

                The observables are not equal to the domain as it does not restrict or
                truncate the model outside this range.
binneddata.param.space: |1+
    Binned space of the data.
                The space is used to define the binning and the limits of the data.
binneddata.param.values: |1+
    Corresponds to the counts of the histogram.
                Follows the definition of the
                `Unified Histogram Interface (UHI) <https://uhi.readthedocs.io/en/latest/plotting.html#plotting>`_.

binneddata.param.variances: |1+
    Corresponds to the uncertainties of the histogram.
                If ``True``, the uncertainties are created assuming that ``values``
                have been drawn from a Poisson distribution. Follows the definition of the
                `Unified Histogram Interface (UHI) <https://uhi.readthedocs.io/en/latest/plotting.html#plotting>`_.

binnedpdf.pdf.x: |1+
    Values to evaluate the PDF at.
                If this is a ``ZfitBinnedData``-like object, a histogram of *densities*
                will be returned. If x is a ``ZfitUnbinnedData``-like object, the densities will be
                evaluated at the points of ``x``.

binnedpdf.out.problike: |1+
  If the input was unbinned, it returns an array
                of shape (nevents,). If the input was binned, the dimensions and ordering of
                the axes corresponds to the input axes.



pdf.init.obs: |1+
  Observables of the
                model. This will be used as the default space of the PDF and,
                if not given explicitly, as the normalization range.

                The default space is used for example in the sample method: if no
                sampling limits are given, the default space is used.

                The observables are not equal to the domain as it does not restrict or
                truncate the model outside this range.

pdf.init.norm: |1+
  Normalization of the PDF.
                By default, this is the same as the default space of the PDF.

pdf.pdf.norm: |1+
  Normalization of the function.
                By default, this is the ``norm`` of the PDF (which by default is the same as
                the space of the PDF).

pdf.param.norm: |1+
  Normalization of the function.
                By default, this is the ``norm`` of the PDF (which by default is the same as
                the space of the PDF). Should be ``ZfitSpace`` to define the space
                to normalize over.

pdf.param.x: |1+
  Data to evaluate the method on. Should be ``ZfitData``
                or a mapping of *obs* to numpy-like arrays.
                If an array is given, the first dimension is interpreted as the events while
                the second is meant to be the dimensionality of a single event.

pdf.init.extended: |1+
  The overall yield of the PDF.
                If this is parameter-like, it will be used as the yield,
                the expected number of events, and the PDF will be extended.
                An extended PDF has additional functionality, such as the
                ``ext_*`` methods and the ``counts`` (for binned PDFs).


pdf.init.extended.auto: |1+
                If ``True``,
                the PDF will be extended automatically if the PDF is extended
                using the total number of events in the histogram.
                This is the default.

pdf.integrate.limits: |1+
  Limits of the integration.

pdf.partial_integrate.limits: |1+
  Limits of the integration that will be integrated out.
                Has to be a subset of the PDFs observables.
pdf.integrate.norm: |1+
  Normalization of the integration.
                By default, this is the same as the default space of the PDF.
                ``False`` means no normalization and returns the unnormed integral.
pdf.integrate.options: |1+
  Options for the integration.
                Additional options for the integration. Currently supported options are:
                - type: one of (``bins``)
                  This hints that bins are integrated. A method that is vectorizable, non-dynamic and
                  therefore less suitable for complicated functions is chosen.

pdf.param.yield: |1+
  Yield (expected number of events) of the PDF.
                This is the expected number of events.
                If this is parameter-like, it will be used as the yield,
                the expected number of events, and the PDF will be extended.
                An extended PDF has additional functionality, such as the
                ``ext_*`` methods and the ``counts`` (for binned PDFs).

pdf.sample.n: |1+
  Number of samples to draw.
                For an extended PDF, the argument is optional and will be the
                poisson-fluctuated expected number of events, i.e. the yield.

pdf.sample.limits: |1+
  Limits of the sampling.
                By default, this is the same as the default space of the PDF.

pdf.binned.counts.x: |1+
  Data for the binned PDF.
                The returned counts correspond to the binned axis in ``x``.

pdf.binned.counts.norm: |1+
  Normalization of the counts.
                This normalizes the counts so that the actual sum of all counts is
                equal to the yield.

pdf.polynomial.init.coeff0: |1+
  Coefficient of the constant term.
                This is the coefficient of the constant term, i.e. the term
                :math:`x^0`. If None, set to 1.

pdf.polynomial.init.coeffs: |1+
  Coefficients of the sum of the polynomial.
                The coefficients of the polynomial, starting with the first order
                term. To set the constant term, use ``coeff0``.

pdf.polynomial.init.apply_scaling: |1+
  Rescale the data so that the actual limits represent (-1, 1).
                This is usually wanted as the polynomial is defined in this range.
                Default is ``True``.



pdf.kde.bandwidth.weights: |1+
  Weights of each event
                in *data*, can be None or Tensor-like with shape compatible
                with *data*. This will change the count of the events, whereas
                weight :math:`w_i` of :math:`x_i`.


pdf.kde.bandwidth.data: |1+
  Data points to determine the bandwidth
                from.

pdf.kde.init.data: |1+
  Data sample to approximate
              the density from. The points represent positions of the *kernel*,
              the :math:`x_i`. This is preferrably a ``ZfitData``, but can also
              be an array-like object.

              If the data has weights, they will be taken into account.
              This will change the count of the events, whereas
              weight :math:`w_i` of :math:`x_i` will scale the value of
              :math:`K_i( x_i)`, resulting in a factor of :math:`\frac{w_i}{\sum w_i} `.

              If no weights are given, each kernel will be scaled by the same
              constant :math:`\frac{1}{n_{data}}`.


pdf.kde.init.obs: |1+
  Observable space of the KDE.
              As with any other PDF, this will be used as the default *norm*, but
              does not define the domain of the PDF. Namely, this can be a smaller
              space than *data*, as long as the name of the observable match.
              Using a larger dataset is actually good practice avoiding
              bountary biases, see also :ref:`sec-boundary-bias-and-padding`.

pdf.kde.init.bandwidth: |1+
  Bandwidth of the kernel,
              often also denoted as :math:`h`. For a Gaussian kernel, this
              corresponds to *sigma*. This can be calculated using
              pre-defined options or by specifying a numerical value that is
              broadcastable to *data* -- a scalar or an array-like
              object with the same size as *data*.

              A scalar value is usually referred to as a global bandwidth while
              an array holds local bandwidths

pdf.kde.init.kernel: |1+
  The kernel is the heart
              of the Kernel Density Estimation, which consists of the sum of
              kernels around each sample point. Therefore, a kernel should represent
              the distribution probability of a single data point as close as
              possible.

              The most widespread kernel is a Gaussian, or Normal, distribution. Due
              to the law of large numbers, the sum of many (arbitrary) random variables
              -- this is the case for most real world observable as they are the result of
              multiple consecutive random effects -- results in a Gaussian distribution.
              However, there are many cases where this assumption is not per-se true. In
              this cases an alternative kernel may offer a better choice.

              Valid choices are callables that return a
              :py:class:`~tensorflow_probability.distribution.Distribution`, such as all distributions
              that belong to the loc-scale family.

pdf.kde.init.padding: |1+
  KDEs have a peculiar
              weakness: the boundaries, as the outside has a zero density. This makes the KDE
              go down at the bountary as well, as the density approaches zero, no matter what the
              density inside the boundary was.

              There are two ways to circumvent this problem:

                - the best solution: providing a larger dataset than the default space the PDF is used in
                - mirroring the existing data at the boundaries, which is equivalent to a boundary condition
                  with a zero derivative. This is a padding technique and can improve the boundaries.
                  However, one important drawback of this method is to keep in mind that this will actually
                  alter the PDF *to look mirrored*. If the PDF is plotted in a larger range, this becomes
                  clear.

              Possible options are a number (default 0.1) that depicts the fraction of the overall space
              that defines the data mirrored on both sides. For example, for a space from 0 to 5, a value of
              0.3 means that all data in the region of 0 to 1.5 is taken, mirrored around 0 as well as
              all data from 3.5 to 5 and mirrored at 5. The new data will go from -1.5 to 6.5, so the
              KDE is also having a shape outside the desired range. Using it only for the range 0 to 5
              hides this.
              Using a dict, each side separately (or only a single one) can be mirrored, like ``{'lowermirror: 0.1}``
              or ``{'lowermirror: 0.2, 'uppermirror': 0.1}``.
              For more control, a callable that takes data and weights can also be used.


pdf.kde.init.weights: |1+
  Weights of each event
              in *data*, can be None or Tensor-like with shape compatible
              with *data*. Instead of using this parameter, it is preferred
              to use a ``ZfitData`` as *data* that contains weights.
              This will change the count of the events, whereas
              weight :math:`w_i` of :math:`x_i` will scale the value of :math:`K_i( x_i)`,
              resulting in a factor of :math:`\frac{w_i}{\sum w_i} `.

              If no weights are given, each kernel will be scaled by the same
              constant :math:`\frac{1}{n_{data}}`.

pdf.kde.init.num_grid_points: |1+
  Number of points in
              the binning grid.

              The data will be binned using the *binning_method* in *num_grid_points*
              and this histogram grid will then be used as kernel points. This has the
              advantage to have a constant computational complexity independent of the data
              size.

              A number from 32 on can already yield good results, while the default is set
              to 1024, creating a fine grid. Lowering the number increases the performance
              at the cost of accuracy.

pdf.kde.init.binning_method: |1+
  Method to be used for
              binning the data. Options are 'linear', 'simple'.

              The data can be binned in the usual way ('simple'), but this is less precise
              for KDEs, where we are interested in the shape of the histogram and smoothing
              it. Therefore, a better suited method, 'linear', is available.

              In normal binnig, each event (or weight) falls into the bin within the bin edges,
              while the neighbouring bins get zero counts from this event.
              In linear binning, the event is split between two bins, proportional to its
              closeness to each bin.

              The 'linear' method provides superior performance, most notably in small (~32)
              grids.

pdf.kde.bandwidth.explain_global: |1+
  A global bandwidth
              is a single parameter that is shared amongst all kernels.
              While this is a fast and robust method,
              it is a rule of thumb approximation. Due to its global nature,
              it cannot take into account the different varying
              local densities. It uses notably the least amount of memory
              of all methods.

pdf.kde.bandwidth.explain_local: |1+
  A local bandwidth
              means that each kernel :math:`i` has a different bandwidth.
              In other words, given some data points with size n,
              we will need n bandwidth parameters.
              This is often more accurate than a global bandwidth,
              as it allows to have larger bandwiths in areas of smaller density,
              where, due to the small local sample size, we have less certainty
              over the true density while having a smaller bandwidth in denser
              populated areas.

              The accuracy comes at the cost of a longer pre-calculation to obtain
              the local bandwidth (there are different methods available), an
              increased runtime and - most importantly - a peak memory usage.

              This can be especially costly for a large number (> few thousand) of
              kernels and/or evaluating on large datasets (> 10'000).

pdf.kde.bandwidth.explain_adaptive: |1+
  Adaptive bandwidths are
              a way to reduce the dependence on the bandwidth parameter
              and are usually local bandwidths that take into account
              the local densities.
              Adaptive bandwidths are constructed by using an initial estimate
              of the local density in order to calculate a sensible bandwidth
              for each kernel. The initial estimator can be a kernel density
              estimation using a global bandwidth with a rule of thumb.
              The adaptive bandwidth h is obtained using this estimate, where
              usually

              .. math::

                h_{i} \propto f( x_{i} ) ^ {-1/2}

              Estimates can still differ in the overall scaling of this
              bandwidth.

minimizer.verbosity: |1+
 Verbosity of the minimizer. Has to be between 0 and 10.
               The verbosity has the meaning:

                - a value of 0 means quiet and no output
                - above 0 up to 5, information that is good to know but without
                  flooding the user, corresponding to a "INFO" level.
                - A value above 5 starts printing out considerably more and
                  is used more for debugging purposes.
                - Setting the verbosity to 10 will print out every
                  evaluation of the loss function and gradient.

                Some minimizers offer additional output which is also
                distributed as above but may duplicate certain printed values.

minimizer.tol: |1+
    Termination value for the
                    convergence/stopping criterion of the algorithm
                    in order to determine if the minimum has
                    been found. Defaults to 1e-3.
minimizer.criterion: |1+
    Criterion of the minimum. This is an
                    estimated measure for the distance to the
                    minimum and can include the relative
                    or absolute changes of the parameters,
                    function value, gradients and more.
                    If the value of the criterion is smaller
                    than ``loss.errordef * tol``, the algorithm
                    stopps and it is assumed that the minimum
                    has been found.
minimizer.strategy: |1+
    A class of type ``ZfitStrategy`` that takes no
                    input arguments in the init. Determines the behavior of the minimizer in
                    certain situations, most notably when encountering
                    NaNs. It can also implement a callback function.
minimizer.maxiter: |1+
    Approximate number of iterations.
                    This corresponds to roughly the maximum number of
                    evaluations of the ``value``, 'gradient`` or ``hessian``.
minimizer.name: |1+
    Human-readable name of the minimizer.
minimizer.maxcor: |1+
    Maximum number of memory history to keep
                    when using a quasi-Newton update formula such as BFGS.
                    It is the number of gradients
                    to “remember” from previous optimization
                    steps: increasing it increases
                    the memory requirements but may speed up the convergence.
minimizer.init.maxls: |1+
    Maximum number of linesearch points.

minimizer.scipy.gradient: |1+
    Define the method to use for the gradient computation
                    that the minimizer should use. This can be the
                    gradient provided by the loss itself or
                    method from the minimizer.
                    In general, using the zfit provided automatic gradient is
                    more precise and needs less computation time for the
                    evaluation compared to a numerical method, but it may not always be
                    possible. In this case, zfit switches to a generic, numerical gradient
                    which in general performs worse than if the minimizer has its own
                    numerical gradient.
                    The following are possible choices:

                    If set to ``False`` or ``'zfit'`` (or ``None``; default), the
                    gradient of the loss (usually the automatic gradient) will be used;
                    the minimizer won't use an internal algorithm.


minimizer.scipy.gradient.internal: |1+
    ``True`` tells the minimizer to use its default internal
                    gradient estimation. This can be specified more clearly using the
                    arguments ``'2-point'`` and ``'3-point'``, which specify the
                    numerical algorithm the minimizer should use in order to
                    estimate the gradient.
minimizer.scipy.hessian: |1+
    Define the method to use for the hessian computation
                    that the minimizer should use. This can be the
                    hessian provided by the loss itself or
                    method from the minimizer.

                    While the exact gradient can speed up the convergence and is
                    often beneficial, this ain't true for the computation of the
                    (inverse) Hessian matrix.
                    Due to the :math:`n^2` number of entries (compared to :math:`n` in the
                    gradient) from the :math:`n` parameters, this can grow quite
                    large and become computationally expensive.

                    Therefore, many algorithms use an approximated (inverse)
                    Hessian matrix making use of the gradient updates instead
                    of calculating the exact matrix. This turns out to be
                    precise enough and usually considerably speeds up the
                    convergence.

                    The following are possible choices:

                    If set to ``False`` or ``'zfit'``, the
                    hessian defined in the loss (usually using automatic differentiation)
                    will be used;
                    the minimizer won't use an internal algorithm.
minimizer.scipy.hessian.internal: |1+
    A :class:`~scipy.optimize.HessianUpdateStrategy` that holds
                    an approximation of the hessian. For example
                    :class:`~scipy.optimize.BFGS` (which performs usually best)
                    or :class:`~scipy.optimize.SR1`
                    (sometimes unstable updates).
                    ``True``  (or ``None``; default) tells the minimizer
                    to use its default internal
                    hessian approximation.
                    Arguments ``'2-point'`` and ``'3-point'`` specify which
                    numerical algorithm the minimizer should use in order to
                    estimate the hessian. This is only possible if the
                    gradient is provided by zfit and not an internal numerical
                    method is already used to determine it.

minimizer.scipy.info: |1+
    This implenemtation wraps the minimizers in
         `SciPy optimize <https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html>`_.
minimizer.trust.eta: |1+
    Trust region related acceptance
                    stringency for proposed steps.
minimizer.trust.init_trust_radius: |1+
    Initial trust-region radius.
minimizer.trust.max_trust_radius: |1+
    Maximum value of the trust-region radius.
                    No steps that are longer than this value will be proposed.

minimizer.nlopt.population: |1+
    The population size for the evolutionary algorithm.

minimizer.nlopt.info: |1+
    More information on the algorithm can be found
         `here <https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/>`_.

         This implenemtation uses internally the
         `NLopt library <https://nlopt.readthedocs.io/en/latest/>`_.
         It is a
         free/open-source library for nonlinear optimization,
         providing a common interface for a number of
         different free optimization routines available online as well as
         original implementations of various other algorithms.




loss.binned.init.model: |1+
   Binned PDF(s) that return the normalized probability
                (``rel_counts`` or ``counts``) for
                *data* under the given parameters.
                If multiple model and data are given, they will be used
                in the same order to do a simultaneous fit.

loss.binned.init.data: |1+
    Binned dataset that will be given to the *model*.
                If multiple model and data are given, they will be used
                in the same order to do a simultaneous fit.
loss.init.model: |1+
    PDFs that return the normalized probability for
                *data* under the given parameters.
                If multiple model and data are given, they will be used
                in the same order to do a simultaneous fit.

loss.init.data: |1+
    Dataset that will be given to the *model*.
                If multiple model and data are given, they will be used
                in the same order to do a simultaneous fit.

loss.init.constraints: |1+
    Auxiliary measurements ("constraints")
                that add a likelihood term to the loss.

                .. math::
                  \mathcal{L}(\theta) = \mathcal{L}_{unconstrained} \prod_{i} f_{constr_i}(\theta)

                Usually, an auxiliary measurement -- by its very nature -S  should only be added once
                to the loss. zfit does not automatically deduplicate constraints if they are given
                multiple times, leaving the freedom for arbitrary constructs.

                Constraints can also be used to restrict the loss by adding any kinds of penalties.

loss.init.explain.unbinnednll: |1+
    The unbinned log likelihood can be written as

         .. math::
             \mathcal{L}_{non-extended}(x | \theta) = \prod_{i} f_{\theta} (x_i)

         where :math:`x_i` is a single event from the dataset *data* and f is the *model*.
loss.init.explain.extendedterm: |1+
    The extended likelihood has an additional term

         .. math::
             \mathcal{L}_{extended term} = poiss(N_{tot}, N_{data})
             = N_{data}^{N_{tot}} \frac{e^{- N_{data}}}{N_{tot}!}

         and the extended likelihood is the product of both.
loss.init.explain.simultaneous: |1+
    A simultaneous fit can be performed by giving one or more ``model``, ``data``, to the loss. The
         length of each has to match the length of the others

         .. math::
             \mathcal{L}_{simultaneous}(\theta | {data_0, data_1, ..., data_n})
             = \prod_{i} \mathcal{L}(\theta_i, data_i)

         where :math:`\theta_i` is a set of parameters and
         a subset of :math:`\theta`


loss.init.explain.negativelog: |1+
    For optimization purposes, it is often easier
         to minimize a function and to use a log transformation. The actual loss is given by

         .. math::
              \mathcal{L} = - \sum_{i}^{n} ln(f(\theta|x_i))

         and therefore being called "negative log ..."

loss.init.explain.spdtransform: |1+
    A scaled Poisson distribution is
         used as described by Bohm and Zech, NIMA 748 (2014) 1-6 if the variance
         of the data is not ``None``. The scaling is forced to be >= 1 in order
         to avoid issues with empty bins.

loss.init.explain.weightednll: |1+
    If the dataset has weights, a weighted likelihood will be constructed instead

         .. math::
             \mathcal{L} = - \sum_{i}^{n} w_i \cdot ln(f(\theta|x_i))

         Note that this is not a real likelihood anymore! Calculating uncertainties
         can be done with hesse (as it has a correction) but will yield wrong
         results with profiling methods. The minimum is however fully valid.

loss.init.binned.explain.chi2zeros: |1+
    If the dataset has empty bins, the errors
         will be zero and :math:`\chi^2` is undefined. Two possibilities are available and
         can be given as an option:

         - "empty": "ignore" will ignore all bins with zero entries and won't count to the loss
         - "errors": "expected" will use the expected counts from the model
           with a Poissonian uncertainty

loss.init.options: |1+
    Additional options (as a dict) for the loss.
                Current possibilities include:

                - 'subtr_const' (default True): subtract from each points
                  log probability density a constant that
                  is approximately equal to the average log probability
                  density in the very first evaluation before
                  the summation. This brings the initial loss value closer to 0 and increases,
                  especially for large datasets, the numerical stability.

                  The value will be stored ith 'subtr_const_value' and can also be given
                  directly.

                  The subtraction should not affect the minimum as the absolute
                  value of the NLL is meaningless. However,
                  with this switch on, one cannot directly compare
                  different likelihoods absolute value as the constant
                  may differ! Use ``create_new`` in order to have a comparable likelihood
                  between different losses or use the ``full`` argument in the value function
                  to calculate the full loss with all constants.


                These settings may extend over time. In order to make sure that a loss is the
                same under the same data, make sure to use ``create_new`` instead of instantiating
                a new loss as the former will automatically overtake any relevant constants
                and behavior.


loss.value.full: |1+
    If True, return the full loss value, otherwise
                allow for the removal of constants and only return
                the part that depends on the parameters. Constants
                don't matter for the task of optimization, but
                they can greatly help with the numerical stability of the loss function.

loss.args.numgrad: |1+
    If ``True``, calculate the numerical gradient/Hessian
                instead of using the automatic one. This is
                usually slower if called repeatedly but can
                be used if the automatic gradient fails (e.g. if
                the model is not differentiable, written not in znp.* etc).

result.init.loss: |1+
    The loss function that was minimized.
                Usually, but not necessary, contains
                also the pdf, data and constraints.
result.init.params: |1+
    Result of the fit where each
                :py:class:`~zfit.Parameter` key has the
                value from the minimum found by the minimizer.

result.init.minimizer: |1+
    Minimizer that was used to obtain this ``FitResult`` and will be used to
                    calculate certain errors. If the minimizer
                    is state-based (like "iminuit"), then this is a copy
                    and the state of other ``FitResults`` or of the *actual*
                    minimizer that performed the minimization
                    won't be altered.
result.init.valid: |1+
    Indicating whether the result is valid or not. This is the strongest
                    indication and serves as
                    the global flag. The reasons why a result may be
                    invalid can be arbitrary, including but not exclusive:

                    - parameter(s) at the limit
                    - maxiter reached without proper convergence
                    - the minimizer maybe even converged but it is known
                      that this is only a local minimum

                    To indicate the reason for the invalidity, pass a message.
result.init.edm: |1+
    The estimated distance to minimum
                    which is the criterion value at the minimum.
result.init.fmin: |1+
    Value of the function at the minimum.
result.init.criterion: |1+
    Criterion that was used during the minimization.
                    This determines the estimated distance to the
                    minimum (edm)
result.init.status: |1+
    A status code (if available) that describes
                    the minimization termination. 0 means a valid
                    termination.
result.init.converged: |1+
    Whether the fit has successfully converged or not.
                    The result itself can still be an invalid minimum
                    such as if the parameters are at or close
                    to the limits or in case another minimum is found.
result.init.message: |1+
    Human-readable message to indicate the reason
                    if the fitresult is not valid.
                    If the fit is valid, the message (should)
                    be an empty string (or None),
                    otherwise, it should denote the reason for the invalidity.
result.init.info: |1+
    Additional information (if available)
                    such as *number of gradient function calls* or the
                    original minimizer return message.
                    This is a relatively free field and _no single field_
                    in it is guaranteed to be stable.
                    Some recommended fields:

                    - *original*: contains the original returned object
                      by the minimizer used internally.
                    - *optimizer*: the actual instance of the wrapped
                      optimizer (if available)
result.init.approx: |1+
    Collection of approximations found during
                    the minimization process such as gradient and hessian.
result.init.niter: |1+
    Approximate number of iterations ~= number
                    of function evaluations ~= number of gradient evaluations.
                    This is an approximated value and the exact meaning
                    can differ between different minimizers.
result.init.evaluator: |1+
    Loss evaluator that was used during the
                    minimization and that may contain information
                    about the last evaluations of the gradient
                    etc. which can serve as approximations.

result.init.values: |1+
    Values of the parameters at the
                    found minimum.


hs3.explain: |1+
    HS3 is the `HEP Statistics Serialization Standard <https://github.com/hep-statistics-serialization-standard/hep-statistics-serialization-standard>`_.
                    It is a JSON/YAML-based serialization that is a
                    coordinated effort of the HEP community to standardize the serialization of statistical models. The standard
                    is still in development and is not yet finalized. This function is experimental and may change in the future.

hs3.layout.explain: |1+
  The keys in the HS3 format are:
                     - 'distributions': list of PDFs
                     - 'variables': list of variables, i.e. ``zfit.Space`` and ``zfit.Parameter`` (or more generally parameters)
                     - 'loss': list of losses
                     - 'data': list of data
                     - 'metadata': contains the version of the HS3 format and the
                       zfit version used to create the file
