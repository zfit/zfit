<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta charset="utf-8" />
    <title>tensorflow.python.ops.math_ops &#8212; zfit 0.3.3 documentation</title>
    <link rel="stylesheet" href="../../../../_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../../../_static/language_data.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="../../../../_static/js/jquery-1.11.0.min.js "></script>
<script type="text/javascript" src="../../../../_static/js/jquery-fix.js "></script>
<script type="text/javascript" src="../../../../_static/bootstrap-2.3.2/js/bootstrap.min.js "></script>
<script type="text/javascript" src="../../../../_static/bootstrap-sphinx.js "></script>

  </head><body>

  <div id="navbar" class="navbar navbar-fixed-top">
    <div class="navbar-inner">
      <div class="container">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>

        <a class="brand" href="../../../../index.html">
          zfit</a>
        <span class="navbar-text pull-left"><b>0.3.3</b></span>

        <div class="nav-collapse">
          <ul class="nav">
            <li class="divider-vertical"></li>
            
                <li><a href="../../../../getting_started.html">Getting started</a></li>
                <li><a href="../../../../space.html">Space</a></li>
                <li><a href="../../../../parameter.html">Parameter</a></li>
                <li><a href="../../../../model.html">Model</a></li>
                <li><a href="../../../../data.html">Data</a></li>
                <li><a href="../../../../loss.html">Loss</a></li>
                <li><a href="../../../../minimize.html">Minimize</a></li>
                <li><a href="../../../../API.html">API</a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../../../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../getting_started.html">Getting started with zfit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../downloading.html">Downloading and Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../space.html">Space, Observable and Range</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../parameter.html">Parameter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model.html">Building a model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data.html">Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../loss.html">Loss</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../minimize.html">Minimization</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../API.html">zfit API documentation</a></li>
</ul>
</ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
      </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body span12 content" role="main">
      
  <h1>Source code for tensorflow.python.ops.math_ops</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2015 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;Basic arithmetic operators.</span>

<span class="sd">See the [python/math_ops](python/math_ops) guide.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="k">import</span> <span class="n">xrange</span>  <span class="c1"># pylint: disable=redefined-builtin</span>

<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">common_shapes</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">constant_op</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">dtypes</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">graph_util</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">sparse_tensor</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">tensor_shape</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">array_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">gen_data_flow_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">gen_math_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">gen_nn_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">gen_sparse_ops</span>
<span class="c1"># go/tf-wildcard-import</span>
<span class="c1"># pylint: disable=wildcard-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops.gen_math_ops</span> <span class="k">import</span> <span class="o">*</span>
<span class="c1"># pylint: enable=wildcard-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.platform</span> <span class="k">import</span> <span class="n">tf_logging</span> <span class="k">as</span> <span class="n">logging</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">compat</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">deprecation</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">dispatch</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">nest</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util.tf_export</span> <span class="k">import</span> <span class="n">tf_export</span>

<span class="c1"># Aliases for some automatically-generated names.</span>
<span class="n">linspace</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">lin_space</span>

<span class="n">arg_max</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Use `tf.math.argmax` instead&quot;</span><span class="p">)(</span><span class="n">arg_max</span><span class="p">)</span>  <span class="c1"># pylint: disable=used-before-assignment</span>
<span class="n">arg_min</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Use `tf.math.argmin` instead&quot;</span><span class="p">)(</span><span class="n">arg_min</span><span class="p">)</span>  <span class="c1"># pylint: disable=used-before-assignment</span>
<span class="n">tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;arg_max&quot;</span><span class="p">])(</span><span class="n">arg_max</span><span class="p">)</span>
<span class="n">tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;arg_min&quot;</span><span class="p">])(</span><span class="n">arg_min</span><span class="p">)</span>

<span class="c1"># This is set by resource_variable_ops.py. It is included in this way since</span>
<span class="c1"># there is a circular dependency between math_ops and resource_variable_ops</span>
<span class="n">_resource_variable_type</span> <span class="o">=</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">_set_doc</span><span class="p">(</span><span class="n">doc</span><span class="p">):</span>

  <span class="k">def</span> <span class="nf">_decorator</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
    <span class="n">func</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">doc</span>
    <span class="k">return</span> <span class="n">func</span>

  <span class="k">return</span> <span class="n">_decorator</span>


<span class="c1"># pylint: disable=redefined-builtin</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.argmax&quot;</span><span class="p">,</span> <span class="s2">&quot;argmax&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_args</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Use the `axis` argument instead&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;dimension&quot;</span><span class="p">)</span>
<span class="nd">@_set_doc</span><span class="p">(</span>
    <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">arg_max</span><span class="o">.</span><span class="vm">__doc__</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;dimensions&quot;</span><span class="p">,</span> <span class="s2">&quot;axes&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span>
        <span class="s2">&quot;dimension&quot;</span><span class="p">,</span> <span class="s2">&quot;axis&quot;</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">argmax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span>
           <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
           <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
           <span class="n">dimension</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
           <span class="n">output_type</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">):</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span>
      <span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;dimension&quot;</span><span class="p">,</span> <span class="n">dimension</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">argmax_v2</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">output_type</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.argmax&quot;</span><span class="p">,</span> <span class="s2">&quot;argmax&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">def</span> <span class="nf">argmax_v2</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span>
              <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
              <span class="n">output_type</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
              <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the index with the largest value across axes of a tensor.</span>

<span class="sd">  Note that in case of ties the identity of the return value is not guaranteed.</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`. Must be one of the following types: `float32`, `float64`,</span>
<span class="sd">    `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`,</span>
<span class="sd">    `qint32`, `bfloat16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.</span>
<span class="sd">    axis: A `Tensor`. Must be one of the following types: `int32`, `int64`.</span>
<span class="sd">      int32 or int64, must be in the range `-rank(input), rank(input))`.</span>
<span class="sd">      Describes which axis of the input Tensor to reduce across. For vectors,</span>
<span class="sd">      use axis = 0.</span>
<span class="sd">    output_type: An optional `tf.DType` from: `tf.int32, tf.int64`.</span>
<span class="sd">      Defaults to `tf.int64`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` of type `output_type`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">arg_max</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="n">output_type</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.argmin&quot;</span><span class="p">,</span> <span class="s2">&quot;argmin&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_args</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Use the `axis` argument instead&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;dimension&quot;</span><span class="p">)</span>
<span class="nd">@_set_doc</span><span class="p">(</span>
    <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">arg_min</span><span class="o">.</span><span class="vm">__doc__</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;dimensions&quot;</span><span class="p">,</span> <span class="s2">&quot;axes&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span>
        <span class="s2">&quot;dimension&quot;</span><span class="p">,</span> <span class="s2">&quot;axis&quot;</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">argmin</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span>
           <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
           <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
           <span class="n">dimension</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
           <span class="n">output_type</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">):</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span>
      <span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;dimension&quot;</span><span class="p">,</span> <span class="n">dimension</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">argmin_v2</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">output_type</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.argmin&quot;</span><span class="p">,</span> <span class="s2">&quot;argmin&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">def</span> <span class="nf">argmin_v2</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span>
              <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
              <span class="n">output_type</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
              <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the index with the smallest value across axes of a tensor.</span>

<span class="sd">  Note that in case of ties the identity of the return value is not guaranteed.</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`. Must be one of the following types: `float32`, `float64`,</span>
<span class="sd">    `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`,</span>
<span class="sd">    `qint32`, `bfloat16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.</span>
<span class="sd">    axis: A `Tensor`. Must be one of the following types: `int32`, `int64`.</span>
<span class="sd">      int32 or int64, must be in the range `-rank(input), rank(input))`.</span>
<span class="sd">      Describes which axis of the input Tensor to reduce across. For vectors,</span>
<span class="sd">      use axis = 0.</span>
<span class="sd">    output_type: An optional `tf.DType` from: `tf.int32, tf.int64`.</span>
<span class="sd">      Defaults to `tf.int64`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` of type `output_type`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">arg_min</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="n">output_type</span><span class="p">)</span>


<span class="c1"># pylint: enable=redefined-builtin</span>


<span class="c1"># pylint: disable=anomalous-backslash-in-string,protected-access</span>
<span class="c1"># pylint: disable=g-docstring-has-escape</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.abs&quot;</span><span class="p">,</span> <span class="s2">&quot;abs&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">abs</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the absolute value of a tensor.</span>

<span class="sd">  Given a tensor `x` of complex numbers, this operation returns a tensor of type</span>
<span class="sd">  `float32` or `float64` that is the absolute value of each element in `x`. All</span>
<span class="sd">  elements in `x` must be complex numbers of the form \\(a + bj\\). The</span>
<span class="sd">  absolute value is computed as \\( \sqrt{a^2 + b^2}\\).  For example:</span>
<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[-2.25 + 4.75j], [-3.25 + 5.75j]])</span>
<span class="sd">  tf.abs(x)  # [5.25594902, 6.60492229]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor` or `SparseTensor` of type `float16`, `float32`, `float64`,</span>
<span class="sd">      `int32`, `int64`, `complex64` or `complex128`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` or `SparseTensor` the same size and type as `x` with absolute</span>
<span class="sd">      values.</span>
<span class="sd">    Note, for `complex64` or `complex128` input, the returned `Tensor` will be</span>
<span class="sd">      of type `float32` or `float64`, respectively.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Abs&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">complex_abs</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tout</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">real_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">_abs</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
<span class="c1"># pylint: enable=g-docstring-has-escape</span>


<span class="c1"># pylint: disable=redefined-builtin</span>
<span class="k">def</span> <span class="nf">_bucketize</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">boundaries</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">bucketize</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">boundaries</span><span class="o">=</span><span class="n">boundaries</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="c1"># pylint: enable=redefined-builtin</span>


<span class="k">class</span> <span class="nc">DivideDelegateWithName</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Use Python2/Python3 division delegation to implement divide for tensors.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Construct DivideDelegateWithName.</span>

<span class="sd">    Args:</span>
<span class="sd">      x: Tensor to use as left operand in operator overloads</span>
<span class="sd">      name: The name that is preferred for the op created.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>

  <span class="k">def</span> <span class="nf">__truediv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_truediv_python3</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__floordiv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">floordiv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__div__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_div_python2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.divide&quot;</span><span class="p">,</span> <span class="s2">&quot;divide&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">divide</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes Python style division of `x` by `y`.&quot;&quot;&quot;</span>

  <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># Cannot use tensors operator overload, because it has no way to track</span>
    <span class="c1"># override names. Use a dummy class to track the runtime division behavior</span>
    <span class="k">return</span> <span class="n">DivideDelegateWithName</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span> <span class="o">/</span> <span class="n">y</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">/</span> <span class="n">y</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.multiply&quot;</span><span class="p">,</span> <span class="s2">&quot;multiply&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">multiply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="n">multiply</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">mul</span><span class="o">.</span><span class="vm">__doc__</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;Multiply&quot;</span><span class="p">,</span> <span class="s2">&quot;`tf.multiply`&quot;</span><span class="p">)</span>


<span class="c1"># TODO(aselle): put deprecation in after another round of global code changes</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span>
    <span class="s2">&quot;2016-12-30&quot;</span><span class="p">,</span>
    <span class="s2">&quot;`tf.mul(x, y)` is deprecated, please use `tf.multiply(x, y)` or `x * y`&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_mul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="n">_mul</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">mul</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">+</span> <span class="p">(</span><span class="s2">&quot;&quot;</span> <span class="k">if</span> <span class="n">_mul</span><span class="o">.</span><span class="vm">__doc__</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">_mul</span><span class="o">.</span><span class="vm">__doc__</span><span class="p">))</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.subtract&quot;</span><span class="p">,</span> <span class="s2">&quot;subtract&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">subtract</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="n">subtract</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sub</span><span class="o">.</span><span class="vm">__doc__</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;`Sub`&quot;</span><span class="p">,</span> <span class="s2">&quot;`tf.subtract`&quot;</span><span class="p">)</span>


<span class="c1"># TODO(aselle): put deprecation in after another round of global code changes</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span>
    <span class="s2">&quot;2016-12-30&quot;</span><span class="p">,</span>
    <span class="s2">&quot;`tf.sub(x, y)` is deprecated, please use `tf.subtract(x, y)` or `x - y`&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_sub</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="n">_sub</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sub</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">+</span> <span class="p">(</span><span class="s2">&quot;&quot;</span> <span class="k">if</span> <span class="n">_sub</span><span class="o">.</span><span class="vm">__doc__</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">_sub</span><span class="o">.</span><span class="vm">__doc__</span><span class="p">))</span>


<span class="n">negative</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">neg</span>


<span class="c1"># pylint: disable=g-docstring-has-escape</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span>
    <span class="s2">&quot;2016-12-30&quot;</span><span class="p">,</span>
    <span class="s2">&quot;`tf.neg(x)` is deprecated, please use `tf.negative(x)` or `-x`&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_neg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes numerical negative value element-wise.</span>

<span class="sd">  I.e., \\(y = -x\\).</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor` or `SparseTensor`. Must be one of the following types: `half`,</span>
<span class="sd">      `float32`, `float64`, `int32`, `int64`, `complex64`, `complex128`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` or `SparseTensor`, respectively. Has the same type as `x`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">negative</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="c1"># pylint: enable=g-docstring-has-escape</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.scalar_mul&quot;</span><span class="p">,</span> <span class="s2">&quot;scalar_mul&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">scalar_mul</span><span class="p">(</span><span class="n">scalar</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Multiplies a scalar times a `Tensor` or `IndexedSlices` object.</span>

<span class="sd">  Intended for use in gradient code which might deal with `IndexedSlices`</span>
<span class="sd">  objects, which are easy to multiply by a scalar but more expensive to</span>
<span class="sd">  multiply with arbitrary tensors.</span>

<span class="sd">  Args:</span>
<span class="sd">    scalar: A 0-D scalar `Tensor`. Must have known shape.</span>
<span class="sd">    x: A `Tensor` or `IndexedSlices` to be scaled.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    `scalar * x` of the same type (`Tensor` or `IndexedSlices`) as `x`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: if scalar is not a 0-D `scalar`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">scalar</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
      <span class="n">scalar</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;scalar&quot;</span><span class="p">)</span>
  <span class="n">shape</span> <span class="o">=</span> <span class="n">scalar</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">(</span><span class="n">gen_math_ops</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">scalar</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">name</span><span class="p">),</span>
                               <span class="n">x</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">scalar</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Only scalar multiply works, got shape </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">shape</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.scalar_mul&quot;</span><span class="p">,</span> <span class="s2">&quot;scalar_mul&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="nd">@_set_doc</span><span class="p">(</span><span class="n">scalar_mul</span><span class="o">.</span><span class="vm">__doc__</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">scalar_mul_v2</span><span class="p">(</span><span class="n">scalar</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;scalar_mul&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">scalar_mul</span><span class="p">(</span><span class="n">scalar</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.pow&quot;</span><span class="p">,</span> <span class="s2">&quot;pow&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">pow</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the power of one value to another.</span>

<span class="sd">  Given a tensor `x` and a tensor `y`, this operation computes \\(x^y\\) for</span>
<span class="sd">  corresponding elements in `x` and `y`. For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[2, 2], [3, 3]])</span>
<span class="sd">  y = tf.constant([[8, 16], [2, 3]])</span>
<span class="sd">  tf.pow(x, y)  # [[256, 65536], [9, 27]]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,</span>
<span class="sd">     `complex64`, or `complex128`.</span>
<span class="sd">    y: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,</span>
<span class="sd">     `complex64`, or `complex128`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Pow&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">_pow</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="c1"># pylint: disable=redefined-builtin,redefined-outer-name</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;dtypes.complex&quot;</span><span class="p">,</span> <span class="s2">&quot;complex&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">complex</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">imag</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Converts two real numbers to a complex number.</span>

<span class="sd">  Given a tensor `real` representing the real part of a complex number, and a</span>
<span class="sd">  tensor `imag` representing the imaginary part of a complex number, this</span>
<span class="sd">  operation returns complex numbers elementwise of the form \\(a + bj\\), where</span>
<span class="sd">  *a* represents the `real` part and *b* represents the `imag` part.</span>

<span class="sd">  The input tensors `real` and `imag` must have the same shape.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  real = tf.constant([2.25, 3.25])</span>
<span class="sd">  imag = tf.constant([4.75, 5.75])</span>
<span class="sd">  tf.complex(real, imag)  # [[2.25 + 4.75j], [3.25 + 5.75j]]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    real: A `Tensor`. Must be one of the following types: `float32`,</span>
<span class="sd">      `float64`.</span>
<span class="sd">    imag: A `Tensor`. Must have the same type as `real`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` of type `complex64` or `complex128`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">real</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;real&quot;</span><span class="p">)</span>
  <span class="n">imag</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">imag</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;imag&quot;</span><span class="p">)</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Complex&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">real</span><span class="p">,</span> <span class="n">imag</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">input_types</span> <span class="o">=</span> <span class="p">(</span><span class="n">real</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">imag</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">input_types</span> <span class="o">==</span> <span class="p">(</span><span class="n">dtypes</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float64</span><span class="p">):</span>
      <span class="n">Tout</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">complex128</span>
    <span class="k">elif</span> <span class="n">input_types</span> <span class="o">==</span> <span class="p">(</span><span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
      <span class="n">Tout</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">complex64</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;real and imag have incorrect types: &quot;</span>
                      <span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">real</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">imag</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">_complex</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">imag</span><span class="p">,</span> <span class="n">Tout</span><span class="o">=</span><span class="n">Tout</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.real&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.real&quot;</span><span class="p">,</span> <span class="s2">&quot;real&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;real&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">real</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the real part of a complex (or real) tensor.</span>

<span class="sd">  Given a tensor `input`, this operation returns a tensor of type `float` that</span>
<span class="sd">  is the real part of each element in `input` considered as a complex number.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([-2.25 + 4.75j, 3.25 + 5.75j])</span>
<span class="sd">  tf.real(x)  # [-2.25, 3.25]</span>
<span class="sd">  ```</span>

<span class="sd">  If `input` is already real, it is returned unchanged.</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`. Must have numeric type.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` of type `float32` or `float64`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Real&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">:</span>
      <span class="n">real_dtype</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">real_dtype</span>
      <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tout</span><span class="o">=</span><span class="n">real_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">input</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.imag&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.imag&quot;</span><span class="p">,</span> <span class="s2">&quot;imag&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;imag&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">imag</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the imaginary part of a complex (or real) tensor.</span>

<span class="sd">  Given a tensor `input`, this operation returns a tensor of type `float` that</span>
<span class="sd">  is the imaginary part of each element in `input` considered as a complex</span>
<span class="sd">  number. If `input` is real, a tensor of all zeros is returned.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([-2.25 + 4.75j, 3.25 + 5.75j])</span>
<span class="sd">  tf.imag(x)  # [4.75, 5.75]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`. Must be one of the following types: `float`, `double`,</span>
<span class="sd">      `complex64`, `complex128`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` of type `float32` or `float64`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Imag&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">imag</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tout</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">real_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.angle&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.angle&quot;</span><span class="p">,</span> <span class="s2">&quot;angle&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;angle&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">angle</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the element-wise argument of a complex (or real) tensor.</span>

<span class="sd">  Given a tensor `input`, this operation returns a tensor of type `float` that</span>
<span class="sd">  is the argument of each element in `input` considered as a complex number.</span>

<span class="sd">  The elements in `input` are considered to be complex numbers of the form</span>
<span class="sd">  \\(a + bj\\), where *a* is the real part and *b* is the imaginary part.</span>
<span class="sd">  If `input` is real then *b* is zero by definition.</span>

<span class="sd">  The argument returned by this function is of the form \\(atan2(b, a)\\).</span>
<span class="sd">  If `input` is real, a tensor of all zeros is returned.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```</span>
<span class="sd">  # tensor &#39;input&#39; is [-2.25 + 4.75j, 3.25 + 5.75j]</span>
<span class="sd">  tf.angle(input) ==&gt; [2.0132, 1.056]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input: A `Tensor`. Must be one of the following types: `float`, `double`,</span>
<span class="sd">      `complex64`, `complex128`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` of type `float32` or `float64`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Angle&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">angle</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tout</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">real_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="c1"># pylint: enable=redefined-outer-name,redefined-builtin</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.round&quot;</span><span class="p">,</span> <span class="s2">&quot;round&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">round</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="sd">&quot;&quot;&quot;Rounds the values of a tensor to the nearest integer, element-wise.</span>

<span class="sd">  Rounds half to even.  Also known as bankers rounding. If you want to round</span>
<span class="sd">  according to the current system rounding mode use tf::cint.</span>
<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([0.9, 2.5, 2.3, 1.5, -4.5])</span>
<span class="sd">  tf.round(x)  # [ 1.0, 2.0, 2.0, 2.0, -4.0 ]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, or `int64`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` of same shape and type as `x`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_integer</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">x</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;dtypes.cast&quot;</span><span class="p">,</span> <span class="s2">&quot;cast&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Casts a tensor to a new type.</span>

<span class="sd">  The operation casts `x` (in case of `Tensor`) or `x.values`</span>
<span class="sd">  (in case of `SparseTensor` or `IndexedSlices`) to `dtype`.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([1.8, 2.2], dtype=tf.float32)</span>
<span class="sd">  tf.cast(x, tf.int32)  # [1, 2], dtype=tf.int32</span>
<span class="sd">  ```</span>

<span class="sd">  The operation supports data types (for `x` and `dtype`) of</span>
<span class="sd">  `uint8`, `uint16`, `uint32`, `uint64`, `int8`, `int16`, `int32`, `int64`,</span>
<span class="sd">  `float16`, `float32`, `float64`, `complex64`, `complex128`, `bfloat16`.</span>
<span class="sd">  In case of casting from complex types (`complex64`, `complex128`) to real</span>
<span class="sd">  types, only the real part of `x` is returned. In case of casting from real</span>
<span class="sd">  types to complex types (`complex64`, `complex128`), the imaginary part of the</span>
<span class="sd">  returned value is set to `0`. The handling of complex types here matches the</span>
<span class="sd">  behavior of numpy.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor` or `SparseTensor` or `IndexedSlices` of numeric type. It could</span>
<span class="sd">      be `uint8`, `uint16`, `uint32`, `uint64`, `int8`, `int16`, `int32`,</span>
<span class="sd">      `int64`, `float16`, `float32`, `float64`, `complex64`, `complex128`,</span>
<span class="sd">      `bfloat16`.</span>
<span class="sd">    dtype: The destination type. The list of supported dtypes is the same as</span>
<span class="sd">      `x`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` or `SparseTensor` or `IndexedSlices` with same shape as `x` and</span>
<span class="sd">      same type as `dtype`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If `x` cannot be cast to the `dtype`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">base_type</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">base_dtype</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span>
                <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">_resource_variable_type</span><span class="p">))</span> <span class="ow">and</span> <span class="n">base_type</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">x</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Cast&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">):</span>
      <span class="n">values_cast</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">base_type</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
      <span class="n">x</span> <span class="o">=</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">values_cast</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="n">values_cast</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">base_type</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
      <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">(</span><span class="n">values_cast</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># TODO(josh11b): If x is not already a Tensor, we could return</span>
      <span class="c1"># ops.convert_to_tensor(x, dtype=dtype, ...)  here, but that</span>
      <span class="c1"># allows some conversions that cast() can&#39;t do, e.g. casting numbers to</span>
      <span class="c1"># strings.</span>
      <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span> <span class="o">!=</span> <span class="n">base_type</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">base_type</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span> <span class="ow">and</span> <span class="n">base_type</span><span class="o">.</span><span class="n">is_floating</span><span class="p">:</span>
      <span class="n">logging</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Casting complex to real discards imaginary part.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;dtypes.saturate_cast&quot;</span><span class="p">,</span> <span class="s2">&quot;saturate_cast&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">saturate_cast</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Performs a safe saturating cast of `value` to `dtype`.</span>

<span class="sd">  This function casts the input to `dtype` without applying any scaling.  If</span>
<span class="sd">  there is a danger that values would over or underflow in the cast, this op</span>
<span class="sd">  applies the appropriate clamping before the cast.</span>

<span class="sd">  Args:</span>
<span class="sd">    value: A `Tensor`.</span>
<span class="sd">    dtype: The desired output `DType`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    `value` safely cast to `dtype`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># When casting to a type with smaller representable range, clamp.</span>
  <span class="c1"># Note that this covers casting to unsigned types as well.</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;saturate_cast&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">value</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;value&quot;</span><span class="p">)</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">base_dtype</span>
    <span class="k">if</span> <span class="n">value</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">min</span> <span class="o">&lt;</span> <span class="n">dtype</span><span class="o">.</span><span class="n">min</span><span class="p">:</span>
      <span class="n">value</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">value</span><span class="p">,</span>
                                   <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
                                       <span class="n">dtype</span><span class="o">.</span><span class="n">min</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">value</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                                       <span class="n">name</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">value</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">max</span> <span class="o">&gt;</span> <span class="n">dtype</span><span class="o">.</span><span class="n">max</span><span class="p">:</span>
      <span class="n">value</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">value</span><span class="p">,</span>
                                   <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
                                       <span class="n">dtype</span><span class="o">.</span><span class="n">max</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">value</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                                       <span class="n">name</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">cast</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span><span class="n">date</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">instructions</span><span class="o">=</span><span class="s2">&quot;Use tf.cast instead.&quot;</span><span class="p">)</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;to_float&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">to_float</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;ToFloat&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Casts a tensor to type `float32`.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor` or `SparseTensor` or `IndexedSlices`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` or `SparseTensor` or `IndexedSlices` with same shape as `x` with</span>
<span class="sd">    type `float32`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If `x` cannot be cast to the `float32`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span><span class="n">date</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">instructions</span><span class="o">=</span><span class="s2">&quot;Use tf.cast instead.&quot;</span><span class="p">)</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;to_double&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">to_double</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;ToDouble&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Casts a tensor to type `float64`.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor` or `SparseTensor` or `IndexedSlices`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` or `SparseTensor` or `IndexedSlices` with same shape as `x` with</span>
<span class="sd">    type `float64`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If `x` cannot be cast to the `float64`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span><span class="n">date</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">instructions</span><span class="o">=</span><span class="s2">&quot;Use tf.cast instead.&quot;</span><span class="p">)</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;to_int32&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">to_int32</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;ToInt32&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Casts a tensor to type `int32`.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor` or `SparseTensor` or `IndexedSlices`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` or `SparseTensor` or `IndexedSlices` with same shape as `x` with</span>
<span class="sd">    type `int32`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If `x` cannot be cast to the `int32`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span><span class="n">date</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">instructions</span><span class="o">=</span><span class="s2">&quot;Use tf.cast instead.&quot;</span><span class="p">)</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;to_int64&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">to_int64</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;ToInt64&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Casts a tensor to type `int64`.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor` or `SparseTensor` or `IndexedSlices`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` or `SparseTensor` or `IndexedSlices` with same shape as `x` with</span>
<span class="sd">    type `int64`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If `x` cannot be cast to the `int64`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span><span class="n">date</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">instructions</span><span class="o">=</span><span class="s2">&quot;Use tf.cast instead.&quot;</span><span class="p">)</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;to_bfloat16&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">to_bfloat16</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;ToBFloat16&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Casts a tensor to type `bfloat16`.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor` or `SparseTensor` or `IndexedSlices`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` or `SparseTensor` or `IndexedSlices` with same shape as `x` with</span>
<span class="sd">    type `bfloat16`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If `x` cannot be cast to the `bfloat16`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span><span class="n">date</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">instructions</span><span class="o">=</span><span class="s2">&quot;Use tf.cast instead.&quot;</span><span class="p">)</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;to_complex64&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">to_complex64</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;ToComplex64&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Casts a tensor to type `complex64`.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor` or `SparseTensor` or `IndexedSlices`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` or `SparseTensor` or `IndexedSlices` with same shape as `x` with</span>
<span class="sd">    type `complex64`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If `x` cannot be cast to the `complex64`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">complex64</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span><span class="n">date</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">instructions</span><span class="o">=</span><span class="s2">&quot;Use tf.cast instead.&quot;</span><span class="p">)</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;to_complex128&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">to_complex128</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;ToComplex128&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Casts a tensor to type `complex128`.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor` or `SparseTensor` or `IndexedSlices`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` or `SparseTensor` or `IndexedSlices` with same shape as `x` with</span>
<span class="sd">    type `complex128`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If `x` cannot be cast to the `complex128`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">complex128</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_override_operator</span><span class="p">(</span><span class="s2">&quot;__neg__&quot;</span><span class="p">,</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">neg</span><span class="p">)</span>
<span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_override_operator</span><span class="p">(</span><span class="s2">&quot;__abs__&quot;</span><span class="p">,</span> <span class="nb">abs</span><span class="p">)</span>
<span class="c1"># __invert__ corresponds to the ~ operator.  Here we follow the numpy convention</span>
<span class="c1"># ~ marks an elementwise bit-wise inverse.  This is only implemented for boolean</span>
<span class="c1"># tensors and will throw a TypeError if used on nonboolean arrays</span>
<span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_override_operator</span><span class="p">(</span><span class="s2">&quot;__invert__&quot;</span><span class="p">,</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">logical_not</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_OverrideBinaryOperatorHelper</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">op_name</span><span class="p">,</span> <span class="n">clazz_object</span><span class="o">=</span><span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Register operators with different tensor and scalar versions.</span>

<span class="sd">  If `clazz_object` is `SparseTensor`, assumes `func` takes `(sp_indices,</span>
<span class="sd">  sp_values, sp_shape, dense)` and outputs `(new_sp_values)`.</span>

<span class="sd">  Args:</span>
<span class="sd">    func: the operator</span>
<span class="sd">    op_name: name of the operator being overridden</span>
<span class="sd">    clazz_object: class to override for.  Either `Tensor` or `SparseTensor`.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">binary_op_wrapper</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">op_name</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
      <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
          <span class="n">y</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
          <span class="c1"># If the RHS is not a tensor, it might be a tensor aware object</span>
          <span class="c1"># that can implement the operator with knowledge of itself</span>
          <span class="c1"># and the tensor.</span>
          <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="s2">&quot;__r</span><span class="si">%s</span><span class="s2">__&quot;</span> <span class="o">%</span> <span class="n">op_name</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">NotImplemented</span>
          <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span>
      <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">binary_op_wrapper_sparse</span><span class="p">(</span><span class="n">sp_x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">op_name</span><span class="p">,</span> <span class="p">[</span><span class="n">sp_x</span><span class="p">,</span> <span class="n">y</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
      <span class="n">y</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sp_x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">(</span><span class="n">sp_x</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
                                        <span class="n">func</span><span class="p">(</span>
                                            <span class="n">sp_x</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
                                            <span class="n">sp_x</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                                            <span class="n">sp_x</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">,</span>
                                            <span class="n">y</span><span class="p">,</span>
                                            <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">),</span> <span class="n">sp_x</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">r_binary_op_wrapper</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">op_name</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
      <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

  <span class="c1"># Propagate func.__doc__ to the wrappers</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="vm">__doc__</span>
  <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="n">binary_op_wrapper</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">doc</span>
  <span class="n">r_binary_op_wrapper</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">doc</span>
  <span class="n">binary_op_wrapper_sparse</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">doc</span>

  <span class="k">if</span> <span class="n">clazz_object</span> <span class="ow">is</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">clazz_object</span><span class="o">.</span><span class="n">_override_operator</span><span class="p">(</span><span class="s2">&quot;__</span><span class="si">%s</span><span class="s2">__&quot;</span> <span class="o">%</span> <span class="n">op_name</span><span class="p">,</span> <span class="n">binary_op_wrapper</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">binary_op_wrapper</span>
    <span class="n">clazz_object</span><span class="o">.</span><span class="n">_override_operator</span><span class="p">(</span><span class="s2">&quot;__r</span><span class="si">%s</span><span class="s2">__&quot;</span> <span class="o">%</span> <span class="n">op_name</span><span class="p">,</span> <span class="n">r_binary_op_wrapper</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">r_binary_op_wrapper</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">clazz_object</span><span class="o">.</span><span class="n">_override_operator</span><span class="p">(</span><span class="s2">&quot;__</span><span class="si">%s</span><span class="s2">__&quot;</span> <span class="o">%</span> <span class="n">op_name</span><span class="p">,</span>
                                    <span class="n">binary_op_wrapper_sparse</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">binary_op_wrapper_sparse</span>


<span class="c1"># Conversion table for __truediv__.  None entries mean no conversion required.</span>
<span class="n">_TRUEDIV_TABLE</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">dtypes</span><span class="o">.</span><span class="n">uint8</span><span class="p">:</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
    <span class="n">dtypes</span><span class="o">.</span><span class="n">int8</span><span class="p">:</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
    <span class="n">dtypes</span><span class="o">.</span><span class="n">uint16</span><span class="p">:</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
    <span class="n">dtypes</span><span class="o">.</span><span class="n">int16</span><span class="p">:</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
    <span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
    <span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">:</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
    <span class="n">dtypes</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dtypes</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dtypes</span><span class="o">.</span><span class="n">float64</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dtypes</span><span class="o">.</span><span class="n">complex64</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dtypes</span><span class="o">.</span><span class="n">complex128</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">}</span>


<span class="c1"># NOTE: the support of &quot;sparse (true)div dense&quot; is currently not baked in into</span>
<span class="c1"># &quot;tf.(true_)div()&quot;.  Until such an API decision is made, the supported usage is</span>
<span class="c1"># to explicitly use the &quot;/&quot; operator to invoke either truediv or div.</span>
<span class="k">def</span> <span class="nf">_sparse_dense_truediv</span><span class="p">(</span><span class="n">sp_indices</span><span class="p">,</span> <span class="n">sp_values</span><span class="p">,</span> <span class="n">sp_shape</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Internal helper function for &#39;sp_t / dense_t&#39;.&quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;truediv&quot;</span><span class="p">,</span>
                      <span class="p">[</span><span class="n">sp_indices</span><span class="p">,</span> <span class="n">sp_values</span><span class="p">,</span> <span class="n">sp_shape</span><span class="p">,</span> <span class="n">y</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">sp_values</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">sp_values</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;sp_values&quot;</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
    <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">sp_values</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span>
    <span class="n">y_dtype</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span>
    <span class="k">if</span> <span class="n">x_dtype</span> <span class="o">!=</span> <span class="n">y_dtype</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;x and y must have the same dtype, got </span><span class="si">%r</span><span class="s2"> != </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span>
                      <span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">))</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">dtype</span> <span class="o">=</span> <span class="n">_TRUEDIV_TABLE</span><span class="p">[</span><span class="n">x_dtype</span><span class="p">]</span>
    <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Invalid dtype </span><span class="si">%r</span><span class="s2"> in __truediv__&quot;</span> <span class="o">%</span> <span class="n">x_dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">sp_values</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">sp_values</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
      <span class="n">y</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gen_sparse_ops</span><span class="o">.</span><span class="n">sparse_dense_cwise_div</span><span class="p">(</span>
        <span class="n">sp_indices</span><span class="p">,</span> <span class="n">sp_values</span><span class="p">,</span> <span class="n">sp_shape</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_truediv_python3</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;truediv&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
    <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span>
    <span class="n">y_dtype</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span>
    <span class="k">if</span> <span class="n">x_dtype</span> <span class="o">!=</span> <span class="n">y_dtype</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;x and y must have the same dtype, got </span><span class="si">%r</span><span class="s2"> != </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span>
                      <span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">))</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">dtype</span> <span class="o">=</span> <span class="n">_TRUEDIV_TABLE</span><span class="p">[</span><span class="n">x_dtype</span><span class="p">]</span>
    <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Invalid dtype </span><span class="si">%r</span><span class="s2"> in __truediv__&quot;</span> <span class="o">%</span> <span class="n">x_dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">x</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
      <span class="n">y</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">real_div</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_div_python2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Divide two values using Python 2 semantics. Used for Tensor.__div__.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: `Tensor` numerator of real numeric type.</span>
<span class="sd">    y: `Tensor` denominator of real numeric type.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">  Returns:</span>
<span class="sd">    `x / y` returns the quotient of x and y.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;div&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">)</span>
    <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span>
    <span class="n">y_dtype</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span>
    <span class="k">if</span> <span class="n">x_dtype</span> <span class="o">!=</span> <span class="n">y_dtype</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;x and y must have the same dtype, got </span><span class="si">%r</span><span class="s2"> != </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span>
                      <span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">x_dtype</span><span class="o">.</span><span class="n">is_floating</span> <span class="ow">or</span> <span class="n">x_dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">real_div</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">floor_div</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.truediv&quot;</span><span class="p">,</span> <span class="s2">&quot;truediv&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">truediv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Divides x / y elementwise (using Python 3 division operator semantics).</span>

<span class="sd">  NOTE: Prefer using the Tensor operator or tf.divide which obey Python</span>
<span class="sd">  division operator semantics.</span>

<span class="sd">  This function forces Python 3 division operator semantics where all integer</span>
<span class="sd">  arguments are cast to floating types first.   This op is generated by normal</span>
<span class="sd">  `x / y` division in Python 3 and in Python 2.7 with</span>
<span class="sd">  `from __future__ import division`.  If you want integer division that rounds</span>
<span class="sd">  down, use `x // y` or `tf.math.floordiv`.</span>

<span class="sd">  `x` and `y` must have the same numeric type.  If the inputs are floating</span>
<span class="sd">  point, the output will have the same type.  If the inputs are integral, the</span>
<span class="sd">  inputs are cast to `float32` for `int8` and `int16` and `float64` for `int32`</span>
<span class="sd">  and `int64` (matching the behavior of Numpy).</span>

<span class="sd">  Args:</span>
<span class="sd">    x: `Tensor` numerator of numeric type.</span>
<span class="sd">    y: `Tensor` denominator of numeric type.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    `x / y` evaluated in floating point.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If `x` and `y` have different dtypes.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">_truediv_python3</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span>
    <span class="n">date</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">instructions</span><span class="o">=</span><span class="s2">&quot;Deprecated in favor of operator or tf.math.divide.&quot;</span><span class="p">)</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;div&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">div</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Divides x / y elementwise (using Python 2 division operator semantics).</span>

<span class="sd">  NOTE: Prefer using the Tensor division operator or tf.divide which obey Python</span>
<span class="sd">  division operator semantics.</span>

<span class="sd">  This function divides `x` and `y`, forcing Python 2.7 semantics. That is,</span>
<span class="sd">  if one of `x` or `y` is a float, then the result will be a float.</span>
<span class="sd">  Otherwise, the output will be an integer type. Flooring semantics are used</span>
<span class="sd">  for integer division.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: `Tensor` numerator of real numeric type.</span>
<span class="sd">    y: `Tensor` denominator of real numeric type.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">  Returns:</span>
<span class="sd">    `x / y` returns the quotient of x and y.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">_div_python2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;div_no_nan&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">div_no_nan</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes an unsafe divide which returns 0 if the y is zero.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor`. Must be one of the following types: `float32`, `float64`.</span>
<span class="sd">    y: A `Tensor` whose dtype is compatible with `x`.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">  Returns:</span>
<span class="sd">    The element-wise value of the x divided by y.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;div_no_nan&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">)</span>
    <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span>
    <span class="n">y_dtype</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span>
    <span class="k">if</span> <span class="n">x_dtype</span> <span class="o">!=</span> <span class="n">y_dtype</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;x and y must have the same dtype, got </span><span class="si">%r</span><span class="s2"> != </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span>
                      <span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">div_no_nan</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="c1"># TODO(aselle): This should be removed</span>
<span class="n">mod</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">floor_mod</span>


<span class="c1"># TODO(aselle): Deprecate this once all internal functionality uses</span>
<span class="c1"># tf.truncatediv</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.floordiv&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.floordiv&quot;</span><span class="p">,</span> <span class="s2">&quot;floordiv&quot;</span><span class="p">])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;floordiv&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">floordiv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Divides `x / y` elementwise, rounding toward the most negative integer.</span>

<span class="sd">  The same as `tf.div(x,y)` for integers, but uses `tf.floor(tf.div(x,y))` for</span>
<span class="sd">  floating point arguments so that the result is always an integer (though</span>
<span class="sd">  possibly an integer represented as floating point).  This op is generated by</span>
<span class="sd">  `x // y` floor division in Python 3 and in Python 2.7 with</span>
<span class="sd">  `from __future__ import division`.</span>

<span class="sd">  `x` and `y` must have the same type, and the result will have the same type</span>
<span class="sd">  as well.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: `Tensor` numerator of real numeric type.</span>
<span class="sd">    y: `Tensor` denominator of real numeric type.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    `x / y` rounded down.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If the inputs are complex.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;floordiv&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">floor_div</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="n">realdiv</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">real_div</span>
<span class="n">truncatediv</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">truncate_div</span>
<span class="c1"># TODO(aselle): Rename this to floordiv when we can.</span>
<span class="n">floor_div</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">floor_div</span>
<span class="n">truncatemod</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">truncate_mod</span>
<span class="n">floormod</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">floor_mod</span>


<span class="k">def</span> <span class="nf">_mul_dispatch</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Dispatches cwise mul for &quot;Dense*Dense&quot; and &quot;Dense*Sparse&quot;.&quot;&quot;&quot;</span>
  <span class="n">is_tensor_y</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">is_tensor_y</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">)</span>  <span class="c1"># Case: Dense * Sparse.</span>
    <span class="n">new_vals</span> <span class="o">=</span> <span class="n">gen_sparse_ops</span><span class="o">.</span><span class="n">sparse_dense_cwise_mul</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                                                     <span class="n">y</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">new_vals</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">)</span>


<span class="c1"># NOTE(aselle): When integer division is added for sparse_dense_cwise,</span>
<span class="c1"># div, truediv, and floordiv should be delegated appropriately for</span>
<span class="c1"># Python sematnics, analogous to dense cwise tensor operations.</span>
<span class="n">_OverrideBinaryOperatorHelper</span><span class="p">(</span><span class="n">gen_sparse_ops</span><span class="o">.</span><span class="n">sparse_dense_cwise_div</span><span class="p">,</span> <span class="s2">&quot;div&quot;</span><span class="p">,</span>
                              <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">)</span>
<span class="n">_OverrideBinaryOperatorHelper</span><span class="p">(</span><span class="n">_sparse_dense_truediv</span><span class="p">,</span> <span class="s2">&quot;truediv&quot;</span><span class="p">,</span>
                              <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">)</span>
<span class="n">_OverrideBinaryOperatorHelper</span><span class="p">(</span><span class="n">gen_sparse_ops</span><span class="o">.</span><span class="n">sparse_dense_cwise_mul</span><span class="p">,</span> <span class="s2">&quot;mul&quot;</span><span class="p">,</span>
                              <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">)</span>

<span class="n">_OverrideBinaryOperatorHelper</span><span class="p">(</span><span class="n">gen_math_ops</span><span class="o">.</span><span class="n">add</span><span class="p">,</span> <span class="s2">&quot;add&quot;</span><span class="p">)</span>
<span class="n">_OverrideBinaryOperatorHelper</span><span class="p">(</span><span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sub</span><span class="p">,</span> <span class="s2">&quot;sub&quot;</span><span class="p">)</span>
<span class="n">_OverrideBinaryOperatorHelper</span><span class="p">(</span><span class="n">_mul_dispatch</span><span class="p">,</span> <span class="s2">&quot;mul&quot;</span><span class="p">)</span>
<span class="n">_OverrideBinaryOperatorHelper</span><span class="p">(</span><span class="n">_div_python2</span><span class="p">,</span> <span class="s2">&quot;div&quot;</span><span class="p">)</span>
<span class="n">_OverrideBinaryOperatorHelper</span><span class="p">(</span><span class="n">_truediv_python3</span><span class="p">,</span> <span class="s2">&quot;truediv&quot;</span><span class="p">)</span>
<span class="n">_OverrideBinaryOperatorHelper</span><span class="p">(</span><span class="n">floordiv</span><span class="p">,</span> <span class="s2">&quot;floordiv&quot;</span><span class="p">)</span>
<span class="n">_OverrideBinaryOperatorHelper</span><span class="p">(</span><span class="n">gen_math_ops</span><span class="o">.</span><span class="n">floor_mod</span><span class="p">,</span> <span class="s2">&quot;mod&quot;</span><span class="p">)</span>
<span class="n">_OverrideBinaryOperatorHelper</span><span class="p">(</span><span class="nb">pow</span><span class="p">,</span> <span class="s2">&quot;pow&quot;</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.logical_xor&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.logical_xor&quot;</span><span class="p">,</span> <span class="s2">&quot;logical_xor&quot;</span><span class="p">])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;logical_xor&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">logical_xor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;LogicalXor&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;x ^ y = (x | y) &amp; ~(x &amp; y).&quot;&quot;&quot;</span>
  <span class="c1"># TODO(alemi) Make this a cwise op if people end up relying on it.</span>
  <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span>
      <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span>
      <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">logical_not</span><span class="p">(</span><span class="n">gen_math_ops</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)),</span>
      <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="n">_OverrideBinaryOperatorHelper</span><span class="p">(</span><span class="n">gen_math_ops</span><span class="o">.</span><span class="n">logical_and</span><span class="p">,</span> <span class="s2">&quot;and&quot;</span><span class="p">)</span>
<span class="n">_OverrideBinaryOperatorHelper</span><span class="p">(</span><span class="n">gen_math_ops</span><span class="o">.</span><span class="n">logical_or</span><span class="p">,</span> <span class="s2">&quot;or&quot;</span><span class="p">)</span>
<span class="n">_OverrideBinaryOperatorHelper</span><span class="p">(</span><span class="n">logical_xor</span><span class="p">,</span> <span class="s2">&quot;xor&quot;</span><span class="p">)</span>

<span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_override_operator</span><span class="p">(</span><span class="s2">&quot;__lt__&quot;</span><span class="p">,</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">less</span><span class="p">)</span>
<span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_override_operator</span><span class="p">(</span><span class="s2">&quot;__le__&quot;</span><span class="p">,</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">less_equal</span><span class="p">)</span>
<span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_override_operator</span><span class="p">(</span><span class="s2">&quot;__gt__&quot;</span><span class="p">,</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">greater</span><span class="p">)</span>
<span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_override_operator</span><span class="p">(</span><span class="s2">&quot;__ge__&quot;</span><span class="p">,</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">greater_equal</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;range&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">range</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;range&quot;</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
  <span class="sd">&quot;&quot;&quot;Creates a sequence of numbers.</span>

<span class="sd">  Creates a sequence of numbers that begins at `start` and extends by</span>
<span class="sd">  increments of `delta` up to but not including `limit`.</span>

<span class="sd">  The dtype of the resulting tensor is inferred from the inputs unless</span>
<span class="sd">  it is provided explicitly.</span>

<span class="sd">  Like the Python builtin `range`, `start` defaults to 0, so that</span>
<span class="sd">  `range(n) = range(0, n)`.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  start = 3</span>
<span class="sd">  limit = 18</span>
<span class="sd">  delta = 3</span>
<span class="sd">  tf.range(start, limit, delta)  # [3, 6, 9, 12, 15]</span>

<span class="sd">  start = 3</span>
<span class="sd">  limit = 1</span>
<span class="sd">  delta = -0.5</span>
<span class="sd">  tf.range(start, limit, delta)  # [3, 2.5, 2, 1.5]</span>

<span class="sd">  limit = 5</span>
<span class="sd">  tf.range(limit)  # [0, 1, 2, 3, 4]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    start: A 0-D `Tensor` (scalar). Acts as first entry in the range if</span>
<span class="sd">      `limit` is not None; otherwise, acts as range limit and first entry</span>
<span class="sd">      defaults to 0.</span>
<span class="sd">    limit: A 0-D `Tensor` (scalar). Upper limit of sequence,</span>
<span class="sd">      exclusive. If None, defaults to the value of `start` while the first</span>
<span class="sd">      entry of the range defaults to 0.</span>
<span class="sd">    delta: A 0-D `Tensor` (scalar). Number that increments</span>
<span class="sd">      `start`. Defaults to 1.</span>
<span class="sd">    dtype: The type of the elements of the resulting tensor.</span>
<span class="sd">    name: A name for the operation. Defaults to &quot;range&quot;.</span>

<span class="sd">  Returns:</span>
<span class="sd">    An 1-D `Tensor` of type `dtype`.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.arange</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">limit</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">start</span><span class="p">,</span> <span class="n">limit</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">start</span>

  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Range&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">start</span><span class="p">,</span> <span class="n">limit</span><span class="p">,</span> <span class="n">delta</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;start&quot;</span><span class="p">)</span>
    <span class="n">limit</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">limit</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;limit&quot;</span><span class="p">)</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;delta&quot;</span><span class="p">)</span>

    <span class="c1"># infer dtype if not explicitly provided</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">dtype_hierarchy</span> <span class="o">=</span> <span class="p">[</span>
          <span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float64</span>
      <span class="p">]</span>
      <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">arg</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="n">dtype_hierarchy</span> <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="p">[</span><span class="n">start</span><span class="p">,</span> <span class="n">limit</span><span class="p">,</span> <span class="n">delta</span><span class="p">])</span>
      <span class="n">inferred_dtype</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span>
          <span class="p">[</span><span class="n">arg</span><span class="o">.</span><span class="n">dtype</span> <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="p">[</span><span class="n">start</span><span class="p">,</span> <span class="n">limit</span><span class="p">,</span> <span class="n">delta</span><span class="p">]],</span>
          <span class="n">key</span><span class="o">=</span><span class="n">dtype_hierarchy</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>

      <span class="n">start</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">inferred_dtype</span><span class="p">)</span>
      <span class="n">limit</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">limit</span><span class="p">,</span> <span class="n">inferred_dtype</span><span class="p">)</span>
      <span class="n">delta</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">inferred_dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">_range</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">limit</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="c1"># Reduction operations</span>
<span class="k">def</span> <span class="nf">_ReductionDims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">reduction_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="sd">&quot;&quot;&quot;Returns range(0, rank(x)) if reduction_indices is None.&quot;&quot;&quot;</span>
  <span class="c1"># TODO(aselle): Remove this after deprecation</span>
  <span class="k">if</span> <span class="n">reduction_indices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Can&#39;t specify both axis&#39; and &#39;reduction_indices&#39;.&quot;</span><span class="p">)</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">reduction_indices</span>
  <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">axis</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="c1"># Fast path: avoid creating Rank and Range ops if ndims is known.</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="n">common_shapes</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">constant_op</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">rank</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">)</span> <span class="ow">and</span>
        <span class="n">x</span><span class="o">.</span><span class="n">dense_shape</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">is_fully_defined</span><span class="p">()):</span>
      <span class="n">rank</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dense_shape</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">value</span>  <span class="c1"># sparse.dense_shape is 1-D.</span>
      <span class="k">return</span> <span class="n">constant_op</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">rank</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="c1"># Otherwise, we rely on Range and Rank to do the right thing at run-time.</span>
    <span class="k">return</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_may_reduce_to_scalar</span><span class="p">(</span><span class="n">keepdims</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Set a reduction&#39;s output shape to be a scalar if we are certain.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">common_shapes</span><span class="o">.</span><span class="n">has_fully_defined_shape</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="n">keepdims</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span>
      <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
    <span class="n">output</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(())</span>
  <span class="k">return</span> <span class="n">output</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.reduce_sum&quot;</span><span class="p">,</span> <span class="s2">&quot;reduce_sum&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_args</span><span class="p">(</span>
    <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;keep_dims is deprecated, use keepdims instead&quot;</span><span class="p">,</span> <span class="s2">&quot;keep_dims&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">reduce_sum_v1</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span>
                  <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">keepdims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">reduction_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">keep_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the sum of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[1, 1, 1], [1, 1, 1]])</span>
<span class="sd">  tf.reduce_sum(x)  # 6</span>
<span class="sd">  tf.reduce_sum(x, 0)  # [2, 2, 2]</span>
<span class="sd">  tf.reduce_sum(x, 1)  # [3, 3]</span>
<span class="sd">  tf.reduce_sum(x, 1, keepdims=True)  # [[3], [3]]</span>
<span class="sd">  tf.reduce_sum(x, [0, 1])  # 6</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The tensor to reduce. Should have numeric type.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default),</span>
<span class="sd">      reduces all dimensions. Must be in the range</span>
<span class="sd">      `[-rank(input_tensor), rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    reduction_indices: The old (deprecated) name for axis.</span>
<span class="sd">    keep_dims: Deprecated alias for `keepdims`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor, of the same dtype as the input_tensor.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.sum apart the fact that numpy upcast uint8 and int32 to</span>
<span class="sd">  int64 while tensorflow returns the same dtype as the input.</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span>
      <span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;reduction_indices&quot;</span><span class="p">,</span> <span class="n">reduction_indices</span><span class="p">)</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;keepdims&quot;</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span>
                                                    <span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">reduce_sum</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="reduce_sum"><a class="viewcode-back" href="../../../../api/zfit.ztf.wrapping_tf.html#zfit.ztf.wrapping_tf.reduce_sum">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.reduce_sum&quot;</span><span class="p">,</span> <span class="s2">&quot;reduce_sum&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">reduce_sum</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the sum of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[1, 1, 1], [1, 1, 1]])</span>
<span class="sd">  tf.reduce_sum(x)  # 6</span>
<span class="sd">  tf.reduce_sum(x, 0)  # [2, 2, 2]</span>
<span class="sd">  tf.reduce_sum(x, 1)  # [3, 3]</span>
<span class="sd">  tf.reduce_sum(x, 1, keepdims=True)  # [[3], [3]]</span>
<span class="sd">  tf.reduce_sum(x, [0, 1])  # 6</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The tensor to reduce. Should have numeric type.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor, of the same dtype as the input_tensor.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.sum apart the fact that numpy upcast uint8 and int32 to</span>
<span class="sd">  int64 while tensorflow returns the same dtype as the input.</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="kc">False</span> <span class="k">if</span> <span class="n">keepdims</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">keepdims</span>
  <span class="k">return</span> <span class="n">_may_reduce_to_scalar</span><span class="p">(</span>
      <span class="n">keepdims</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span>
      <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">_sum</span><span class="p">(</span>
          <span class="n">input_tensor</span><span class="p">,</span> <span class="n">_ReductionDims</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">),</span> <span class="n">keepdims</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.count_nonzero&quot;</span><span class="p">,</span> <span class="s2">&quot;count_nonzero&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_args</span><span class="p">(</span>
    <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;keep_dims is deprecated, use keepdims instead&quot;</span><span class="p">,</span> <span class="s2">&quot;keep_dims&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">count_nonzero</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span>
                  <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">keepdims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
                  <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">reduction_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">keep_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes number of nonzero elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` has no entries, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  **NOTE** Floating point comparison to zero is done by exact floating point</span>
<span class="sd">  equality check.  Small values are **not** rounded to zero for purposes of</span>
<span class="sd">  the nonzero check.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[0, 1, 0], [1, 1, 0]])</span>
<span class="sd">  tf.count_nonzero(x)  # 3</span>
<span class="sd">  tf.count_nonzero(x, 0)  # [1, 2, 0]</span>
<span class="sd">  tf.count_nonzero(x, 1)  # [1, 2]</span>
<span class="sd">  tf.count_nonzero(x, 1, keepdims=True)  # [[1], [2]]</span>
<span class="sd">  tf.count_nonzero(x, [0, 1])  # 3</span>
<span class="sd">  ```</span>

<span class="sd">  **NOTE** Strings are compared against zero-length empty string `&quot;&quot;`. Any</span>
<span class="sd">  string with a size greater than zero is already considered as nonzero.</span>

<span class="sd">  For example:</span>
<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([&quot;&quot;, &quot;a&quot;, &quot;  &quot;, &quot;b&quot;, &quot;&quot;])</span>
<span class="sd">  tf.count_nonzero(x) # 3, with &quot;a&quot;, &quot;  &quot;, and &quot;b&quot; as nonzero strings.</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The tensor to reduce. Should be of numeric type, `bool`,</span>
<span class="sd">      or `string`.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default),</span>
<span class="sd">      reduces all dimensions. Must be in the range</span>
<span class="sd">      `[-rank(input_tensor), rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    dtype: The output dtype; defaults to `tf.int64`.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    reduction_indices: The old (deprecated) name for axis.</span>
<span class="sd">    keep_dims: Deprecated alias for `keepdims`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor (number of nonzero values).</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;keepdims&quot;</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span>
                                                    <span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span>
      <span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span>
      <span class="s2">&quot;reduction_indices&quot;</span><span class="p">,</span> <span class="n">reduction_indices</span>
      <span class="p">)</span>

  <span class="k">return</span> <span class="n">count_nonzero_v2</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.count_nonzero&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">def</span> <span class="nf">count_nonzero_v2</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
                     <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                     <span class="n">keepdims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                     <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
                     <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes number of nonzero elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` has no entries, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  **NOTE** Floating point comparison to zero is done by exact floating point</span>
<span class="sd">  equality check.  Small values are **not** rounded to zero for purposes of</span>
<span class="sd">  the nonzero check.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[0, 1, 0], [1, 1, 0]])</span>
<span class="sd">  tf.count_nonzero(x)  # 3</span>
<span class="sd">  tf.count_nonzero(x, 0)  # [1, 2, 0]</span>
<span class="sd">  tf.count_nonzero(x, 1)  # [1, 2]</span>
<span class="sd">  tf.count_nonzero(x, 1, keepdims=True)  # [[1], [2]]</span>
<span class="sd">  tf.count_nonzero(x, [0, 1])  # 3</span>
<span class="sd">  ```</span>

<span class="sd">  **NOTE** Strings are compared against zero-length empty string `&quot;&quot;`. Any</span>
<span class="sd">  string with a size greater than zero is already considered as nonzero.</span>

<span class="sd">  For example:</span>
<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([&quot;&quot;, &quot;a&quot;, &quot;  &quot;, &quot;b&quot;, &quot;&quot;])</span>
<span class="sd">  tf.count_nonzero(x) # 3, with &quot;a&quot;, &quot;  &quot;, and &quot;b&quot; as nonzero strings.</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input: The tensor to reduce. Should be of numeric type, `bool`,</span>
<span class="sd">      or `string`.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default),</span>
<span class="sd">      reduces all dimensions. Must be in the range</span>
<span class="sd">      `[-rank(input), rank(input))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    dtype: The output dtype; defaults to `tf.int64`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor (number of nonzero values).</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">keepdims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">keepdims</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;count_nonzero&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">]):</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;input&quot;</span><span class="p">)</span>
    <span class="c1"># A scalar of &#39;zero&#39; is enough as `not_equal` will broadcast.</span>
    <span class="n">zero</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cast</span><span class="p">(</span>
        <span class="n">reduce_sum</span><span class="p">(</span>
            <span class="c1"># int64 reduction happens on GPU</span>
            <span class="n">cast</span><span class="p">(</span><span class="n">gen_math_ops</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">zero</span><span class="p">),</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
            <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span>
            <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.reduce_mean&quot;</span><span class="p">,</span> <span class="s2">&quot;reduce_mean&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">reduce_mean_v1</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span>
                   <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">keepdims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">reduction_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">keep_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the mean of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[1., 1.], [2., 2.]])</span>
<span class="sd">  tf.reduce_mean(x)  # 1.5</span>
<span class="sd">  tf.reduce_mean(x, 0)  # [1.5, 1.5]</span>
<span class="sd">  tf.reduce_mean(x, 1)  # [1.,  2.]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The tensor to reduce. Should have numeric type.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default),</span>
<span class="sd">      reduces all dimensions. Must be in the range</span>
<span class="sd">      `[-rank(input_tensor), rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    reduction_indices: The old (deprecated) name for axis.</span>
<span class="sd">    keep_dims: Deprecated alias for `keepdims`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.mean</span>

<span class="sd">  Please note that `np.mean` has a `dtype` parameter that could be used to</span>
<span class="sd">  specify the output type. By default this is `dtype=float64`. On the other</span>
<span class="sd">  hand, `tf.reduce_mean` has an aggressive type inference from `input_tensor`,</span>
<span class="sd">  for example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([1, 0, 1, 0])</span>
<span class="sd">  tf.reduce_mean(x)  # 0</span>
<span class="sd">  y = tf.constant([1., 0., 1., 0.])</span>
<span class="sd">  tf.reduce_mean(y)  # 0.5</span>
<span class="sd">  ```</span>

<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span>
      <span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;reduction_indices&quot;</span><span class="p">,</span> <span class="n">reduction_indices</span><span class="p">)</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;keepdims&quot;</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span>
                                                    <span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">reduce_mean</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.reduce_mean&quot;</span><span class="p">,</span> <span class="s2">&quot;reduce_mean&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">reduce_mean</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the mean of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[1., 1.], [2., 2.]])</span>
<span class="sd">  tf.reduce_mean(x)  # 1.5</span>
<span class="sd">  tf.reduce_mean(x, 0)  # [1.5, 1.5]</span>
<span class="sd">  tf.reduce_mean(x, 1)  # [1.,  2.]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The tensor to reduce. Should have numeric type.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.mean</span>

<span class="sd">  Please note that `np.mean` has a `dtype` parameter that could be used to</span>
<span class="sd">  specify the output type. By default this is `dtype=float64`. On the other</span>
<span class="sd">  hand, `tf.reduce_mean` has an aggressive type inference from `input_tensor`,</span>
<span class="sd">  for example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([1, 0, 1, 0])</span>
<span class="sd">  tf.reduce_mean(x)  # 0</span>
<span class="sd">  y = tf.constant([1., 0., 1., 0.])</span>
<span class="sd">  tf.reduce_mean(y)  # 0.5</span>
<span class="sd">  ```</span>

<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="kc">False</span> <span class="k">if</span> <span class="n">keepdims</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">keepdims</span>
  <span class="k">return</span> <span class="n">_may_reduce_to_scalar</span><span class="p">(</span>
      <span class="n">keepdims</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span>
      <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
          <span class="n">input_tensor</span><span class="p">,</span> <span class="n">_ReductionDims</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">),</span> <span class="n">keepdims</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.reduce_variance&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">reduce_variance</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the variance of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[1., 2.], [3., 4.]])</span>
<span class="sd">  tf.reduce_variance(x)  # 1.25</span>
<span class="sd">  tf.reduce_variance(x, 0)  # [1., 1.]</span>
<span class="sd">  tf.reduce_variance(x, 1)  # [0.25,  0.25]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The tensor to reduce. Should have numeric type.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name scope for the associated operations (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor, of the same dtype as the input_tensor.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.var</span>

<span class="sd">  Please note that `np.var` has a `dtype` parameter that could be used to</span>
<span class="sd">  specify the output type. By default this is `dtype=float64`. On the other</span>
<span class="sd">  hand, `tf.reduce_variance` has an aggressive type inference from</span>
<span class="sd">  `input_tensor`,</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">name</span> <span class="o">=</span> <span class="n">name</span> <span class="k">if</span> <span class="n">name</span> <span class="k">else</span> <span class="s2">&quot;reduce_variance&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">means</span> <span class="o">=</span> <span class="n">reduce_mean</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">squared_deviations</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">input_tensor</span> <span class="o">-</span> <span class="n">means</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">reduce_mean</span><span class="p">(</span><span class="n">squared_deviations</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.reduce_std&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">reduce_std</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the standard deviation of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[1., 2.], [3., 4.]])</span>
<span class="sd">  tf.reduce_std(x)  # 1.1180339887498949</span>
<span class="sd">  tf.reduce_std(x, 0)  # [1., 1.]</span>
<span class="sd">  tf.reduce_std(x, 1)  # [0.5,  0.5]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The tensor to reduce. Should have numeric type.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name scope for the associated operations (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor, of the same dtype as the input_tensor.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.std</span>

<span class="sd">  Please note that `np.std` has a `dtype` parameter that could be used to</span>
<span class="sd">  specify the output type. By default this is `dtype=float64`. On the other</span>
<span class="sd">  hand, `tf.reduce_std` has an aggressive type inference from `input_tensor`,</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">name</span> <span class="o">=</span> <span class="n">name</span> <span class="k">if</span> <span class="n">name</span> <span class="k">else</span> <span class="s2">&quot;reduce_std&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="n">reduce_variance</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">variance</span><span class="p">)</span>


<div class="viewcode-block" id="reduce_prod"><a class="viewcode-back" href="../../../../api/zfit.ztf.wrapping_tf.html#zfit.ztf.wrapping_tf.reduce_prod">[docs]</a><span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.reduce_prod&quot;</span><span class="p">,</span> <span class="s2">&quot;reduce_prod&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">reduce_prod</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the product of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The tensor to reduce. Should have numeric type.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default),</span>
<span class="sd">      reduces all dimensions. Must be in the range</span>
<span class="sd">      `[-rank(input_tensor), rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.prod</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="kc">False</span> <span class="k">if</span> <span class="n">keepdims</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">keepdims</span>
  <span class="k">return</span> <span class="n">_may_reduce_to_scalar</span><span class="p">(</span>
      <span class="n">keepdims</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span>
      <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span>
          <span class="n">input_tensor</span><span class="p">,</span> <span class="n">_ReductionDims</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">),</span> <span class="n">keepdims</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span></div>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.reduce_prod&quot;</span><span class="p">,</span> <span class="s2">&quot;reduce_prod&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_args</span><span class="p">(</span>
    <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;keep_dims is deprecated, use keepdims instead&quot;</span><span class="p">,</span> <span class="s2">&quot;keep_dims&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">reduce_prod_v1</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span>
                   <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">keepdims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">reduction_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">keep_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the product of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The tensor to reduce. Should have numeric type.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    reduction_indices: The old (deprecated) name for axis.</span>
<span class="sd">    keep_dims: Deprecated alias for `keepdims`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.prod</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span>
      <span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;reduction_indices&quot;</span><span class="p">,</span> <span class="n">reduction_indices</span><span class="p">)</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;keepdims&quot;</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span>
                                                    <span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">reduce_prod</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.reduce_min&quot;</span><span class="p">,</span> <span class="s2">&quot;reduce_min&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_args</span><span class="p">(</span>
    <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;keep_dims is deprecated, use keepdims instead&quot;</span><span class="p">,</span> <span class="s2">&quot;keep_dims&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">reduce_min_v1</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span>
                  <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">keepdims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">reduction_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">keep_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the minimum of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The tensor to reduce. Should have real numeric type.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    reduction_indices: The old (deprecated) name for axis.</span>
<span class="sd">    keep_dims: Deprecated alias for `keepdims`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.min</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span>
      <span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;reduction_indices&quot;</span><span class="p">,</span> <span class="n">reduction_indices</span><span class="p">)</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;keepdims&quot;</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span>
                                                    <span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">reduce_min</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.reduce_min&quot;</span><span class="p">,</span> <span class="s2">&quot;reduce_min&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">reduce_min</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the minimum of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The tensor to reduce. Should have real numeric type.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.min</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="kc">False</span> <span class="k">if</span> <span class="n">keepdims</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">keepdims</span>
  <span class="k">return</span> <span class="n">_may_reduce_to_scalar</span><span class="p">(</span>
      <span class="n">keepdims</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span>
      <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">_min</span><span class="p">(</span>
          <span class="n">input_tensor</span><span class="p">,</span> <span class="n">_ReductionDims</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">),</span> <span class="n">keepdims</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.reduce_max&quot;</span><span class="p">,</span> <span class="s2">&quot;reduce_max&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_args</span><span class="p">(</span>
    <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;keep_dims is deprecated, use keepdims instead&quot;</span><span class="p">,</span> <span class="s2">&quot;keep_dims&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">reduce_max_v1</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span>
                  <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">keepdims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">reduction_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">keep_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the maximum of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The tensor to reduce. Should have real numeric type.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default),</span>
<span class="sd">      reduces all dimensions. Must be in the range</span>
<span class="sd">      `[-rank(input_tensor), rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    reduction_indices: The old (deprecated) name for axis.</span>
<span class="sd">    keep_dims: Deprecated alias for `keepdims`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.max</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span>
      <span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;reduction_indices&quot;</span><span class="p">,</span> <span class="n">reduction_indices</span><span class="p">)</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;keepdims&quot;</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span>
                                                    <span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">reduce_max</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.reduce_max&quot;</span><span class="p">,</span> <span class="s2">&quot;reduce_max&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">reduce_max</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the maximum of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The tensor to reduce. Should have real numeric type.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.max</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="kc">False</span> <span class="k">if</span> <span class="n">keepdims</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">keepdims</span>
  <span class="k">return</span> <span class="n">_may_reduce_to_scalar</span><span class="p">(</span>
      <span class="n">keepdims</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span>
      <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">_max</span><span class="p">(</span>
          <span class="n">input_tensor</span><span class="p">,</span> <span class="n">_ReductionDims</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">),</span> <span class="n">keepdims</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.reduce_all&quot;</span><span class="p">,</span> <span class="s2">&quot;reduce_all&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_args</span><span class="p">(</span>
    <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;keep_dims is deprecated, use keepdims instead&quot;</span><span class="p">,</span> <span class="s2">&quot;keep_dims&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">reduce_all_v1</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span>
                  <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">keepdims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">reduction_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">keep_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the &quot;logical and&quot; of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[True,  True], [False, False]])</span>
<span class="sd">  tf.reduce_all(x)  # False</span>
<span class="sd">  tf.reduce_all(x, 0)  # [False, False]</span>
<span class="sd">  tf.reduce_all(x, 1)  # [True, False]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The boolean tensor to reduce.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    reduction_indices: The old (deprecated) name for axis.</span>
<span class="sd">    keep_dims: Deprecated alias for `keepdims`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.all</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span>
      <span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;reduction_indices&quot;</span><span class="p">,</span> <span class="n">reduction_indices</span><span class="p">)</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;keepdims&quot;</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span>
                                                    <span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">reduce_all</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;reduce_all&quot;</span><span class="p">,</span> <span class="s2">&quot;math.reduce_all&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">reduce_all</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the &quot;logical and&quot; of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[True,  True], [False, False]])</span>
<span class="sd">  tf.reduce_all(x)  # False</span>
<span class="sd">  tf.reduce_all(x, 0)  # [False, False]</span>
<span class="sd">  tf.reduce_all(x, 1)  # [True, False]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The boolean tensor to reduce.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.all</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="kc">False</span> <span class="k">if</span> <span class="n">keepdims</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">keepdims</span>
  <span class="k">return</span> <span class="n">_may_reduce_to_scalar</span><span class="p">(</span>
      <span class="n">keepdims</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span>
      <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">_all</span><span class="p">(</span>
          <span class="n">input_tensor</span><span class="p">,</span> <span class="n">_ReductionDims</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">),</span> <span class="n">keepdims</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.reduce_any&quot;</span><span class="p">,</span> <span class="s2">&quot;reduce_any&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_args</span><span class="p">(</span>
    <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;keep_dims is deprecated, use keepdims instead&quot;</span><span class="p">,</span> <span class="s2">&quot;keep_dims&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">reduce_any_v1</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span>
                  <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">keepdims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">reduction_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">keep_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the &quot;logical or&quot; of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[True,  True], [False, False]])</span>
<span class="sd">  tf.reduce_any(x)  # True</span>
<span class="sd">  tf.reduce_any(x, 0)  # [True, True]</span>
<span class="sd">  tf.reduce_any(x, 1)  # [True, False]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The boolean tensor to reduce.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    reduction_indices: The old (deprecated) name for axis.</span>
<span class="sd">    keep_dims: Deprecated alias for `keepdims`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.any</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span>
      <span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;reduction_indices&quot;</span><span class="p">,</span> <span class="n">reduction_indices</span><span class="p">)</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;keepdims&quot;</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span>
                                                    <span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">reduce_any</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.reduce_any&quot;</span><span class="p">,</span> <span class="s2">&quot;reduce_any&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">reduce_any</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the &quot;logical or&quot; of elements across dimensions of a tensor.</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` is None, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[True,  True], [False, False]])</span>
<span class="sd">  tf.reduce_any(x)  # True</span>
<span class="sd">  tf.reduce_any(x, 0)  # [True, True]</span>
<span class="sd">  tf.reduce_any(x, 1)  # [True, False]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The boolean tensor to reduce.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to np.any</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="kc">False</span> <span class="k">if</span> <span class="n">keepdims</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">keepdims</span>
  <span class="k">return</span> <span class="n">_may_reduce_to_scalar</span><span class="p">(</span>
      <span class="n">keepdims</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span>
      <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">_any</span><span class="p">(</span>
          <span class="n">input_tensor</span><span class="p">,</span> <span class="n">_ReductionDims</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">),</span> <span class="n">keepdims</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.reduce_logsumexp&quot;</span><span class="p">,</span> <span class="s2">&quot;reduce_logsumexp&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_args</span><span class="p">(</span>
    <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;keep_dims is deprecated, use keepdims instead&quot;</span><span class="p">,</span> <span class="s2">&quot;keep_dims&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">reduce_logsumexp_v1</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span>
                        <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">keepdims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">reduction_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">keep_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes log(sum(exp(elements across dimensions of a tensor))).</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` has no entries, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  This function is more numerically stable than log(sum(exp(input))). It avoids</span>
<span class="sd">  overflows caused by taking the exp of large inputs and underflows caused by</span>
<span class="sd">  taking the log of small inputs.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[0., 0., 0.], [0., 0., 0.]])</span>
<span class="sd">  tf.reduce_logsumexp(x)  # log(6)</span>
<span class="sd">  tf.reduce_logsumexp(x, 0)  # [log(2), log(2), log(2)]</span>
<span class="sd">  tf.reduce_logsumexp(x, 1)  # [log(3), log(3)]</span>
<span class="sd">  tf.reduce_logsumexp(x, 1, keepdims=True)  # [[log(3)], [log(3)]]</span>
<span class="sd">  tf.reduce_logsumexp(x, [0, 1])  # log(6)</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The tensor to reduce. Should have numeric type.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    reduction_indices: The old (deprecated) name for axis.</span>
<span class="sd">    keep_dims: Deprecated alias for `keepdims`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span>
      <span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;reduction_indices&quot;</span><span class="p">,</span> <span class="n">reduction_indices</span><span class="p">)</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated_argument_lookup</span><span class="p">(</span><span class="s2">&quot;keepdims&quot;</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span>
                                                    <span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">reduce_logsumexp</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.reduce_logsumexp&quot;</span><span class="p">,</span> <span class="s2">&quot;reduce_logsumexp&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">def</span> <span class="nf">reduce_logsumexp</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes log(sum(exp(elements across dimensions of a tensor))).</span>

<span class="sd">  Reduces `input_tensor` along the dimensions given in `axis`.</span>
<span class="sd">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span>
<span class="sd">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span>
<span class="sd">  are retained with length 1.</span>

<span class="sd">  If `axis` has no entries, all dimensions are reduced, and a</span>
<span class="sd">  tensor with a single element is returned.</span>

<span class="sd">  This function is more numerically stable than log(sum(exp(input))). It avoids</span>
<span class="sd">  overflows caused by taking the exp of large inputs and underflows caused by</span>
<span class="sd">  taking the log of small inputs.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[0., 0., 0.], [0., 0., 0.]])</span>
<span class="sd">  tf.reduce_logsumexp(x)  # log(6)</span>
<span class="sd">  tf.reduce_logsumexp(x, 0)  # [log(2), log(2), log(2)]</span>
<span class="sd">  tf.reduce_logsumexp(x, 1)  # [log(3), log(3)]</span>
<span class="sd">  tf.reduce_logsumexp(x, 1, keepdims=True)  # [[log(3)], [log(3)]]</span>
<span class="sd">  tf.reduce_logsumexp(x, [0, 1])  # log(6)</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    input_tensor: The tensor to reduce. Should have numeric type.</span>
<span class="sd">    axis: The dimensions to reduce. If `None` (the default), reduces all</span>
<span class="sd">      dimensions. Must be in the range `[-rank(input_tensor),</span>
<span class="sd">      rank(input_tensor))`.</span>
<span class="sd">    keepdims: If true, retains reduced dimensions with length 1.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The reduced tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">keepdims</span> <span class="o">=</span> <span class="kc">False</span> <span class="k">if</span> <span class="n">keepdims</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">keepdims</span>
  <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;ReduceLogSumExp&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">input_tensor</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">raw_max</span> <span class="o">=</span> <span class="n">reduce_max</span><span class="p">(</span>
        <span class="n">input_tensor</span><span class="p">,</span>
        <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span>
        <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">my_max</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span>
        <span class="n">array_ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
            <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">is_finite</span><span class="p">(</span><span class="n">raw_max</span><span class="p">),</span> <span class="n">raw_max</span><span class="p">,</span>
            <span class="n">array_ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">raw_max</span><span class="p">)))</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
        <span class="n">reduce_sum</span><span class="p">(</span>
            <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">my_max</span><span class="p">)),</span>
            <span class="n">axis</span><span class="p">,</span>
            <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">))</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">keepdims</span><span class="p">:</span>
      <span class="n">my_max</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">my_max</span><span class="p">,</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">result</span><span class="p">))</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">my_max</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_may_reduce_to_scalar</span><span class="p">(</span><span class="n">keepdims</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;linalg.trace&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;linalg.trace&quot;</span><span class="p">,</span> <span class="s2">&quot;trace&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;trace&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">trace</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Compute the trace of a tensor `x`.</span>

<span class="sd">  `trace(x)` returns the sum along the main diagonal of each inner-most matrix</span>
<span class="sd">  in x. If x is of rank `k` with shape `[I, J, K, ..., L, M, N]`, then output</span>
<span class="sd">  is a tensor of rank `k-2` with dimensions `[I, J, K, ..., L]` where</span>

<span class="sd">  `output[i, j, k, ..., l] = trace(x[i, j, i, ..., l, :, :])`</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = tf.constant([[1, 2], [3, 4]])</span>
<span class="sd">  tf.linalg.trace(x)  # 5</span>

<span class="sd">  x = tf.constant([[1, 2, 3],</span>
<span class="sd">                   [4, 5, 6],</span>
<span class="sd">                   [7, 8, 9]])</span>
<span class="sd">  tf.linalg.trace(x)  # 15</span>

<span class="sd">  x = tf.constant([[[1, 2, 3],</span>
<span class="sd">                    [4, 5, 6],</span>
<span class="sd">                    [7, 8, 9]],</span>
<span class="sd">                   [[-1, -2, -3],</span>
<span class="sd">                    [-4, -5, -6],</span>
<span class="sd">                    [-7, -8, -9]]])</span>
<span class="sd">  tf.linalg.trace(x)  # [15, -15]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    x: tensor.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The trace of input tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Trace&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">reduce_sum</span><span class="p">(</span><span class="n">array_ops</span><span class="o">.</span><span class="n">matrix_diag_part</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;linalg.matmul&quot;</span><span class="p">,</span> <span class="s2">&quot;matmul&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span>
           <span class="n">b</span><span class="p">,</span>
           <span class="n">transpose_a</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">transpose_b</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">adjoint_a</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">adjoint_b</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">a_is_sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">b_is_sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Multiplies matrix `a` by matrix `b`, producing `a` * `b`.</span>

<span class="sd">  The inputs must, following any transpositions, be tensors of rank &gt;= 2</span>
<span class="sd">  where the inner 2 dimensions specify valid matrix multiplication arguments,</span>
<span class="sd">  and any further outer dimensions match.</span>

<span class="sd">  Both matrices must be of the same type. The supported types are:</span>
<span class="sd">  `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.</span>

<span class="sd">  Either matrix can be transposed or adjointed (conjugated and transposed) on</span>
<span class="sd">  the fly by setting one of the corresponding flag to `True`. These are `False`</span>
<span class="sd">  by default.</span>

<span class="sd">  If one or both of the matrices contain a lot of zeros, a more efficient</span>
<span class="sd">  multiplication algorithm can be used by setting the corresponding</span>
<span class="sd">  `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.</span>
<span class="sd">  This optimization is only available for plain matrices (rank-2 tensors) with</span>
<span class="sd">  datatypes `bfloat16` or `float32`.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  # 2-D tensor `a`</span>
<span class="sd">  # [[1, 2, 3],</span>
<span class="sd">  #  [4, 5, 6]]</span>
<span class="sd">  a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])</span>

<span class="sd">  # 2-D tensor `b`</span>
<span class="sd">  # [[ 7,  8],</span>
<span class="sd">  #  [ 9, 10],</span>
<span class="sd">  #  [11, 12]]</span>
<span class="sd">  b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])</span>

<span class="sd">  # `a` * `b`</span>
<span class="sd">  # [[ 58,  64],</span>
<span class="sd">  #  [139, 154]]</span>
<span class="sd">  c = tf.matmul(a, b)</span>


<span class="sd">  # 3-D tensor `a`</span>
<span class="sd">  # [[[ 1,  2,  3],</span>
<span class="sd">  #   [ 4,  5,  6]],</span>
<span class="sd">  #  [[ 7,  8,  9],</span>
<span class="sd">  #   [10, 11, 12]]]</span>
<span class="sd">  a = tf.constant(np.arange(1, 13, dtype=np.int32),</span>
<span class="sd">                  shape=[2, 2, 3])</span>

<span class="sd">  # 3-D tensor `b`</span>
<span class="sd">  # [[[13, 14],</span>
<span class="sd">  #   [15, 16],</span>
<span class="sd">  #   [17, 18]],</span>
<span class="sd">  #  [[19, 20],</span>
<span class="sd">  #   [21, 22],</span>
<span class="sd">  #   [23, 24]]]</span>
<span class="sd">  b = tf.constant(np.arange(13, 25, dtype=np.int32),</span>
<span class="sd">                  shape=[2, 3, 2])</span>

<span class="sd">  # `a` * `b`</span>
<span class="sd">  # [[[ 94, 100],</span>
<span class="sd">  #   [229, 244]],</span>
<span class="sd">  #  [[508, 532],</span>
<span class="sd">  #   [697, 730]]]</span>
<span class="sd">  c = tf.matmul(a, b)</span>

<span class="sd">  # Since python &gt;= 3.5 the @ operator is supported (see PEP 465).</span>
<span class="sd">  # In TensorFlow, it simply calls the `tf.matmul()` function, so the</span>
<span class="sd">  # following lines are equivalent:</span>
<span class="sd">  d = a @ b @ [[10.], [11.]]</span>
<span class="sd">  d = tf.matmul(tf.matmul(a, b), [[10.], [11.]])</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    a: `Tensor` of type `float16`, `float32`, `float64`, `int32`, `complex64`,</span>
<span class="sd">      `complex128` and rank &gt; 1.</span>
<span class="sd">    b: `Tensor` with same type and rank as `a`.</span>
<span class="sd">    transpose_a: If `True`, `a` is transposed before multiplication.</span>
<span class="sd">    transpose_b: If `True`, `b` is transposed before multiplication.</span>
<span class="sd">    adjoint_a: If `True`, `a` is conjugated and transposed before</span>
<span class="sd">      multiplication.</span>
<span class="sd">    adjoint_b: If `True`, `b` is conjugated and transposed before</span>
<span class="sd">      multiplication.</span>
<span class="sd">    a_is_sparse: If `True`, `a` is treated as a sparse matrix.</span>
<span class="sd">    b_is_sparse: If `True`, `b` is treated as a sparse matrix.</span>
<span class="sd">    name: Name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` of the same type as `a` and `b` where each inner-most matrix is</span>
<span class="sd">    the product of the corresponding matrices in `a` and `b`, e.g. if all</span>
<span class="sd">    transpose or adjoint attributes are `False`:</span>

<span class="sd">    `output`[..., i, j] = sum_k (`a`[..., i, k] * `b`[..., k, j]),</span>
<span class="sd">    for all indices i, j.</span>

<span class="sd">    Note: This is matrix product, not element-wise product.</span>


<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If transpose_a and adjoint_a, or transpose_b and adjoint_b</span>
<span class="sd">      are both set to True.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;MatMul&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">transpose_a</span> <span class="ow">and</span> <span class="n">adjoint_a</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Only one of transpose_a and adjoint_a can be True.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">transpose_b</span> <span class="ow">and</span> <span class="n">adjoint_b</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Only one of transpose_b and adjoint_b can be True.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">EagerTensor</span><span class="p">,</span> <span class="n">_resource_variable_type</span><span class="p">)):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;a&quot;</span><span class="p">)</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">EagerTensor</span><span class="p">,</span> <span class="n">_resource_variable_type</span><span class="p">)):</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;b&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">a</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;a&quot;</span><span class="p">)</span>
      <span class="n">b</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;b&quot;</span><span class="p">)</span>

    <span class="c1"># TODO(apassos) remove _shape_tuple here when it is not needed.</span>
    <span class="n">a_shape</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">_shape_tuple</span><span class="p">()</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="n">b_shape</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">_shape_tuple</span><span class="p">()</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">a_is_sparse</span> <span class="ow">and</span>
        <span class="ow">not</span> <span class="n">b_is_sparse</span><span class="p">)</span> <span class="ow">and</span> <span class="p">((</span><span class="n">a_shape</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">a_shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">)</span> <span class="ow">and</span>
                              <span class="p">(</span><span class="n">b_shape</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">b_shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">)):</span>
      <span class="c1"># BatchMatmul does not support transpose, so we conjugate the matrix and</span>
      <span class="c1"># use adjoint instead. Conj() is a noop for real matrices.</span>
      <span class="k">if</span> <span class="n">transpose_a</span><span class="p">:</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">conj</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="n">adjoint_a</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="k">if</span> <span class="n">transpose_b</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">conj</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
        <span class="n">adjoint_b</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">batch_mat_mul</span><span class="p">(</span>
          <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">adj_x</span><span class="o">=</span><span class="n">adjoint_a</span><span class="p">,</span> <span class="n">adj_y</span><span class="o">=</span><span class="n">adjoint_b</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="c1"># Neither matmul nor sparse_matmul support adjoint, so we conjugate</span>
    <span class="c1"># the matrix and use transpose instead. Conj() is a noop for real</span>
    <span class="c1"># matrices.</span>
    <span class="k">if</span> <span class="n">adjoint_a</span><span class="p">:</span>
      <span class="n">a</span> <span class="o">=</span> <span class="n">conj</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
      <span class="n">transpose_a</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">if</span> <span class="n">adjoint_b</span><span class="p">:</span>
      <span class="n">b</span> <span class="o">=</span> <span class="n">conj</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
      <span class="n">transpose_b</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">use_sparse_matmul</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">a_is_sparse</span> <span class="ow">or</span> <span class="n">b_is_sparse</span><span class="p">:</span>
      <span class="n">sparse_matmul_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">dtypes</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
      <span class="n">use_sparse_matmul</span> <span class="o">=</span> <span class="p">(</span>
          <span class="n">a</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="n">sparse_matmul_types</span> <span class="ow">and</span> <span class="n">b</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="n">sparse_matmul_types</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">((</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">bfloat16</span> <span class="ow">or</span> <span class="n">b</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span> <span class="ow">and</span>
        <span class="n">a</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">b</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
      <span class="c1"># matmul currently doesn&#39;t handle mixed-precision inputs.</span>
      <span class="n">use_sparse_matmul</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">if</span> <span class="n">use_sparse_matmul</span><span class="p">:</span>
      <span class="n">ret</span> <span class="o">=</span> <span class="n">sparse_matmul</span><span class="p">(</span>
          <span class="n">a</span><span class="p">,</span>
          <span class="n">b</span><span class="p">,</span>
          <span class="n">transpose_a</span><span class="o">=</span><span class="n">transpose_a</span><span class="p">,</span>
          <span class="n">transpose_b</span><span class="o">=</span><span class="n">transpose_b</span><span class="p">,</span>
          <span class="n">a_is_sparse</span><span class="o">=</span><span class="n">a_is_sparse</span><span class="p">,</span>
          <span class="n">b_is_sparse</span><span class="o">=</span><span class="n">b_is_sparse</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
      <span class="c1"># sparse_matmul always returns float32, even with</span>
      <span class="c1"># bfloat16 inputs. This prevents us from configuring bfloat16 training.</span>
      <span class="c1"># casting to bfloat16 also matches non-sparse matmul behavior better.</span>
      <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">bfloat16</span> <span class="ow">and</span> <span class="n">b</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">ret</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">mat_mul</span><span class="p">(</span>
          <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">transpose_a</span><span class="o">=</span><span class="n">transpose_a</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="n">transpose_b</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;linalg.matvec&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">matvec</span><span class="p">(</span><span class="n">a</span><span class="p">,</span>
           <span class="n">b</span><span class="p">,</span>
           <span class="n">transpose_a</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">adjoint_a</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">a_is_sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">b_is_sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Multiplies matrix `a` by vector `b`, producing `a` * `b`.</span>

<span class="sd">  The matrix `a` must, following any transpositions, be a tensor of rank &gt;= 2,</span>
<span class="sd">  and we must have `shape(b) = shape(a)[:-2] + [shape(a)[-1]]`.</span>

<span class="sd">  Both `a` and `b` must be of the same type. The supported types are:</span>
<span class="sd">  `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.</span>

<span class="sd">  Matrix `a` can be transposed or adjointed (conjugated and transposed) on</span>
<span class="sd">  the fly by setting one of the corresponding flag to `True`. These are `False`</span>
<span class="sd">  by default.</span>

<span class="sd">  If one or both of the inputs contain a lot of zeros, a more efficient</span>
<span class="sd">  multiplication algorithm can be used by setting the corresponding</span>
<span class="sd">  `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.</span>
<span class="sd">  This optimization is only available for plain matrices/vectors (rank-2/1</span>
<span class="sd">  tensors) with datatypes `bfloat16` or `float32`.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  # 2-D tensor `a`</span>
<span class="sd">  # [[1, 2, 3],</span>
<span class="sd">  #  [4, 5, 6]]</span>
<span class="sd">  a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])</span>

<span class="sd">  # 1-D tensor `b`</span>
<span class="sd">  # [7, 9, 11]</span>
<span class="sd">  b = tf.constant([7, 9, 11], shape=[3])</span>

<span class="sd">  # `a` * `b`</span>
<span class="sd">  # [ 58,  64]</span>
<span class="sd">  c = tf.matvec(a, b)</span>


<span class="sd">  # 3-D tensor `a`</span>
<span class="sd">  # [[[ 1,  2,  3],</span>
<span class="sd">  #   [ 4,  5,  6]],</span>
<span class="sd">  #  [[ 7,  8,  9],</span>
<span class="sd">  #   [10, 11, 12]]]</span>
<span class="sd">  a = tf.constant(np.arange(1, 13, dtype=np.int32),</span>
<span class="sd">                  shape=[2, 2, 3])</span>

<span class="sd">  # 2-D tensor `b`</span>
<span class="sd">  # [[13, 14, 15],</span>
<span class="sd">  #  [16, 17, 18]]</span>
<span class="sd">  b = tf.constant(np.arange(13, 19, dtype=np.int32),</span>
<span class="sd">                  shape=[2, 3])</span>

<span class="sd">  # `a` * `b`</span>
<span class="sd">  # [[ 86, 212],</span>
<span class="sd">  #  [410, 563]]</span>
<span class="sd">  c = tf.matvec(a, b)</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    a: `Tensor` of type `float16`, `float32`, `float64`, `int32`, `complex64`,</span>
<span class="sd">      `complex128` and rank &gt; 1.</span>
<span class="sd">    b: `Tensor` with same type and rank = `rank(a) - 1`.</span>
<span class="sd">    transpose_a: If `True`, `a` is transposed before multiplication.</span>
<span class="sd">    adjoint_a: If `True`, `a` is conjugated and transposed before</span>
<span class="sd">      multiplication.</span>
<span class="sd">    a_is_sparse: If `True`, `a` is treated as a sparse matrix.</span>
<span class="sd">    b_is_sparse: If `True`, `b` is treated as a sparse matrix.</span>
<span class="sd">    name: Name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` of the same type as `a` and `b` where each inner-most vector is</span>
<span class="sd">    the product of the corresponding matrices in `a` and vectors in `b`, e.g. if</span>
<span class="sd">    all transpose or adjoint attributes are `False`:</span>

<span class="sd">    `output`[..., i] = sum_k (`a`[..., i, k] * `b`[..., k]), for all indices i.</span>

<span class="sd">    Note: This is matrix-vector product, not element-wise product.</span>


<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If transpose_a and adjoint_a are both set to True.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;MatVec&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">(</span>
        <span class="n">a</span><span class="p">,</span>
        <span class="n">array_ops</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">transpose_a</span><span class="o">=</span><span class="n">transpose_a</span><span class="p">,</span>
        <span class="n">adjoint_a</span><span class="o">=</span><span class="n">adjoint_a</span><span class="p">,</span>
        <span class="n">a_is_sparse</span><span class="o">=</span><span class="n">a_is_sparse</span><span class="p">,</span>
        <span class="n">b_is_sparse</span><span class="o">=</span><span class="n">b_is_sparse</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>


<span class="n">_OverrideBinaryOperatorHelper</span><span class="p">(</span><span class="n">matmul</span><span class="p">,</span> <span class="s2">&quot;matmul&quot;</span><span class="p">)</span>

<span class="n">sparse_matmul</span> <span class="o">=</span> <span class="n">deprecation</span><span class="o">.</span><span class="n">deprecated</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Use `tf.linalg.matmul` instead&quot;</span><span class="p">)(</span>
    <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sparse_mat_mul</span><span class="p">)</span>
<span class="n">tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;sparse_matmul&quot;</span><span class="p">])(</span><span class="n">sparse_matmul</span><span class="p">)</span>


<span class="nd">@ops</span><span class="o">.</span><span class="n">RegisterStatistics</span><span class="p">(</span><span class="s2">&quot;MatMul&quot;</span><span class="p">,</span> <span class="s2">&quot;flops&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_calc_mat_mul_flops</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">node</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Calculates the compute resources needed for MatMul.&quot;&quot;&quot;</span>
  <span class="n">transpose_a</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">attr</span><span class="p">[</span><span class="s2">&quot;transpose_a&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">b</span>
  <span class="n">a_shape</span> <span class="o">=</span> <span class="n">graph_util</span><span class="o">.</span><span class="n">tensor_shape_from_node_def_name</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">input</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
  <span class="n">a_shape</span><span class="o">.</span><span class="n">assert_is_fully_defined</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">transpose_a</span><span class="p">:</span>
    <span class="n">k</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">a_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">k</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">a_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
  <span class="n">output_shape</span> <span class="o">=</span> <span class="n">graph_util</span><span class="o">.</span><span class="n">tensor_shape_from_node_def_name</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
  <span class="n">output_shape</span><span class="o">.</span><span class="n">assert_is_fully_defined</span><span class="p">()</span>
  <span class="n">output_count</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">output_shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">())</span>
  <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">OpStats</span><span class="p">(</span><span class="s2">&quot;flops&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">k</span> <span class="o">*</span> <span class="n">output_count</span> <span class="o">*</span> <span class="mi">2</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_as_indexed_slices</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Convert &#39;x&#39; to IndexedSlices.</span>

<span class="sd">  Convert a dense Tensor to a block-sparse IndexedSlices.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: Either a Tensor object, or an IndexedSlices object.</span>
<span class="sd">    optimize: if true, attempt to optimize the conversion of &#39;x&#39;.</span>

<span class="sd">  Returns:</span>
<span class="sd">    An IndexedSlices object.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If &#39;x&#39; is not a Tensor or an IndexedSlices object.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># TODO(touts): op_scope</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">)):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Not a Tensor or IndexedSlices: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span>
  <span class="n">x_shape</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape_internal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="n">optimize</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">x_shape</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_as_indexed_slices_list</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Convert all elements of &#39;inputs&#39; to IndexedSlices.</span>

<span class="sd">  Additionally, homogenize the types of all the indices to</span>
<span class="sd">  either int32 or int64.</span>

<span class="sd">  Args:</span>
<span class="sd">    inputs: List containing either Tensor or IndexedSlices objects.</span>
<span class="sd">    optimize: if true, attempt to optimize the conversion of each input.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of IndexedSlices objects.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If &#39;inputs&#39; is not a list or a tuple.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Expected a list or tuple, not a </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
  <span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">_as_indexed_slices</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="n">optimize</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">]</span>
  <span class="n">with_int32_index</span> <span class="o">=</span> <span class="p">[</span>
      <span class="n">o</span><span class="o">.</span><span class="n">indices</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">outputs</span> <span class="k">if</span> <span class="n">o</span><span class="o">.</span><span class="n">indices</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span>
  <span class="p">]</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">with_int32_index</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">with_int32_index</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">outputs</span>
  <span class="n">casted_outputs</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">o</span><span class="o">.</span><span class="n">indices</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span>
      <span class="n">casted_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
          <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">cast</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
                            <span class="n">o</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">casted_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">o</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">casted_outputs</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.add_n&quot;</span><span class="p">,</span> <span class="s2">&quot;add_n&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">add_n</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Adds all input tensors element-wise.</span>

<span class="sd">  Converts `IndexedSlices` objects into dense tensors prior to adding.</span>

<span class="sd">  Args:</span>
<span class="sd">    inputs: A list of `Tensor` or `IndexedSlices` objects, each with same shape</span>
<span class="sd">      and type.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` of same shape and type as the elements of `inputs`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If `inputs` don&#39;t all have same shape and dtype or the shape</span>
<span class="sd">    cannot be inferred.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">inputs</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;inputs must be a list of at least one&quot;</span>
                     <span class="s2">&quot;Tensor/IndexedSlices with the same dtype and shape&quot;</span><span class="p">)</span>
  <span class="n">inputs</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_n_to_tensor_or_indexed_slices</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;inputs must be a list of at least one&quot;</span>
                     <span class="s2">&quot;Tensor/IndexedSlices with the same dtype and shape&quot;</span><span class="p">)</span>

  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="n">values</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">values</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">name</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">values</span>
  <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">add_n</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.accumulate_n&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.accumulate_n&quot;</span><span class="p">,</span> <span class="s2">&quot;accumulate_n&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;accumulate_n&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">accumulate_n</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tensor_dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the element-wise sum of a list of tensors.</span>

<span class="sd">  Optionally, pass `shape` and `tensor_dtype` for shape and type checking,</span>
<span class="sd">  otherwise, these are inferred.</span>

<span class="sd">  `tf.math.accumulate_n` performs the same operation as `tf.add_n`, but does not</span>
<span class="sd">  wait for all of its inputs to be ready before beginning to sum. This can</span>
<span class="sd">  save memory if inputs are ready at different times, since minimum temporary</span>
<span class="sd">  storage is proportional to the output size rather than the inputs size.</span>

<span class="sd">  `accumulate_n` is differentiable (but wasn&#39;t previous to TensorFlow 1.7).</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  a = tf.constant([[1, 2], [3, 4]])</span>
<span class="sd">  b = tf.constant([[5, 0], [0, 6]])</span>
<span class="sd">  tf.math.accumulate_n([a, b, a])  # [[7, 4], [6, 14]]</span>

<span class="sd">  # Explicitly pass shape and type</span>
<span class="sd">  tf.math.accumulate_n([a, b, a], shape=[2, 2], tensor_dtype=tf.int32)</span>
<span class="sd">                                                                 # [[7,  4],</span>
<span class="sd">                                                                 #  [6, 14]]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    inputs: A list of `Tensor` objects, each with same shape and type.</span>
<span class="sd">    shape: Shape of elements of `inputs`.</span>
<span class="sd">    tensor_dtype: The type of `inputs`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` of same shape and type as the elements of `inputs`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If `inputs` don&#39;t all have same shape and dtype or the shape</span>
<span class="sd">    cannot be inferred.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">_input_error</span><span class="p">():</span>
    <span class="k">return</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;inputs must be a list of at least one Tensor with the &quot;</span>
                      <span class="s2">&quot;same dtype and shape&quot;</span><span class="p">)</span>

  <span class="k">if</span> <span class="ow">not</span> <span class="n">inputs</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
    <span class="k">raise</span> <span class="n">_input_error</span><span class="p">()</span>
  <span class="n">inputs</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_n_to_tensor_or_indexed_slices</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="k">raise</span> <span class="n">_input_error</span><span class="p">()</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="k">raise</span> <span class="n">_input_error</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">as_shape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">unknown_shape</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">input_tensor</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
      <span class="n">shape</span> <span class="o">=</span> <span class="n">shape</span><span class="o">.</span><span class="n">merge_with</span><span class="p">(</span><span class="n">input_tensor</span><span class="o">.</span><span class="n">get_shape</span><span class="p">())</span>

  <span class="c1"># tensor_dtype is for safety only; operator&#39;s output type computed in C++</span>
  <span class="k">if</span> <span class="n">tensor_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">tensor_dtype</span> <span class="o">!=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;tensor_dtype is </span><span class="si">{}</span><span class="s2">, but input is of type </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">tensor_dtype</span><span class="p">,</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>

  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="c1"># TemporaryVariable not currently supported in eager mode; fall back</span>
    <span class="c1"># onto AddN for now.</span>
    <span class="c1"># TODO(frreiss) remove this once the lifetime of eager variables gets</span>
    <span class="c1"># addressed</span>
    <span class="k">return</span> <span class="n">add_n</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">accumulate_nv2</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>


<span class="nd">@ops</span><span class="o">.</span><span class="n">RegisterGradient</span><span class="p">(</span><span class="s2">&quot;AccumulateNV2&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_accumulate_n_grad</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Same as gradient for AddN. Copies the gradient to all inputs.&quot;&quot;&quot;</span>
  <span class="c1"># Not broadcasting.</span>
  <span class="k">return</span> <span class="p">[</span><span class="n">grad</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.sigmoid&quot;</span><span class="p">,</span> <span class="s2">&quot;nn.sigmoid&quot;</span><span class="p">,</span> <span class="s2">&quot;sigmoid&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes sigmoid of `x` element-wise.</span>

<span class="sd">  Specifically, `y = 1 / (1 + exp(-x))`.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A Tensor with type `float16`, `float32`, `float64`, `complex64`,</span>
<span class="sd">      or `complex128`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A Tensor with the same type as `x`.</span>

<span class="sd">  @compatibility(scipy)</span>
<span class="sd">  Equivalent to scipy.special.expit</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Sigmoid&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.log_sigmoid&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.log_sigmoid&quot;</span><span class="p">,</span> <span class="s2">&quot;log_sigmoid&quot;</span><span class="p">])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;log_sigmoid&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">log_sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes log sigmoid of `x` element-wise.</span>

<span class="sd">  Specifically, `y = log(1 / (1 + exp(-x)))`.  For numerical stability,</span>
<span class="sd">  we use `y = -tf.nn.softplus(-x)`.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A Tensor with type `float32` or `float64`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A Tensor with the same type as `x`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;LogSigmoid&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">neg</span><span class="p">(</span><span class="n">gen_nn_ops</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.bincount&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">def</span> <span class="nf">bincount</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span>
             <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">minlength</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">maxlength</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span>
             <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Counts the number of occurrences of each value in an integer array.</span>

<span class="sd">  If `minlength` and `maxlength` are not given, returns a vector with length</span>
<span class="sd">  `tf.reduce_max(arr) + 1` if `arr` is non-empty, and length 0 otherwise.</span>
<span class="sd">  If `weights` are non-None, then index `i` of the output stores the sum of the</span>
<span class="sd">  value in `weights` at each index where the corresponding value in `arr` is</span>
<span class="sd">  `i`.</span>

<span class="sd">  Args:</span>
<span class="sd">    arr: An int32 tensor of non-negative values.</span>
<span class="sd">    weights: If non-None, must be the same shape as arr. For each value in</span>
<span class="sd">      `arr`, the bin will be incremented by the corresponding weight instead of</span>
<span class="sd">      1.</span>
<span class="sd">    minlength: If given, ensures the output has length at least `minlength`,</span>
<span class="sd">      padding with zeros at the end if necessary.</span>
<span class="sd">    maxlength: If given, skips values in `arr` that are equal or greater than</span>
<span class="sd">      `maxlength`, ensuring that the output has length at most `maxlength`.</span>
<span class="sd">    dtype: If `weights` is None, determines the type of the output bins.</span>
<span class="sd">    name: A name scope for the associated operations (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A vector with the same dtype as `weights` or the given `dtype`. The bin</span>
<span class="sd">    values.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;bincount&quot;</span> <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">name</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">arr</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;arr&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">array_is_nonempty</span> <span class="o">=</span> <span class="n">reduce_prod</span><span class="p">(</span><span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">arr</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">array_is_nonempty</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">minlength</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">minlength</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
          <span class="n">minlength</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;minlength&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
      <span class="n">output_size</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">minlength</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">maxlength</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">maxlength</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
          <span class="n">maxlength</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;maxlength&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
      <span class="n">output_size</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">maxlength</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">weights</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;weights&quot;</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">unsorted_segment_sum</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">arr</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">constant_op</span><span class="o">.</span><span class="n">constant</span><span class="p">([],</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.bincount&quot;</span><span class="p">,</span> <span class="s2">&quot;bincount&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;bincount&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">bincount_v1</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span>
                <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">minlength</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">maxlength</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Counts the number of occurrences of each value in an integer array.</span>

<span class="sd">  If `minlength` and `maxlength` are not given, returns a vector with length</span>
<span class="sd">  `tf.reduce_max(arr) + 1` if `arr` is non-empty, and length 0 otherwise.</span>
<span class="sd">  If `weights` are non-None, then index `i` of the output stores the sum of the</span>
<span class="sd">  value in `weights` at each index where the corresponding value in `arr` is</span>
<span class="sd">  `i`.</span>

<span class="sd">  Args:</span>
<span class="sd">    arr: An int32 tensor of non-negative values.</span>
<span class="sd">    weights: If non-None, must be the same shape as arr. For each value in</span>
<span class="sd">      `arr`, the bin will be incremented by the corresponding weight instead of</span>
<span class="sd">      1.</span>
<span class="sd">    minlength: If given, ensures the output has length at least `minlength`,</span>
<span class="sd">      padding with zeros at the end if necessary.</span>
<span class="sd">    maxlength: If given, skips values in `arr` that are equal or greater than</span>
<span class="sd">      `maxlength`, ensuring that the output has length at most `maxlength`.</span>
<span class="sd">    dtype: If `weights` is None, determines the type of the output bins.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A vector with the same dtype as `weights` or the given `dtype`. The bin</span>
<span class="sd">    values.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">bincount</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">minlength</span><span class="p">,</span> <span class="n">maxlength</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.cumsum&quot;</span><span class="p">,</span> <span class="s2">&quot;cumsum&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">cumsum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Compute the cumulative sum of the tensor `x` along `axis`.</span>

<span class="sd">  By default, this op performs an inclusive cumsum, which means that the first</span>
<span class="sd">  element of the input is identical to the first element of the output:</span>

<span class="sd">  ```python</span>
<span class="sd">  tf.cumsum([a, b, c])  # [a, a + b, a + b + c]</span>
<span class="sd">  ```</span>

<span class="sd">  By setting the `exclusive` kwarg to `True`, an exclusive cumsum is performed</span>
<span class="sd">  instead:</span>

<span class="sd">  ```python</span>
<span class="sd">  tf.cumsum([a, b, c], exclusive=True)  # [0, a, a + b]</span>
<span class="sd">  ```</span>

<span class="sd">  By setting the `reverse` kwarg to `True`, the cumsum is performed in the</span>
<span class="sd">  opposite direction:</span>

<span class="sd">  ```python</span>
<span class="sd">  tf.cumsum([a, b, c], reverse=True)  # [a + b + c, b + c, c]</span>
<span class="sd">  ```</span>

<span class="sd">  This is more efficient than using separate `tf.reverse` ops.</span>

<span class="sd">  The `reverse` and `exclusive` kwargs can also be combined:</span>

<span class="sd">  ```python</span>
<span class="sd">  tf.cumsum([a, b, c], exclusive=True, reverse=True)  # [b + c, c, 0]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor`. Must be one of the following types: `float32`, `float64`,</span>
<span class="sd">       `int64`, `int32`, `uint8`, `uint16`, `int16`, `int8`, `complex64`,</span>
<span class="sd">       `complex128`, `qint8`, `quint8`, `qint32`, `half`.</span>
<span class="sd">    axis: A `Tensor` of type `int32` (default: 0). Must be in the range</span>
<span class="sd">      `[-rank(x), rank(x))`.</span>
<span class="sd">    exclusive: If `True`, perform exclusive cumsum.</span>
<span class="sd">    reverse: A `bool` (default: False).</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `x`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Cumsum&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="n">exclusive</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="n">reverse</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.cumprod&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.cumprod&quot;</span><span class="p">,</span> <span class="s2">&quot;cumprod&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;cumprod&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">cumprod</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Compute the cumulative product of the tensor `x` along `axis`.</span>

<span class="sd">  By default, this op performs an inclusive cumprod, which means that the</span>
<span class="sd">  first element of the input is identical to the first element of the output:</span>

<span class="sd">  ```python</span>
<span class="sd">  tf.math.cumprod([a, b, c])  # [a, a * b, a * b * c]</span>
<span class="sd">  ```</span>

<span class="sd">  By setting the `exclusive` kwarg to `True`, an exclusive cumprod is</span>
<span class="sd">  performed</span>
<span class="sd">  instead:</span>

<span class="sd">  ```python</span>
<span class="sd">  tf.math.cumprod([a, b, c], exclusive=True)  # [1, a, a * b]</span>
<span class="sd">  ```</span>

<span class="sd">  By setting the `reverse` kwarg to `True`, the cumprod is performed in the</span>
<span class="sd">  opposite direction:</span>

<span class="sd">  ```python</span>
<span class="sd">  tf.math.cumprod([a, b, c], reverse=True)  # [a * b * c, b * c, c]</span>
<span class="sd">  ```</span>

<span class="sd">  This is more efficient than using separate `tf.reverse` ops.</span>
<span class="sd">  The `reverse` and `exclusive` kwargs can also be combined:</span>

<span class="sd">  ```python</span>
<span class="sd">  tf.math.cumprod([a, b, c], exclusive=True, reverse=True)  # [b * c, c, 1]</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A `Tensor`. Must be one of the following types: `float32`, `float64`,</span>
<span class="sd">       `int64`, `int32`, `uint8`, `uint16`, `int16`, `int8`, `complex64`,</span>
<span class="sd">       `complex128`, `qint8`, `quint8`, `qint32`, `half`.</span>
<span class="sd">    axis: A `Tensor` of type `int32` (default: 0). Must be in the range</span>
<span class="sd">      `[-rank(x), rank(x))`.</span>
<span class="sd">    exclusive: If `True`, perform exclusive cumprod.</span>
<span class="sd">    reverse: A `bool` (default: False).</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`. Has the same type as `x`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Cumprod&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">cumprod</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="n">exclusive</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="n">reverse</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.conj&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.conj&quot;</span><span class="p">,</span> <span class="s2">&quot;conj&quot;</span><span class="p">])</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;conj&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">conj</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the complex conjugate of a complex number.</span>

<span class="sd">  Given a tensor `input` of complex numbers, this operation returns a tensor of</span>
<span class="sd">  complex numbers that are the complex conjugate of each element in `input`. The</span>
<span class="sd">  complex numbers in `input` must be of the form \\(a + bj\\), where *a* is the</span>
<span class="sd">  real part and *b* is the imaginary part.</span>

<span class="sd">  The complex conjugate returned by this operation is of the form \\(a - bj\\).</span>

<span class="sd">  For example:</span>

<span class="sd">      # tensor &#39;input&#39; is [-2.25 + 4.75j, 3.25 + 5.75j]</span>
<span class="sd">      tf.math.conj(input) ==&gt; [-2.25 - 4.75j, 3.25 - 5.75j]</span>

<span class="sd">  If `x` is real, it is returned unchanged.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: `Tensor` to conjugate.  Must have numeric or variant type.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` that is the conjugate of `x` (with the same type).</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If `x` is not a numeric tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="n">dt</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">if</span> <span class="n">dt</span><span class="o">.</span><span class="n">is_floating</span> <span class="ow">or</span> <span class="n">dt</span><span class="o">.</span><span class="n">is_integer</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">x</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Conj&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span> <span class="ow">or</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">variant</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">conj</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_floating</span> <span class="ow">or</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_integer</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">x</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
          <span class="s2">&quot;Expected numeric or variant tensor, got dtype </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_BroadcastShape</span><span class="p">(</span><span class="n">op</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Common shape function for binary operators that broadcast their inputs.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="p">[</span>
      <span class="n">common_shapes</span><span class="o">.</span><span class="n">broadcast_shape</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_shape</span><span class="p">(),</span>
                                    <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_shape</span><span class="p">())</span>
  <span class="p">]</span>


<span class="k">def</span> <span class="nf">reduced_shape</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">axes</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Helper function for reduction ops.</span>

<span class="sd">  Args:</span>
<span class="sd">    input_shape: 1-D Tensor, the shape of the Tensor being reduced.</span>
<span class="sd">    axes: 1-D Tensor, the reduction axes.</span>
<span class="sd">  Returns:</span>
<span class="sd">    A 1-D Tensor, the output shape as if keepdims were set to True.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># Example:</span>
  <span class="c1"># cast needed for SparseTensor reductions</span>
  <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="n">input_shape</span> <span class="o">=</span> <span class="n">input_shape</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">input_shape</span><span class="p">[</span><span class="n">axes</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">input_shape</span>

  <span class="n">input_shape</span> <span class="o">=</span> <span class="n">to_int32</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>  <span class="c1"># [2, 3, 5, 7]</span>
  <span class="n">axes</span> <span class="o">=</span> <span class="n">to_int32</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span>  <span class="c1"># [1, 2]</span>

  <span class="n">input_rank</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>  <span class="c1"># 4</span>
  <span class="n">axes</span> <span class="o">=</span> <span class="p">(</span><span class="n">axes</span> <span class="o">+</span> <span class="n">input_rank</span><span class="p">)</span> <span class="o">%</span> <span class="n">input_rank</span>
  <span class="n">axes_shape</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span>  <span class="c1"># [2]</span>
  <span class="k">return</span> <span class="n">gen_data_flow_ops</span><span class="o">.</span><span class="n">dynamic_stitch</span><span class="p">(</span>  <span class="c1"># [2, 1, 1, 7]</span>
      <span class="p">[</span>
          <span class="nb">range</span><span class="p">(</span><span class="n">input_rank</span><span class="p">),</span>  <span class="c1"># [0, 1, 2, 3]</span>
          <span class="n">axes</span>
      <span class="p">],</span>  <span class="c1"># [1, 2]</span>
      <span class="p">[</span>
          <span class="n">input_shape</span><span class="p">,</span>  <span class="c1"># [2, 3, 5, 7]</span>
          <span class="n">array_ops</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">axes_shape</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
      <span class="p">])</span>  <span class="c1"># [1, 1]</span>


<span class="k">def</span> <span class="nf">_unsorted_segment_N</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot; Helper function for unsorted_segment_mean/_sqrtN. Computes the number</span>
<span class="sd">      of segment entries with 0-entries set to 1 to allow division by N.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># bincount doesn&#39;t support negative indices so we use unsorted_segment_sum</span>
  <span class="n">segment_ids_shape</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape_internal</span><span class="p">(</span><span class="n">segment_ids</span><span class="p">)</span>
  <span class="n">ones_tensor</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">segment_ids_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
  <span class="n">N</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">unsorted_segment_sum</span><span class="p">(</span><span class="n">ones_tensor</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">)</span>
  <span class="c1"># add dimensions for all non-reduced axes</span>
  <span class="n">ndims_output</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">-</span> <span class="n">segment_ids</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span>
  <span class="n">broadcast_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">num_segments</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">ndims_output</span>
  <span class="n">N</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">broadcast_shape</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span>
    <span class="s2">&quot;math.unsorted_segment_mean&quot;</span><span class="p">,</span>
    <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.unsorted_segment_mean&quot;</span><span class="p">,</span> <span class="s2">&quot;unsorted_segment_mean&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;unsorted_segment_mean&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">unsorted_segment_mean</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the mean along segments of a tensor.</span>

<span class="sd">  Read [the section on</span>
<span class="sd">  segmentation](https://tensorflow.org/api_guides/python/math_ops#segmentation)</span>
<span class="sd">  for an explanation of segments.</span>

<span class="sd">  This operator is similar to the unsorted segment sum operator found</span>
<span class="sd">  [here](../../../api_docs/python/math_ops.md#UnsortedSegmentSum).</span>
<span class="sd">  Instead of computing the sum over segments, it computes the mean of all</span>
<span class="sd">  entries belonging to a segment such that:</span>

<span class="sd">  \\(output_i = 1/N_i \sum_{j...} data[j...]\\) where the sum is over tuples</span>
<span class="sd">  `j...` such that `segment_ids[j...] == i` with \\N_i\\ being the number of</span>
<span class="sd">  occurrences of id \\i\\.</span>

<span class="sd">  If there is no entry for a given segment ID `i`, it outputs 0.</span>

<span class="sd">  If the given segment ID `i` is negative, the value is dropped and will not</span>
<span class="sd">  be added to the sum of the segment.</span>

<span class="sd">  Args:</span>
<span class="sd">    data: A `Tensor` with floating point or complex dtype.</span>
<span class="sd">    segment_ids: An integer tensor whose shape is a prefix of `data.shape`.</span>
<span class="sd">    num_segments: An integer scalar `Tensor`.  The number of distinct</span>
<span class="sd">      segment IDs.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`.  Has same shape as data, except for the first `segment_ids.rank`</span>
<span class="sd">    dimensions, which are replaced with a single dimension which has size</span>
<span class="sd">   `num_segments`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;UnsortedSegmentMean&quot;</span><span class="p">):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">segment_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">segment_ids</span><span class="p">)</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">_unsorted_segment_N</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">)</span>
    <span class="n">summed</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">unsorted_segment_sum</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">summed</span> <span class="o">/</span> <span class="n">N</span>


<span class="nd">@tf_export</span><span class="p">(</span>
    <span class="s2">&quot;math.unsorted_segment_sqrt_n&quot;</span><span class="p">,</span>
    <span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;math.unsorted_segment_sqrt_n&quot;</span><span class="p">,</span> <span class="s2">&quot;unsorted_segment_sqrt_n&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;unsorted_segment_sqrt_n&quot;</span><span class="p">)</span>
<span class="nd">@dispatch</span><span class="o">.</span><span class="n">add_dispatch_support</span>
<span class="k">def</span> <span class="nf">unsorted_segment_sqrt_n</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the sum along segments of a tensor divided by the sqrt(N).</span>

<span class="sd">  Read [the section on</span>
<span class="sd">  segmentation](https://tensorflow.org/api_guides/python/math_ops#segmentation)</span>
<span class="sd">  for an explanation of segments.</span>

<span class="sd">  This operator is similar to the unsorted segment sum operator found</span>
<span class="sd">  [here](../../../api_docs/python/math_ops.md#UnsortedSegmentSum).</span>
<span class="sd">  Additionally to computing the sum over segments, it divides the results by</span>
<span class="sd">  sqrt(N).</span>

<span class="sd">  \\(output_i = 1/sqrt(N_i) \sum_{j...} data[j...]\\) where the sum is over</span>
<span class="sd">  tuples `j...` such that `segment_ids[j...] == i` with \\N_i\\ being the</span>
<span class="sd">  number of occurrences of id \\i\\.</span>

<span class="sd">  If there is no entry for a given segment ID `i`, it outputs 0.</span>

<span class="sd">  Note that this op only supports floating point and complex dtypes,</span>
<span class="sd">  due to tf.sqrt only supporting these types.</span>

<span class="sd">  If the given segment ID `i` is negative, the value is dropped and will not</span>
<span class="sd">  be added to the sum of the segment.</span>

<span class="sd">  Args:</span>
<span class="sd">    data: A `Tensor` with floating point or complex dtype.</span>
<span class="sd">    segment_ids: An integer tensor whose shape is a prefix of `data.shape`.</span>
<span class="sd">    num_segments: An integer scalar `Tensor`.  The number of distinct</span>
<span class="sd">      segment IDs.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor`.  Has same shape as data, except for the first `segment_ids.rank`</span>
<span class="sd">    dimensions, which are replaced with a single dimension which has size</span>
<span class="sd">   `num_segments`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;UnsortedSegmentSqrtN&quot;</span><span class="p">):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">segment_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">segment_ids</span><span class="p">)</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">_unsorted_segment_N</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">)</span>
    <span class="n">summed</span> <span class="o">=</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">unsorted_segment_sum</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">summed</span> <span class="o">/</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;sparse.segment_sum&quot;</span><span class="p">,</span> <span class="s2">&quot;sparse_segment_sum&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;sparse_segment_sum&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">sparse_segment_sum</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                       <span class="n">num_segments</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the sum along sparse segments of a tensor.</span>

<span class="sd">  Read [the section on</span>
<span class="sd">  segmentation](https://tensorflow.org/api_guides/python/math_ops#Segmentation)</span>
<span class="sd">  for an explanation of segments.</span>

<span class="sd">  Like `SegmentSum`, but `segment_ids` can have rank less than `data`&#39;s first</span>
<span class="sd">  dimension, selecting a subset of dimension 0, specified by `indices`.</span>
<span class="sd">  `segment_ids` is allowed to have missing ids, in which case the output will</span>
<span class="sd">  be zeros at those indices. In those cases `num_segments` is used to determine</span>
<span class="sd">  the size of the output.</span>

<span class="sd">  For example:</span>

<span class="sd">  ```python</span>
<span class="sd">  c = tf.constant([[1,2,3,4], [-1,-2,-3,-4], [5,6,7,8]])</span>

<span class="sd">  # Select two rows, one segment.</span>
<span class="sd">  tf.sparse.segment_sum(c, tf.constant([0, 1]), tf.constant([0, 0]))</span>
<span class="sd">  # =&gt; [[0 0 0 0]]</span>

<span class="sd">  # Select two rows, two segment.</span>
<span class="sd">  tf.sparse.segment_sum(c, tf.constant([0, 1]), tf.constant([0, 1]))</span>
<span class="sd">  # =&gt; [[ 1  2  3  4]</span>
<span class="sd">  #     [-1 -2 -3 -4]]</span>

<span class="sd">  # With missing segment ids.</span>
<span class="sd">  tf.sparse.segment_sum(c, tf.constant([0, 1]), tf.constant([0, 2]),</span>
<span class="sd">                        num_segments=4)</span>
<span class="sd">  # =&gt; [[ 1  2  3  4]</span>
<span class="sd">  #     [ 0  0  0  0]</span>
<span class="sd">  #     [-1 -2 -3 -4]</span>
<span class="sd">  #     [ 0  0  0  0]]</span>

<span class="sd">  # Select all rows, two segments.</span>
<span class="sd">  tf.sparse.segment_sum(c, tf.constant([0, 1, 2]), tf.constant([0, 0, 1]))</span>
<span class="sd">  # =&gt; [[0 0 0 0]</span>
<span class="sd">  #     [5 6 7 8]]</span>

<span class="sd">  # Which is equivalent to:</span>
<span class="sd">  tf.segment_sum(c, tf.constant([0, 0, 1]))</span>
<span class="sd">  ```</span>

<span class="sd">  Args:</span>
<span class="sd">    data: A `Tensor` with data that will be assembled in the output.</span>
<span class="sd">    indices: A 1-D `Tensor` with indices into `data`. Has same rank as</span>
<span class="sd">      `segment_ids`.</span>
<span class="sd">    segment_ids: A 1-D `Tensor` with indices into the output `Tensor`.</span>
<span class="sd">      Values should be sorted and can be repeated.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    num_segments: An optional int32 scalar. Indicates the size of the output</span>
<span class="sd">      `Tensor`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `tensor` of the shape as data, except for dimension 0 which</span>
<span class="sd">    has size `k`, the number of segments specified via `num_segments` or</span>
<span class="sd">    inferred for the last element in `segments_ids`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">num_segments</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sparse_segment_sum_with_num_segments</span><span class="p">(</span>
        <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
        <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">,</span>
        <span class="n">segment_ids</span><span class="o">=</span><span class="n">segment_ids</span><span class="p">,</span>
        <span class="n">num_segments</span><span class="o">=</span><span class="n">num_segments</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sparse_segment_sum</span><span class="p">(</span>
        <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">,</span> <span class="n">segment_ids</span><span class="o">=</span><span class="n">segment_ids</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;sparse.segment_sum&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">def</span> <span class="nf">sparse_segment_sum_v2</span><span class="p">(</span><span class="n">data</span><span class="p">,</span>
                          <span class="n">indices</span><span class="p">,</span>
                          <span class="n">segment_ids</span><span class="p">,</span>
                          <span class="n">num_segments</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">sparse_segment_mean</span><span class="p">(</span>
      <span class="n">data</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">num_segments</span><span class="o">=</span><span class="n">num_segments</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;sparse.segment_mean&quot;</span><span class="p">,</span> <span class="s2">&quot;sparse_segment_mean&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;sparse_segment_mean&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">sparse_segment_mean</span><span class="p">(</span><span class="n">data</span><span class="p">,</span>
                        <span class="n">indices</span><span class="p">,</span>
                        <span class="n">segment_ids</span><span class="p">,</span>
                        <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">num_segments</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the mean along sparse segments of a tensor.</span>

<span class="sd">  Read [the section on</span>
<span class="sd">  segmentation](https://tensorflow.org/api_guides/python/math_ops#Segmentation)</span>
<span class="sd">  for an explanation of segments.</span>

<span class="sd">  Like `SegmentMean`, but `segment_ids` can have rank less than `data`&#39;s first</span>
<span class="sd">  dimension, selecting a subset of dimension 0, specified by `indices`.</span>
<span class="sd">  `segment_ids` is allowed to have missing ids, in which case the output will</span>
<span class="sd">  be zeros at those indices. In those cases `num_segments` is used to determine</span>
<span class="sd">  the size of the output.</span>

<span class="sd">  Args:</span>
<span class="sd">    data: A `Tensor` with data that will be assembled in the output.</span>
<span class="sd">    indices: A 1-D `Tensor` with indices into `data`. Has same rank as</span>
<span class="sd">      `segment_ids`.</span>
<span class="sd">    segment_ids: A 1-D `Tensor` with indices into the output `Tensor`.</span>
<span class="sd">      Values should be sorted and can be repeated.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    num_segments: An optional int32 scalar. Indicates the size of the output</span>
<span class="sd">      `Tensor`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `tensor` of the shape as data, except for dimension 0 which</span>
<span class="sd">    has size `k`, the number of segments specified via `num_segments` or</span>
<span class="sd">    inferred for the last element in `segments_ids`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">num_segments</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sparse_segment_mean_with_num_segments</span><span class="p">(</span>
        <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
        <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">,</span>
        <span class="n">segment_ids</span><span class="o">=</span><span class="n">segment_ids</span><span class="p">,</span>
        <span class="n">num_segments</span><span class="o">=</span><span class="n">num_segments</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sparse_segment_mean</span><span class="p">(</span>
        <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">,</span> <span class="n">segment_ids</span><span class="o">=</span><span class="n">segment_ids</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;sparse.segment_mean&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">def</span> <span class="nf">sparse_segment_mean_v2</span><span class="p">(</span><span class="n">data</span><span class="p">,</span>
                           <span class="n">indices</span><span class="p">,</span>
                           <span class="n">segment_ids</span><span class="p">,</span>
                           <span class="n">num_segments</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the mean along sparse segments of a tensor.</span>

<span class="sd">  Read [the section on</span>
<span class="sd">  segmentation](https://tensorflow.org/api_guides/python/math_ops#Segmentation)</span>
<span class="sd">  for an explanation of segments.</span>

<span class="sd">  Like `SegmentMean`, but `segment_ids` can have rank less than `data`&#39;s first</span>
<span class="sd">  dimension, selecting a subset of dimension 0, specified by `indices`.</span>
<span class="sd">  `segment_ids` is allowed to have missing ids, in which case the output will</span>
<span class="sd">  be zeros at those indices. In those cases `num_segments` is used to determine</span>
<span class="sd">  the size of the output.</span>

<span class="sd">  Args:</span>
<span class="sd">    data: A `Tensor` with data that will be assembled in the output.</span>
<span class="sd">    indices: A 1-D `Tensor` with indices into `data`. Has same rank as</span>
<span class="sd">      `segment_ids`.</span>
<span class="sd">    segment_ids: A 1-D `Tensor` with indices into the output `Tensor`. Values</span>
<span class="sd">      should be sorted and can be repeated.</span>
<span class="sd">    num_segments: An optional int32 scalar. Indicates the size of the output</span>
<span class="sd">      `Tensor`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `tensor` of the shape as data, except for dimension 0 which</span>
<span class="sd">    has size `k`, the number of segments specified via `num_segments` or</span>
<span class="sd">    inferred for the last element in `segments_ids`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">sparse_segment_mean</span><span class="p">(</span>
      <span class="n">data</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">num_segments</span><span class="o">=</span><span class="n">num_segments</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;sparse.segment_sqrt_n&quot;</span><span class="p">,</span> <span class="s2">&quot;sparse_segment_sqrt_n&quot;</span><span class="p">])</span>
<span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_endpoints</span><span class="p">(</span><span class="s2">&quot;sparse_segment_sqrt_n&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">sparse_segment_sqrt_n</span><span class="p">(</span><span class="n">data</span><span class="p">,</span>
                          <span class="n">indices</span><span class="p">,</span>
                          <span class="n">segment_ids</span><span class="p">,</span>
                          <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">num_segments</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the sum along sparse segments of a tensor divided by the sqrt(N).</span>

<span class="sd">  `N` is the size of the segment being reduced.</span>

<span class="sd">  Args:</span>
<span class="sd">    data: A `Tensor` with data that will be assembled in the output.</span>
<span class="sd">    indices: A 1-D `Tensor` with indices into `data`. Has same rank as</span>
<span class="sd">      `segment_ids`.</span>
<span class="sd">    segment_ids: A 1-D `Tensor` with indices into the output `Tensor`.</span>
<span class="sd">      Values should be sorted and can be repeated.</span>
<span class="sd">    name: A name for the operation (optional).</span>
<span class="sd">    num_segments: An optional int32 scalar. Indicates the size of the output</span>
<span class="sd">      `Tensor`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `tensor` of the shape as data, except for dimension 0 which</span>
<span class="sd">    has size `k`, the number of segments specified via `num_segments` or</span>
<span class="sd">    inferred for the last element in `segments_ids`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">num_segments</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sparse_segment_sqrt_n_with_num_segments</span><span class="p">(</span>
        <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
        <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">,</span>
        <span class="n">segment_ids</span><span class="o">=</span><span class="n">segment_ids</span><span class="p">,</span>
        <span class="n">num_segments</span><span class="o">=</span><span class="n">num_segments</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">sparse_segment_sqrt_n</span><span class="p">(</span>
        <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">,</span> <span class="n">segment_ids</span><span class="o">=</span><span class="n">segment_ids</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;sparse.segment_sqrt_n&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">def</span> <span class="nf">sparse_segment_sqrt_n_v2</span><span class="p">(</span><span class="n">data</span><span class="p">,</span>
                             <span class="n">indices</span><span class="p">,</span>
                             <span class="n">segment_ids</span><span class="p">,</span>
                             <span class="n">num_segments</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                             <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the sum along sparse segments of a tensor divided by the sqrt(N).</span>

<span class="sd">  `N` is the size of the segment being reduced.</span>

<span class="sd">  Args:</span>
<span class="sd">    data: A `Tensor` with data that will be assembled in the output.</span>
<span class="sd">    indices: A 1-D `Tensor` with indices into `data`. Has same rank as</span>
<span class="sd">      `segment_ids`.</span>
<span class="sd">    segment_ids: A 1-D `Tensor` with indices into the output `Tensor`. Values</span>
<span class="sd">      should be sorted and can be repeated.</span>
<span class="sd">    num_segments: An optional int32 scalar. Indicates the size of the output</span>
<span class="sd">      `Tensor`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `tensor` of the shape as data, except for dimension 0 which</span>
<span class="sd">    has size `k`, the number of segments specified via `num_segments` or</span>
<span class="sd">    inferred for the last element in `segments_ids`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">sparse_segment_sqrt_n</span><span class="p">(</span>
      <span class="n">data</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">num_segments</span><span class="o">=</span><span class="n">num_segments</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;tensordot&quot;</span><span class="p">,</span> <span class="s2">&quot;linalg.tensordot&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">tensordot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Tensor contraction of a and b along specified axes.</span>

<span class="sd">  Tensordot (also known as tensor contraction) sums the product of elements</span>
<span class="sd">  from `a` and `b` over the indices specified by `a_axes` and `b_axes`.</span>
<span class="sd">  The lists `a_axes` and `b_axes` specify those pairs of axes along which to</span>
<span class="sd">  contract the tensors. The axis `a_axes[i]` of `a` must have the same dimension</span>
<span class="sd">  as axis `b_axes[i]` of `b` for all `i` in `range(0, len(a_axes))`. The lists</span>
<span class="sd">  `a_axes` and `b_axes` must have identical length and consist of unique</span>
<span class="sd">  integers that specify valid axes for each of the tensors.</span>

<span class="sd">  This operation corresponds to `numpy.tensordot(a, b, axes)`.</span>

<span class="sd">  Example 1: When `a` and `b` are matrices (order 2), the case `axes = 1`</span>
<span class="sd">  is equivalent to matrix multiplication.</span>

<span class="sd">  Example 2: When `a` and `b` are matrices (order 2), the case</span>
<span class="sd">  `axes = [[1], [0]]` is equivalent to matrix multiplication.</span>

<span class="sd">  Example 3: Suppose that \\(a_{ijk}\\) and \\(b_{lmn}\\) represent two</span>
<span class="sd">  tensors of order 3. Then, `contract(a, b, [[0], [2]])` is the order 4 tensor</span>
<span class="sd">  \\(c_{jklm}\\) whose entry</span>
<span class="sd">  corresponding to the indices \\((j,k,l,m)\\) is given by:</span>

<span class="sd">  \\( c_{jklm} = \sum_i a_{ijk} b_{lmi} \\).</span>

<span class="sd">  In general, `order(c) = order(a) + order(b) - 2*len(axes[0])`.</span>

<span class="sd">  Args:</span>
<span class="sd">    a: `Tensor` of type `float32` or `float64`.</span>
<span class="sd">    b: `Tensor` with the same type as `a`.</span>
<span class="sd">    axes: Either a scalar `N`, or a list or an `int32` `Tensor` of shape [2, k].</span>
<span class="sd">      If axes is a scalar, sum over the last N axes of a and the first N axes of</span>
<span class="sd">      b in order. If axes is a list or `Tensor` the first and second row contain</span>
<span class="sd">      the set of unique integers specifying axes along which the contraction is</span>
<span class="sd">      computed, for `a` and `b`, respectively. The number of axes for `a` and</span>
<span class="sd">      `b` must be equal.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` with the same type as `a`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If the shapes of `a`, `b`, and `axes` are incompatible.</span>
<span class="sd">    IndexError: If the values in axes exceed the rank of the corresponding</span>
<span class="sd">      tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">_tensordot_reshape</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">flipped</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Helper method to perform transpose and reshape for contraction op.</span>

<span class="sd">    This method is helpful in reducing `math_ops.tensordot` to `math_ops.matmul`</span>
<span class="sd">    using `array_ops.transpose` and `array_ops.reshape`. The method takes a</span>
<span class="sd">    tensor and performs the correct transpose and reshape operation for a given</span>
<span class="sd">    set of indices. It returns the reshaped tensor as well as a list of indices</span>
<span class="sd">    necessary to reshape the tensor again after matrix multiplication.</span>

<span class="sd">    Args:</span>
<span class="sd">      a: `Tensor`.</span>
<span class="sd">      axes: List or `int32` `Tensor` of unique indices specifying valid axes of</span>
<span class="sd">       `a`.</span>
<span class="sd">      flipped: An optional `bool`. Defaults to `False`. If `True`, the method</span>
<span class="sd">        assumes that `a` is the second argument in the contraction operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple `(reshaped_a, free_dims, free_dims_static)` where `reshaped_a` is</span>
<span class="sd">      the tensor `a` reshaped to allow contraction via `matmul`, `free_dims` is</span>
<span class="sd">      either a list of integers or an `int32` `Tensor`, depending on whether</span>
<span class="sd">      the shape of a is fully specified, and free_dims_static is either a list</span>
<span class="sd">      of integers and None values, or None, representing the inferred</span>
<span class="sd">      static shape of the free dimensions</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">is_fully_defined</span><span class="p">()</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
      <span class="n">shape_a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
      <span class="n">axes</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">i</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape_a</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">]</span>
      <span class="n">free</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">shape_a</span><span class="p">))</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">]</span>
      <span class="n">free_dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">shape_a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">free</span><span class="p">]</span>
      <span class="n">prod_free</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">([</span><span class="n">shape_a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">free</span><span class="p">]))</span>
      <span class="n">prod_axes</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">([</span><span class="n">shape_a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">]))</span>
      <span class="n">perm</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span> <span class="o">+</span> <span class="n">free</span> <span class="k">if</span> <span class="n">flipped</span> <span class="k">else</span> <span class="n">free</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span>
      <span class="n">new_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">prod_axes</span><span class="p">,</span> <span class="n">prod_free</span><span class="p">]</span> <span class="k">if</span> <span class="n">flipped</span> <span class="k">else</span> <span class="p">[</span><span class="n">prod_free</span><span class="p">,</span> <span class="n">prod_axes</span><span class="p">]</span>
      <span class="n">reshaped_a</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">array_ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">perm</span><span class="p">),</span> <span class="n">new_shape</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">reshaped_a</span><span class="p">,</span> <span class="n">free_dims</span><span class="p">,</span> <span class="n">free_dims</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="n">shape_a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">i</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape_a</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">]</span>
        <span class="n">free</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">shape_a</span><span class="p">))</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">]</span>
        <span class="n">axes_dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">shape_a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">]</span>
        <span class="n">free_dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">shape_a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">free</span><span class="p">]</span>
        <span class="n">free_dims_static</span> <span class="o">=</span> <span class="n">free_dims</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;axes&quot;</span><span class="p">)</span>
        <span class="n">free</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">free</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;free&quot;</span><span class="p">)</span>
        <span class="n">shape_a</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">free_dims_static</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">shape_a</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="n">rank_a</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;axes&quot;</span><span class="p">)</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">axes</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">axes</span> <span class="o">+</span> <span class="n">rank_a</span><span class="p">)</span>
        <span class="n">free</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">setdiff1d</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">rank_a</span><span class="p">),</span> <span class="n">axes</span><span class="p">)</span>
      <span class="n">free_dims</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">shape_a</span><span class="p">,</span> <span class="n">free</span><span class="p">)</span>
      <span class="n">axes_dims</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">shape_a</span><span class="p">,</span> <span class="n">axes</span><span class="p">)</span>
      <span class="n">prod_free_dims</span> <span class="o">=</span> <span class="n">reduce_prod</span><span class="p">(</span><span class="n">free_dims</span><span class="p">)</span>
      <span class="n">prod_axes_dims</span> <span class="o">=</span> <span class="n">reduce_prod</span><span class="p">(</span><span class="n">axes_dims</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">flipped</span><span class="p">:</span>
        <span class="n">perm</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">axes</span><span class="p">,</span> <span class="n">free</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">new_shape</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">prod_axes_dims</span><span class="p">,</span> <span class="n">prod_free_dims</span><span class="p">])</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">perm</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">free</span><span class="p">,</span> <span class="n">axes</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">new_shape</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">prod_free_dims</span><span class="p">,</span> <span class="n">prod_axes_dims</span><span class="p">])</span>
      <span class="n">reshaped_a</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">array_ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">perm</span><span class="p">),</span> <span class="n">new_shape</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">reshaped_a</span><span class="p">,</span> <span class="n">free_dims</span><span class="p">,</span> <span class="n">free_dims_static</span>

  <span class="k">def</span> <span class="nf">_tensordot_axes</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axes</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Generates two sets of contraction axes for the two tensor arguments.&quot;&quot;&quot;</span>
    <span class="n">a_shape</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">compat</span><span class="o">.</span><span class="n">integral_types</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">axes</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;axes&#39; must be at least 0.&quot;</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">a_shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">axes</span> <span class="o">&gt;</span> <span class="n">a_shape</span><span class="o">.</span><span class="n">ndims</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;axes&#39; must not be larger than the number of &quot;</span>
                           <span class="s2">&quot;dimensions of tensor </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="n">a</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">xrange</span><span class="p">(</span><span class="n">a_shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">-</span> <span class="n">axes</span><span class="p">,</span> <span class="n">a_shape</span><span class="o">.</span><span class="n">ndims</span><span class="p">)),</span>
                <span class="nb">list</span><span class="p">(</span><span class="n">xrange</span><span class="p">(</span><span class="n">axes</span><span class="p">)))</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">rank</span> <span class="o">-</span> <span class="n">axes</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
                <span class="nb">range</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
      <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;axes&#39; must be an integer or have length 2.&quot;</span><span class="p">)</span>
      <span class="n">a_axes</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">b_axes</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a_axes</span><span class="p">,</span> <span class="n">compat</span><span class="o">.</span><span class="n">integral_types</span><span class="p">)</span> <span class="ow">and</span> \
          <span class="nb">isinstance</span><span class="p">(</span><span class="n">b_axes</span><span class="p">,</span> <span class="n">compat</span><span class="o">.</span><span class="n">integral_types</span><span class="p">):</span>
        <span class="n">a_axes</span> <span class="o">=</span> <span class="p">[</span><span class="n">a_axes</span><span class="p">]</span>
        <span class="n">b_axes</span> <span class="o">=</span> <span class="p">[</span><span class="n">b_axes</span><span class="p">]</span>
      <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">a_axes</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">b_axes</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Different number of contraction axes &#39;a&#39; and &#39;b&#39;, </span><span class="si">%s</span><span class="s2"> != </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span>
            <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">a_axes</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">b_axes</span><span class="p">)))</span>
      <span class="k">return</span> <span class="n">a_axes</span><span class="p">,</span> <span class="n">b_axes</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">axes</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;axes&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Tensordot&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">axes</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;a&quot;</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;b&quot;</span><span class="p">)</span>
    <span class="n">a_axes</span><span class="p">,</span> <span class="n">b_axes</span> <span class="o">=</span> <span class="n">_tensordot_axes</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axes</span><span class="p">)</span>
    <span class="n">a_reshape</span><span class="p">,</span> <span class="n">a_free_dims</span><span class="p">,</span> <span class="n">a_free_dims_static</span> <span class="o">=</span> <span class="n">_tensordot_reshape</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">a_axes</span><span class="p">)</span>
    <span class="n">b_reshape</span><span class="p">,</span> <span class="n">b_free_dims</span><span class="p">,</span> <span class="n">b_free_dims_static</span> <span class="o">=</span> <span class="n">_tensordot_reshape</span><span class="p">(</span>
        <span class="n">b</span><span class="p">,</span> <span class="n">b_axes</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">ab_matmul</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">(</span><span class="n">a_reshape</span><span class="p">,</span> <span class="n">b_reshape</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a_free_dims</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">b_free_dims</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">ab_matmul</span><span class="p">,</span> <span class="n">a_free_dims</span> <span class="o">+</span> <span class="n">b_free_dims</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">a_free_dims</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">a_free_dims</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
      <span class="n">b_free_dims</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">b_free_dims</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
      <span class="n">product</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
          <span class="n">ab_matmul</span><span class="p">,</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">a_free_dims</span><span class="p">,</span> <span class="n">b_free_dims</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">a_free_dims_static</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">b_free_dims_static</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">product</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">a_free_dims_static</span> <span class="o">+</span> <span class="n">b_free_dims_static</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">product</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;math.polyval&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">polyval</span><span class="p">(</span><span class="n">coeffs</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the elementwise value of a polynomial.</span>

<span class="sd">  If `x` is a tensor and `coeffs` is a list n + 1 tensors, this function returns</span>
<span class="sd">  the value of the n-th order polynomial</span>

<span class="sd">     p(x) = coeffs[n-1] + coeffs[n-2] * x + ...  + coeffs[0] * x**(n-1)</span>

<span class="sd">  evaluated using Horner&#39;s method, i.e.</span>

<span class="sd">     p(x) = coeffs[n-1] + x * (coeffs[n-2] + ... + x * (coeffs[1] +</span>
<span class="sd">            x * coeffs[0]))</span>

<span class="sd">  Args:</span>
<span class="sd">    coeffs: A list of `Tensor` representing the coefficients of the polynomial.</span>
<span class="sd">    x: A `Tensor` representing the variable of the polynomial.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `tensor` of the shape as the expression p(x) with usual broadcasting rules</span>
<span class="sd">    for element-wise addition and multiplication applied.</span>

<span class="sd">  @compatibility(numpy)</span>
<span class="sd">  Equivalent to numpy.polyval.</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;polyval&quot;</span><span class="p">,</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">coeffs</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">coeffs</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="n">coeffs</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">coeff</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;coeff_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">index</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">coeff</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">coeffs</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">coeffs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">coeffs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
      <span class="n">p</span> <span class="o">=</span> <span class="n">c</span> <span class="o">+</span> <span class="n">p</span> <span class="o">*</span> <span class="n">x</span>
    <span class="k">return</span> <span class="n">p</span>
</pre></div>

    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
    </p>
    <p>
        &copy; Copyright Copyright 2018, zfit.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 2.1.0.<br/>
    </p>
  </div>
</footer>
  </body>
</html>