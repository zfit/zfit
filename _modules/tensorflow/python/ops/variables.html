<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta charset="utf-8" />
    <title>tensorflow.python.ops.variables &#8212; zfit 0.4.3.dev13+ge51e6aa documentation</title>
    <link rel="stylesheet" href="../../../../_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <script id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/language_data.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="../../../../_static/js/jquery-1.11.0.min.js "></script>
<script type="text/javascript" src="../../../../_static/js/jquery-fix.js "></script>
<script type="text/javascript" src="../../../../_static/bootstrap-2.3.2/js/bootstrap.min.js "></script>
<script type="text/javascript" src="../../../../_static/bootstrap-sphinx.js "></script>

  </head><body>

  <div id="navbar" class="navbar navbar-fixed-top">
    <div class="navbar-inner">
      <div class="container">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>

        <a class="brand" href="../../../../index.html"><img src="../../../../_static/zfit-fin_57x24.png">
          zfit</a>
        <span class="navbar-text pull-left"><b>0.4.3.dev13</b></span>

        <div class="nav-collapse">
          <ul class="nav">
            <li class="divider-vertical"></li>
            
                <li><a href="../../../../getting_started.html">Getting started</a></li>
                <li><a href="../../../../introduction.html">Intro</a></li>
                <li><a href="../../../../project.html">Project</a></li>
                <li><a href="../../../../API.html">API</a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../../../index.html">Overview <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../getting_started.html">Getting started with zfit</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../getting_started.html#what-did-just-happen">What did just happen?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../introduction.html">zfit introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../intro/space.html">Space, Observable and Range</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../intro/space.html#definitions">Definitions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../intro/space.html#limits">Limits</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../intro/space.html#defining-limits">Defining limits</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../intro/parameter.html">Parameter</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../intro/parameter.html#independent-parameter">Independent Parameter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../intro/parameter.html#dependent-parameter">Dependent Parameter</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../intro/model.html">Building a model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../intro/model.html#predefined-pdfs-and-basic-properties">Predefined PDFs and basic properties</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../intro/model.html#composite-pdf">Composite PDF</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../intro/model.html#extended-pdf">Extended PDF</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../intro/model.html#custom-pdf">Custom PDF</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../intro/model.html#sampling-from-a-model">Sampling from a Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../intro/model.html#tensor-sampling">Tensor sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../intro/model.html#playing-with-toys-multiple-samplings">Playing with toys: Multiple samplings</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../intro/data.html">Data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../intro/data.html#import-dataset-from-a-root-file">Import dataset from a ROOT file</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../intro/data.html#import-dataset-from-a-pandas-dataframe-or-numpy-ndarray">Import dataset from a pandas DataFrame or Numpy ndarray</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../intro/loss.html">Loss</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../intro/loss.html#adding-constraints">Adding constraints</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../intro/loss.html#simultaneous-fits">Simultaneous fits</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../intro/minimize.html">Minimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../intro/minimize.html#baseline-minimizers">Baseline minimizers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../project.html">zfit Project</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../project/installation.html">Installation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/installation.html#stable-release">Stable release</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/installation.html#from-sources">From sources</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../project/contributing.html">Contributing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/contributing.html#get-started">Get Started!</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/contributing.html#pull-request-guidelines">Pull Request Guidelines</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../project/upgrade_guide.html">Upgrade guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/upgrade_guide.html#upgrade-from-zfit-0-3-x-to-0-4-0">Upgrade from zfit 0.3.x to 0.4.0</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/upgrade_guide.html#dependents">Dependents</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../project/changelog.html">Changelog</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/changelog.html#develop">Develop</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#major-features-and-improvements">Major Features and Improvements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#behavioral-changes">Behavioral changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#bug-fixes-and-small-changes">Bug fixes and small changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#requirement-changes">Requirement changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#thanks">Thanks</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/changelog.html#id1">0.4.2 (27.2.2020)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id2">Major Features and Improvements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id3">Behavioral changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id4">Bug fixes and small changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id5">Requirement changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id6">Thanks</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/changelog.html#id7">0.4.1 (12.1.20)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id8">Major Features and Improvements</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/changelog.html#id9">0.4.0 (7.1.2020)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id10">Major Features and Improvements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id11">Behavioral changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id12">Bug fixes and small changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id13">Requirement changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id14">Thanks</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/changelog.html#id15">0.3.7 (6.12.19)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id16">Major Features and Improvements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id17">Behavioral changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id18">Bug fixes and small changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id19">Requirement changes</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/changelog.html#id20">0.3.6 (12.10.19)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id21">Major Features and Improvements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id22">Behavioral changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id23">Bug fixes and small changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id24">Requirement changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id25">Thanks</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/changelog.html#id26">0.3.4 (30-07-19)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id27">Major Features and Improvements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id28">Behavioral changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id29">Bug fixes and small changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id30">Thanks</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/changelog.html#id31">0.3.3 (15-05-19)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/changelog.html#id32">0.3.2 (01-05-19)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#breaking-changes">Breaking changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#bugfixes">Bugfixes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#improvements">Improvements</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/changelog.html#id33">0.3.1 (30-04-19)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/changelog.html#id34">0.3.0 (2019-03-20)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/changelog.html#id35">0.0.1 (2018-03-22)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../project/authors.html">Development Lead</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../project/authors.html#authors">Authors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../project/authors.html#contributors">Contributors</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../API.html">zfit API documentation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/zfit.html">zfit package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/zfit.html#subpackages">Subpackages</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.core.html">core</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/zfit.core.html#submodules">Submodules</a><ul>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.core.basefunc.html">basefunc</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.core.basemodel.html">basemodel</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.core.baseobject.html">baseobject</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.core.basepdf.html">basepdf</a><ul>
<li class="toctree-l7"><a class="reference internal" href="../../../../api/zfit.core.basepdf.html#defining-your-own-pdf">Defining your own pdf</a></li>
</ul>
</li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.core.constraint.html">constraint</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.core.data.html">data</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.core.dependents.html">dependents</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.core.dimension.html">dimension</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.core.integration.html">integration</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.core.interfaces.html">interfaces</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.core.limits.html">limits</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.core.loss.html">loss</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.core.operations.html">operations</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.core.parameter.html">parameter</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.core.sample.html">sample</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.core.testing.html">testing</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.minimizers.html">minimizers</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/zfit.minimizers.html#submodules">Submodules</a><ul>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.minimizers.base_tf.html">base_tf</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.minimizers.baseminimizer.html">baseminimizer</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.minimizers.fitresult.html">fitresult</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.minimizers.interface.html">interface</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.minimizers.minimizer_minuit.html">minimizer_minuit</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.minimizers.minimizer_tfp.html">minimizer_tfp</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.minimizers.minimizers_scipy.html">minimizers_scipy</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.minimizers.optimizers_tf.html">optimizers_tf</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.minimizers.tf_external_optimizer.html">tf_external_optimizer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.models.html">models</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/zfit.models.html#submodules">Submodules</a><ul>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.models.basefunctor.html">basefunctor</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.models.basic.html">basic</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.models.dist_tfp.html">dist_tfp</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.models.functions.html">functions</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.models.functor.html">functor</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.models.physics.html">physics</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.models.polynomials.html">polynomials</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.models.special.html">special</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.util.html">util</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/zfit.util.html#submodules">Submodules</a><ul>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.util.cache.html">cache</a><ul>
<li class="toctree-l7"><a class="reference internal" href="../../../../api/zfit.util.cache.html#basic-principle">Basic principle</a></li>
</ul>
</li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.util.checks.html">checks</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.util.container.html">container</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.util.diverse.html">diverse</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.util.exception.html">exception</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.util.execution.html">execution</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.util.graph.html">graph</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.util.logging.html">logging</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.util.temporary.html">temporary</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.util.ztyping.html">ztyping</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.z.html">z</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/zfit.z.html#submodules">Submodules</a><ul>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.z.const.html">const</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.z.math.html">math</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.z.random.html">random</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.z.tools.html">tools</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.z.wrapping_tf.html">wrapping_tf</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.z.zextension.html">zextension</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/zfit.html#submodules">Submodules</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.constraint.html">constraint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.data.html">data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.func.html">func</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.loss.html">loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.minimize.html">minimize</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.param.html">param</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.pdf.html">pdf</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.sample.html">sample</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.settings.html">settings</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/zfit.pdf.html">pdf</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/zfit.func.html">func</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/zfit.data.html">data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/zfit.loss.html">loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/zfit.constraint.html">constraint</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/zfit.minimize.html">minimize</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/zfit.minimizers.fitresult.html">fitresult</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/zfit.settings.html">settings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/zfit.z.html">z</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/zfit.z.html#submodules">Submodules</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.z.const.html">const</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.z.math.html">math</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.z.random.html">random</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.z.tools.html">tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.z.wrapping_tf.html">wrapping_tf</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.z.zextension.html">zextension</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"></ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
      </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body span12 content" role="main">
      
  <h1>Source code for tensorflow.python.ops.variables</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2015 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;Variable class.&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">enum</span>  <span class="c1"># pylint: disable=g-bad-import-order</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">six</span>

<span class="kn">from</span> <span class="nn">tensorflow.core.framework</span> <span class="kn">import</span> <span class="n">attr_value_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.core.framework</span> <span class="kn">import</span> <span class="n">variable_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.python</span> <span class="kn">import</span> <span class="n">pywrap_tensorflow</span>  <span class="c1"># pylint: disable=unused-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python</span> <span class="kn">import</span> <span class="n">_pywrap_utils</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">dtypes</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">tensor_shape</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">array_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">control_flow_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">gen_array_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">gen_state_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">gen_math_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">math_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">state_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.platform</span> <span class="kn">import</span> <span class="n">tf_logging</span> <span class="k">as</span> <span class="n">logging</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.training.tracking</span> <span class="kn">import</span> <span class="n">base</span> <span class="k">as</span> <span class="n">trackable</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="kn">import</span> <span class="n">compat</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="kn">import</span> <span class="n">object_identity</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="kn">import</span> <span class="n">tf_should_use</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util.deprecation</span> <span class="kn">import</span> <span class="n">deprecated</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util.deprecation</span> <span class="kn">import</span> <span class="n">deprecated_args</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util.tf_export</span> <span class="kn">import</span> <span class="n">tf_export</span>


<span class="k">def</span> <span class="nf">default_variable_creator</span><span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">):</span>
  <span class="k">del</span> <span class="n">kwds</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;variable_scope needs to be imported&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">default_variable_creator_v2</span><span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">):</span>
  <span class="k">del</span> <span class="n">kwds</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;variable_scope needs to be imported&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_make_getter</span><span class="p">(</span><span class="n">captured_getter</span><span class="p">,</span> <span class="n">captured_previous</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;To avoid capturing loop variables.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">getter</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">captured_getter</span><span class="p">(</span><span class="n">captured_previous</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">getter</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;VariableSynchronization&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">VariableSynchronization</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">Enum</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Indicates when a distributed variable will be synced.</span>

<span class="sd">  * `AUTO`: Indicates that the synchronization will be determined by the current</span>
<span class="sd">    `DistributionStrategy` (eg. With `MirroredStrategy` this would be</span>
<span class="sd">    `ON_WRITE`).</span>
<span class="sd">  * `NONE`: Indicates that there will only be one copy of the variable, so</span>
<span class="sd">    there is no need to sync.</span>
<span class="sd">  * `ON_WRITE`: Indicates that the variable will be updated across devices</span>
<span class="sd">    every time it is written.</span>
<span class="sd">  * `ON_READ`: Indicates that the variable will be aggregated across devices</span>
<span class="sd">    when it is read (eg. when checkpointing or when evaluating an op that uses</span>
<span class="sd">    the variable).</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">AUTO</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">NONE</span> <span class="o">=</span> <span class="mi">1</span>
  <span class="n">ON_WRITE</span> <span class="o">=</span> <span class="mi">2</span>
  <span class="n">ON_READ</span> <span class="o">=</span> <span class="mi">3</span>


<span class="c1"># LINT.IfChange</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;VariableAggregation&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">class</span> <span class="nc">VariableAggregationV2</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">Enum</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Indicates how a distributed variable will be aggregated.</span>

<span class="sd">  `tf.distribute.Strategy` distributes a model by making multiple copies</span>
<span class="sd">  (called &quot;replicas&quot;) acting data-parallel on different elements of the input</span>
<span class="sd">  batch. When performing some variable-update operation, say</span>
<span class="sd">  `var.assign_add(x)`, in a model, we need to resolve how to combine the</span>
<span class="sd">  different values for `x` computed in the different replicas.</span>

<span class="sd">  * `NONE`: This is the default, giving an error if you use a</span>
<span class="sd">    variable-update operation with multiple replicas.</span>
<span class="sd">  * `SUM`: Add the updates across replicas.</span>
<span class="sd">  * `MEAN`: Take the arithmetic mean (&quot;average&quot;) of the updates across replicas.</span>
<span class="sd">  * `ONLY_FIRST_REPLICA`: This is for when every replica is performing the same</span>
<span class="sd">    update, but we only want to perform the update once. Used, e.g., for the</span>
<span class="sd">    global step counter.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">NONE</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">SUM</span> <span class="o">=</span> <span class="mi">1</span>
  <span class="n">MEAN</span> <span class="o">=</span> <span class="mi">2</span>
  <span class="n">ONLY_FIRST_REPLICA</span> <span class="o">=</span> <span class="mi">3</span>

  <span class="k">def</span> <span class="fm">__hash__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">hash</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__eq__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span> <span class="ow">is</span> <span class="n">other</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">True</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">VariableAggregation</span><span class="p">):</span>
      <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">)</span> <span class="o">==</span> <span class="nb">int</span><span class="p">(</span><span class="n">other</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">False</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;VariableAggregation&quot;</span><span class="p">])</span>
<span class="k">class</span> <span class="nc">VariableAggregation</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">Enum</span><span class="p">):</span>
  <span class="n">NONE</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">SUM</span> <span class="o">=</span> <span class="mi">1</span>
  <span class="n">MEAN</span> <span class="o">=</span> <span class="mi">2</span>
  <span class="n">ONLY_FIRST_REPLICA</span> <span class="o">=</span> <span class="mi">3</span>
  <span class="n">ONLY_FIRST_TOWER</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># DEPRECATED</span>

  <span class="k">def</span> <span class="fm">__hash__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">hash</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>


<span class="c1"># LINT.ThenChange(//tensorflow/core/framework/variable.proto)</span>
<span class="c1">#</span>
<span class="c1"># Note that we are currently relying on the integer values of the Python enums</span>
<span class="c1"># matching the integer values of the proto enums.</span>

<span class="n">VariableAggregation</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">VariableAggregationV2</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">+</span>
    <span class="s2">&quot;* `ONLY_FIRST_TOWER`: Deprecated alias for `ONLY_FIRST_REPLICA`.</span><span class="se">\n</span><span class="s2">  &quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">validate_synchronization_aggregation_trainable</span><span class="p">(</span><span class="n">synchronization</span><span class="p">,</span> <span class="n">aggregation</span><span class="p">,</span>
                                                   <span class="n">trainable</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Given user-provided variable properties, sets defaults and validates.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">aggregation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">aggregation</span> <span class="o">=</span> <span class="n">VariableAggregation</span><span class="o">.</span><span class="n">NONE</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">aggregation</span><span class="p">,</span>
                      <span class="p">(</span><span class="n">VariableAggregation</span><span class="p">,</span> <span class="n">VariableAggregationV2</span><span class="p">)):</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="n">aggregation</span> <span class="o">=</span> <span class="n">VariableAggregationV2</span><span class="p">(</span><span class="n">aggregation</span><span class="p">)</span>
      <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Invalid variable aggregation mode: </span><span class="si">{}</span><span class="s2"> for variable: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">aggregation</span><span class="p">,</span> <span class="n">name</span><span class="p">))</span>
  <span class="k">if</span> <span class="n">synchronization</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">synchronization</span> <span class="o">=</span> <span class="n">VariableSynchronization</span><span class="o">.</span><span class="n">AUTO</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">synchronization</span> <span class="o">=</span> <span class="n">VariableSynchronization</span><span class="p">(</span><span class="n">synchronization</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Invalid variable synchronization mode: </span><span class="si">{}</span><span class="s2"> for variable: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
              <span class="n">synchronization</span><span class="p">,</span> <span class="n">name</span><span class="p">))</span>
  <span class="k">if</span> <span class="n">trainable</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">trainable</span> <span class="o">=</span> <span class="n">synchronization</span> <span class="o">!=</span> <span class="n">VariableSynchronization</span><span class="o">.</span><span class="n">ON_READ</span>
  <span class="k">return</span> <span class="n">synchronization</span><span class="p">,</span> <span class="n">aggregation</span><span class="p">,</span> <span class="n">trainable</span>


<span class="k">class</span> <span class="nc">VariableMetaclass</span><span class="p">(</span><span class="nb">type</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Metaclass to allow construction of tf.Variable to be overridden.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">_variable_v1_call</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span>
                        <span class="n">initial_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">collections</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">validate_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">caching_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">variable_def</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">expected_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">import_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">use_resource</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">synchronization</span><span class="o">=</span><span class="n">VariableSynchronization</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span>
                        <span class="n">aggregation</span><span class="o">=</span><span class="n">VariableAggregation</span><span class="o">.</span><span class="n">NONE</span><span class="p">,</span>
                        <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Call on Variable class. Useful to force the signature.&quot;&quot;&quot;</span>
    <span class="n">previous_getter</span> <span class="o">=</span> <span class="k">lambda</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">default_variable_creator</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">getter</span> <span class="ow">in</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">_variable_creator_stack</span><span class="p">:</span>  <span class="c1"># pylint: disable=protected-access</span>
      <span class="n">previous_getter</span> <span class="o">=</span> <span class="n">_make_getter</span><span class="p">(</span><span class="n">getter</span><span class="p">,</span> <span class="n">previous_getter</span><span class="p">)</span>

    <span class="c1"># Reset `aggregation` that is explicitly set as `None` to the enum NONE.</span>
    <span class="k">if</span> <span class="n">aggregation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">aggregation</span> <span class="o">=</span> <span class="n">VariableAggregation</span><span class="o">.</span><span class="n">NONE</span>
    <span class="k">return</span> <span class="n">previous_getter</span><span class="p">(</span>
        <span class="n">initial_value</span><span class="o">=</span><span class="n">initial_value</span><span class="p">,</span>
        <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="n">collections</span><span class="p">,</span>
        <span class="n">validate_shape</span><span class="o">=</span><span class="n">validate_shape</span><span class="p">,</span>
        <span class="n">caching_device</span><span class="o">=</span><span class="n">caching_device</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">variable_def</span><span class="o">=</span><span class="n">variable_def</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">expected_shape</span><span class="o">=</span><span class="n">expected_shape</span><span class="p">,</span>
        <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">,</span>
        <span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">,</span>
        <span class="n">use_resource</span><span class="o">=</span><span class="n">use_resource</span><span class="p">,</span>
        <span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span>
        <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_variable_v2_call</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span>
                        <span class="n">initial_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">validate_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">caching_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">variable_def</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">import_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">synchronization</span><span class="o">=</span><span class="n">VariableSynchronization</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span>
                        <span class="n">aggregation</span><span class="o">=</span><span class="n">VariableAggregation</span><span class="o">.</span><span class="n">NONE</span><span class="p">,</span>
                        <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Call on Variable class. Useful to force the signature.&quot;&quot;&quot;</span>
    <span class="n">previous_getter</span> <span class="o">=</span> <span class="k">lambda</span> <span class="o">**</span><span class="n">kws</span><span class="p">:</span> <span class="n">default_variable_creator_v2</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kws</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">getter</span> <span class="ow">in</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">_variable_creator_stack</span><span class="p">:</span>  <span class="c1"># pylint: disable=protected-access</span>
      <span class="n">previous_getter</span> <span class="o">=</span> <span class="n">_make_getter</span><span class="p">(</span><span class="n">getter</span><span class="p">,</span> <span class="n">previous_getter</span><span class="p">)</span>

    <span class="c1"># Reset `aggregation` that is explicitly set as `None` to the enum NONE.</span>
    <span class="k">if</span> <span class="n">aggregation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">aggregation</span> <span class="o">=</span> <span class="n">VariableAggregation</span><span class="o">.</span><span class="n">NONE</span>
    <span class="k">return</span> <span class="n">previous_getter</span><span class="p">(</span>
        <span class="n">initial_value</span><span class="o">=</span><span class="n">initial_value</span><span class="p">,</span>
        <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
        <span class="n">validate_shape</span><span class="o">=</span><span class="n">validate_shape</span><span class="p">,</span>
        <span class="n">caching_device</span><span class="o">=</span><span class="n">caching_device</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">variable_def</span><span class="o">=</span><span class="n">variable_def</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">,</span>
        <span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">,</span>
        <span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span>
        <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">cls</span> <span class="ow">is</span> <span class="n">VariableV1</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_variable_v1_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">cls</span> <span class="ow">is</span> <span class="n">Variable</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_variable_v2_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">VariableMetaclass</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="s2">&quot;Variable&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="o">=</span><span class="p">[])</span>
<span class="k">class</span> <span class="nc">Variable</span><span class="p">(</span><span class="n">six</span><span class="o">.</span><span class="n">with_metaclass</span><span class="p">(</span><span class="n">VariableMetaclass</span><span class="p">,</span> <span class="n">trackable</span><span class="o">.</span><span class="n">Trackable</span><span class="p">)):</span>
  <span class="sd">&quot;&quot;&quot;See the [variable guide](https://tensorflow.org/guide/variable).</span>

<span class="sd">  A variable maintains shared, persistent state manipulated by a program.</span>

<span class="sd">  The `Variable()` constructor requires an initial value for the variable, which</span>
<span class="sd">  can be a `Tensor` of any type and shape. This initial value defines the type</span>
<span class="sd">  and shape of the variable. After construction, the type and shape of the</span>
<span class="sd">  variable are fixed. The value can be changed using one of the assign methods.</span>

<span class="sd">  &gt;&gt;&gt; v = tf.Variable(1.)</span>
<span class="sd">  &gt;&gt;&gt; v.assign(2.)</span>
<span class="sd">  &lt;tf.Variable ... shape=() dtype=float32, numpy=2.0&gt;</span>
<span class="sd">  &gt;&gt;&gt; v.assign_add(0.5)</span>
<span class="sd">  &lt;tf.Variable ... shape=() dtype=float32, numpy=2.5&gt;</span>

<span class="sd">  The `shape` argument to `Variable`&#39;s constructor allows you to construct a</span>
<span class="sd">  variable with a less defined shape than its `initial_value`:</span>

<span class="sd">  &gt;&gt;&gt; v = tf.Variable(1., shape=tf.TensorShape(None))</span>
<span class="sd">  &gt;&gt;&gt; v.assign([[1.]])</span>
<span class="sd">  &lt;tf.Variable ... shape=&lt;unknown&gt; dtype=float32, numpy=array([[1.]], ...)&gt;</span>

<span class="sd">  Just like any `Tensor`, variables created with `Variable()` can be used as</span>
<span class="sd">  inputs to operations. Additionally, all the operators overloaded for the</span>
<span class="sd">  `Tensor` class are carried over to variables.</span>

<span class="sd">  &gt;&gt;&gt; w = tf.Variable([[1.], [2.]])</span>
<span class="sd">  &gt;&gt;&gt; x = tf.constant([[3., 4.]])</span>
<span class="sd">  &gt;&gt;&gt; tf.matmul(w, x)</span>
<span class="sd">  &lt;tf.Tensor:... shape=(2, 2), ... numpy=</span>
<span class="sd">    array([[3., 4.],</span>
<span class="sd">           [6., 8.]], dtype=float32)&gt;</span>
<span class="sd">  &gt;&gt;&gt; tf.sigmoid(w + x)</span>
<span class="sd">  &lt;tf.Tensor:... shape=(2, 2), ...&gt;</span>

<span class="sd">  When building a machine learning model it is often convenient to distinguish</span>
<span class="sd">  between variables holding trainable model parameters and other variables such</span>
<span class="sd">  as a `step` variable used to count training steps. To make this easier, the</span>
<span class="sd">  variable constructor supports a `trainable=&lt;bool&gt;`</span>
<span class="sd">  parameter. `tf.GradientTape` watches trainable variables by default:</span>

<span class="sd">  &gt;&gt;&gt; with tf.GradientTape(persistent=True) as tape:</span>
<span class="sd">  ...   trainable = tf.Variable(1.)</span>
<span class="sd">  ...   non_trainable = tf.Variable(2., trainable=False)</span>
<span class="sd">  ...   x1 = trainable * 2.</span>
<span class="sd">  ...   x2 = non_trainable * 3.</span>
<span class="sd">  &gt;&gt;&gt; tape.gradient(x1, trainable)</span>
<span class="sd">  &lt;tf.Tensor:... shape=(), dtype=float32, numpy=2.0&gt;</span>
<span class="sd">  &gt;&gt;&gt; assert tape.gradient(x2, non_trainable) is None  # Unwatched</span>

<span class="sd">  Variables are automatically tracked when assigned to attributes of types</span>
<span class="sd">  inheriting from `tf.Module`.</span>

<span class="sd">  &gt;&gt;&gt; m = tf.Module()</span>
<span class="sd">  &gt;&gt;&gt; m.v = tf.Variable([1.])</span>
<span class="sd">  &gt;&gt;&gt; m.trainable_variables</span>
<span class="sd">  (&lt;tf.Variable ... shape=(1,) ... numpy=array([1.], dtype=float32)&gt;,)</span>

<span class="sd">  This tracking then allows saving variable values to</span>
<span class="sd">  [training checkpoints](https://www.tensorflow.org/guide/checkpoint), or to</span>
<span class="sd">  [SavedModels](https://www.tensorflow.org/guide/saved_model) which include</span>
<span class="sd">  serialized TensorFlow graphs.</span>

<span class="sd">  Variables are often captured and manipulated by `tf.function`s. This works the</span>
<span class="sd">  same way the un-decorated function would have:</span>

<span class="sd">  &gt;&gt;&gt; v = tf.Variable(0.)</span>
<span class="sd">  &gt;&gt;&gt; read_and_decrement = tf.function(lambda: v.assign_sub(0.1))</span>
<span class="sd">  &gt;&gt;&gt; read_and_decrement()</span>
<span class="sd">  &lt;tf.Tensor: shape=(), dtype=float32, numpy=-0.1&gt;</span>
<span class="sd">  &gt;&gt;&gt; read_and_decrement()</span>
<span class="sd">  &lt;tf.Tensor: shape=(), dtype=float32, numpy=-0.2&gt;</span>

<span class="sd">  Variables created inside a `tf.function` must be owned outside the function</span>
<span class="sd">  and be created only once:</span>

<span class="sd">  &gt;&gt;&gt; class M(tf.Module):</span>
<span class="sd">  ...   @tf.function</span>
<span class="sd">  ...   def __call__(self, x):</span>
<span class="sd">  ...     if not hasattr(self, &quot;v&quot;):  # Or set self.v to None in __init__</span>
<span class="sd">  ...       self.v = tf.Variable(x)</span>
<span class="sd">  ...     return self.v * x</span>
<span class="sd">  &gt;&gt;&gt; m = M()</span>
<span class="sd">  &gt;&gt;&gt; m(2.)</span>
<span class="sd">  &lt;tf.Tensor: shape=(), dtype=float32, numpy=4.0&gt;</span>
<span class="sd">  &gt;&gt;&gt; m(3.)</span>
<span class="sd">  &lt;tf.Tensor: shape=(), dtype=float32, numpy=6.0&gt;</span>
<span class="sd">  &gt;&gt;&gt; m.v</span>
<span class="sd">  &lt;tf.Variable ... shape=() dtype=float32, numpy=2.0&gt;</span>

<span class="sd">  See the `tf.function` documentation for details.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="nd">@deprecated_args</span><span class="p">(</span>
      <span class="kc">None</span><span class="p">,</span>
      <span class="s2">&quot;A variable&#39;s value can be manually cached by calling &quot;</span>
      <span class="s2">&quot;tf.Variable.read_value() under a tf.device scope. The caching_device &quot;</span>
      <span class="s2">&quot;argument does not work properly.&quot;</span><span class="p">,</span>
      <span class="s2">&quot;caching_device&quot;</span><span class="p">)</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">initial_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">validate_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
               <span class="n">caching_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">variable_def</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">import_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">synchronization</span><span class="o">=</span><span class="n">VariableSynchronization</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span>
               <span class="n">aggregation</span><span class="o">=</span><span class="n">VariableAggregation</span><span class="o">.</span><span class="n">NONE</span><span class="p">,</span>
               <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a new variable with value `initial_value`.</span>

<span class="sd">    Args:</span>
<span class="sd">      initial_value: A `Tensor`, or Python object convertible to a `Tensor`,</span>
<span class="sd">        which is the initial value for the Variable. The initial value must have</span>
<span class="sd">        a shape specified unless `validate_shape` is set to False. Can also be a</span>
<span class="sd">        callable with no argument that returns the initial value when called. In</span>
<span class="sd">        that case, `dtype` must be specified. (Note that initializer functions</span>
<span class="sd">        from init_ops.py must first be bound to a shape before being used here.)</span>
<span class="sd">      trainable: If `True`, GradientTapes automatically watch uses of this</span>
<span class="sd">        variable. Defaults to `True`, unless `synchronization` is set to</span>
<span class="sd">        `ON_READ`, in which case it defaults to `False`.</span>
<span class="sd">      validate_shape: If `False`, allows the variable to be initialized with a</span>
<span class="sd">        value of unknown shape. If `True`, the default, the shape of</span>
<span class="sd">        `initial_value` must be known.</span>
<span class="sd">      caching_device: Optional device string describing where the Variable</span>
<span class="sd">        should be cached for reading.  Defaults to the Variable&#39;s device. If not</span>
<span class="sd">        `None`, caches on another device.  Typical use is to cache on the device</span>
<span class="sd">        where the Ops using the Variable reside, to deduplicate copying through</span>
<span class="sd">        `Switch` and other conditional statements.</span>
<span class="sd">      name: Optional name for the variable. Defaults to `&#39;Variable&#39;` and gets</span>
<span class="sd">        uniquified automatically.</span>
<span class="sd">      variable_def: `VariableDef` protocol buffer. If not `None`, recreates the</span>
<span class="sd">        Variable object with its contents, referencing the variable&#39;s nodes in</span>
<span class="sd">        the graph, which must already exist. The graph is not changed.</span>
<span class="sd">        `variable_def` and the other arguments are mutually exclusive.</span>
<span class="sd">      dtype: If set, initial_value will be converted to the given type. If</span>
<span class="sd">        `None`, either the datatype will be kept (if `initial_value` is a</span>
<span class="sd">        Tensor), or `convert_to_tensor` will decide.</span>
<span class="sd">      import_scope: Optional `string`. Name scope to add to the `Variable.` Only</span>
<span class="sd">        used when initializing from protocol buffer.</span>
<span class="sd">      constraint: An optional projection function to be applied to the variable</span>
<span class="sd">        after being updated by an `Optimizer` (e.g. used to implement norm</span>
<span class="sd">        constraints or value constraints for layer weights). The function must</span>
<span class="sd">        take as input the unprojected Tensor representing the value of the</span>
<span class="sd">        variable and return the Tensor for the projected value (which must have</span>
<span class="sd">        the same shape). Constraints are not safe to use when doing asynchronous</span>
<span class="sd">        distributed training.</span>
<span class="sd">      synchronization: Indicates when a distributed a variable will be</span>
<span class="sd">        aggregated. Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableSynchronization`. By default the synchronization is set to</span>
<span class="sd">        `AUTO` and the current `DistributionStrategy` chooses when to</span>
<span class="sd">        synchronize.</span>
<span class="sd">      aggregation: Indicates how a distributed variable will be aggregated.</span>
<span class="sd">        Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableAggregation`.</span>
<span class="sd">      shape: (optional) The shape of this variable. If None, the shape of</span>
<span class="sd">        `initial_value` will be used. When setting this argument to</span>
<span class="sd">        `tf.TensorShape(None)` (representing an unspecified shape), the variable</span>
<span class="sd">        can be assigned with values of different shapes.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If both `variable_def` and initial_value are specified.</span>
<span class="sd">      ValueError: If the initial value is not specified, or does not have a</span>
<span class="sd">        shape and `validate_shape` is `True`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="k">def</span> <span class="nf">value</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the last snapshot of this variable.</span>

<span class="sd">    You usually do not need to call this method as all ops that need the value</span>
<span class="sd">    of the variable call it automatically through a `convert_to_tensor()` call.</span>

<span class="sd">    Returns a `Tensor` which holds the value of the variable.  You can not</span>
<span class="sd">    assign a new value to this tensor as it is not a reference to the variable.</span>

<span class="sd">    To avoid copies, if the consumer of the returned value is on the same device</span>
<span class="sd">    as the variable, this actually returns the live value of the variable, not</span>
<span class="sd">    a copy.  Updates to the variable are seen by the consumer.  If the consumer</span>
<span class="sd">    is on a different device it will get a copy of the variable.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` containing the value of the variable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="k">def</span> <span class="nf">read_value</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the value of this variable, read in the current context.</span>

<span class="sd">    Can be different from value() if it&#39;s on another device, with control</span>
<span class="sd">    dependencies, etc.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` containing the value of the variable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="k">def</span> <span class="nf">set_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Overrides the shape for this variable.</span>

<span class="sd">    Args:</span>
<span class="sd">      shape: the `TensorShape` representing the overridden shape.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">trainable</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">synchronization</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">aggregation</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">session</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;In a session, computes and returns the value of this variable.</span>

<span class="sd">    This is not a graph construction method, it does not add ops to the graph.</span>

<span class="sd">    This convenience method requires a session where the graph</span>
<span class="sd">    containing this variable has been launched. If no session is</span>
<span class="sd">    passed, the default session is used.  See `tf.compat.v1.Session` for more</span>
<span class="sd">    information on launching a graph and on sessions.</span>

<span class="sd">    ```python</span>
<span class="sd">    v = tf.Variable([1, 2])</span>
<span class="sd">    init = tf.compat.v1.global_variables_initializer()</span>

<span class="sd">    with tf.compat.v1.Session() as sess:</span>
<span class="sd">        sess.run(init)</span>
<span class="sd">        # Usage passing the session explicitly.</span>
<span class="sd">        print(v.eval(sess))</span>
<span class="sd">        # Usage with the default session.  The &#39;with&#39; block</span>
<span class="sd">        # above makes &#39;sess&#39; the default session.</span>
<span class="sd">        print(v.eval())</span>
<span class="sd">    ```</span>

<span class="sd">    Args:</span>
<span class="sd">      session: The session to use to evaluate this variable. If none, the</span>
<span class="sd">        default session is used.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A numpy `ndarray` with a copy of the value of this variable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="nd">@deprecated</span><span class="p">(</span>
      <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Use Variable.read_value. Variables in 2.X are initialized &quot;</span>
      <span class="s2">&quot;automatically both in eager and graph (inside tf.defun) contexts.&quot;</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">initialized_value</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the value of the initialized variable.</span>

<span class="sd">    You should use this instead of the variable itself to initialize another</span>
<span class="sd">    variable with a value that depends on the value of this variable.</span>

<span class="sd">    ```python</span>
<span class="sd">    # Initialize &#39;v&#39; with a random tensor.</span>
<span class="sd">    v = tf.Variable(tf.random.truncated_normal([10, 40]))</span>
<span class="sd">    # Use `initialized_value` to guarantee that `v` has been</span>
<span class="sd">    # initialized before its value is used to initialize `w`.</span>
<span class="sd">    # The random values are picked only once.</span>
<span class="sd">    w = tf.Variable(v.initialized_value() * 2.0)</span>
<span class="sd">    ```</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` holding the value of this variable after its initializer</span>
<span class="sd">      has run.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">init_scope</span><span class="p">():</span>
      <span class="k">return</span> <span class="n">control_flow_ops</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span>
          <span class="n">is_variable_initialized</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">read_value</span><span class="p">,</span>
          <span class="k">lambda</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">initial_value</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">initial_value</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the Tensor used as the initial value for the variable.</span>

<span class="sd">    Note that this is different from `initialized_value()` which runs</span>
<span class="sd">    the op that initializes the variable before returning its value.</span>
<span class="sd">    This method returns the tensor that is used by the op that initializes</span>
<span class="sd">    the variable.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">constraint</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the constraint function associated with this variable.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The constraint function that was passed to the variable constructor.</span>
<span class="sd">      Can be `None` if no constraint was passed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="k">def</span> <span class="nf">assign</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">read_value</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Assigns a new value to the variable.</span>

<span class="sd">    This is essentially a shortcut for `assign(self, value)`.</span>

<span class="sd">    Args:</span>
<span class="sd">      value: A `Tensor`. The new value for this variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the assignment.</span>
<span class="sd">      name: The name of the operation to be created</span>
<span class="sd">      read_value: if True, will return something which evaluates to the new</span>
<span class="sd">        value of the variable; if False will return the assign op.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the assignment has completed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="k">def</span> <span class="nf">assign_add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">read_value</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Adds a value to this variable.</span>

<span class="sd">     This is essentially a shortcut for `assign_add(self, delta)`.</span>

<span class="sd">    Args:</span>
<span class="sd">      delta: A `Tensor`. The value to add to this variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: The name of the operation to be created</span>
<span class="sd">      read_value: if True, will return something which evaluates to the new</span>
<span class="sd">        value of the variable; if False will return the assign op.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the addition has completed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="k">def</span> <span class="nf">assign_sub</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">read_value</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Subtracts a value from this variable.</span>

<span class="sd">    This is essentially a shortcut for `assign_sub(self, delta)`.</span>

<span class="sd">    Args:</span>
<span class="sd">      delta: A `Tensor`. The value to subtract from this variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: The name of the operation to be created</span>
<span class="sd">      read_value: if True, will return something which evaluates to the new</span>
<span class="sd">        value of the variable; if False will return the assign op.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the subtraction has completed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="k">def</span> <span class="nf">scatter_sub</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Subtracts `tf.IndexedSlices` from this variable.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_delta: `tf.IndexedSlices` to be subtracted from this variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered subtraction has completed.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `sparse_delta` is not an `IndexedSlices`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="k">def</span> <span class="nf">scatter_add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Adds `tf.IndexedSlices` to this variable.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_delta: `tf.IndexedSlices` to be added to this variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered addition has completed.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `sparse_delta` is not an `IndexedSlices`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="k">def</span> <span class="nf">scatter_max</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Updates this variable with the max of `tf.IndexedSlices` and itself.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_delta: `tf.IndexedSlices` to use as an argument of max with this</span>
<span class="sd">        variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered maximization has completed.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `sparse_delta` is not an `IndexedSlices`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="k">def</span> <span class="nf">scatter_min</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Updates this variable with the min of `tf.IndexedSlices` and itself.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_delta: `tf.IndexedSlices` to use as an argument of min with this</span>
<span class="sd">        variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered minimization has completed.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `sparse_delta` is not an `IndexedSlices`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="k">def</span> <span class="nf">scatter_mul</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Multiply this variable by `tf.IndexedSlices`.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_delta: `tf.IndexedSlices` to multiply this variable by.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered multiplication has completed.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `sparse_delta` is not an `IndexedSlices`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="k">def</span> <span class="nf">scatter_div</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Divide this variable by `tf.IndexedSlices`.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_delta: `tf.IndexedSlices` to divide this variable by.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered division has completed.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `sparse_delta` is not an `IndexedSlices`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="k">def</span> <span class="nf">scatter_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Assigns `tf.IndexedSlices` to this variable.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_delta: `tf.IndexedSlices` to be assigned to this variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered assignment has completed.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `sparse_delta` is not an `IndexedSlices`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="k">def</span> <span class="nf">batch_scatter_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Assigns `tf.IndexedSlices` to this variable batch-wise.</span>

<span class="sd">    Analogous to `batch_gather`. This assumes that this variable and the</span>
<span class="sd">    sparse_delta IndexedSlices have a series of leading dimensions that are the</span>
<span class="sd">    same for all of them, and the updates are performed on the last dimension of</span>
<span class="sd">    indices. In other words, the dimensions should be the following:</span>

<span class="sd">    `num_prefix_dims = sparse_delta.indices.ndims - 1`</span>
<span class="sd">    `batch_dim = num_prefix_dims + 1`</span>
<span class="sd">    `sparse_delta.updates.shape = sparse_delta.indices.shape + var.shape[</span>
<span class="sd">         batch_dim:]`</span>

<span class="sd">    where</span>

<span class="sd">    `sparse_delta.updates.shape[:num_prefix_dims]`</span>
<span class="sd">    `== sparse_delta.indices.shape[:num_prefix_dims]`</span>
<span class="sd">    `== var.shape[:num_prefix_dims]`</span>

<span class="sd">    And the operation performed can be expressed as:</span>

<span class="sd">    `var[i_1, ..., i_n,</span>
<span class="sd">         sparse_delta.indices[i_1, ..., i_n, j]] = sparse_delta.updates[</span>
<span class="sd">            i_1, ..., i_n, j]`</span>

<span class="sd">    When sparse_delta.indices is a 1D tensor, this operation is equivalent to</span>
<span class="sd">    `scatter_update`.</span>

<span class="sd">    To avoid this operation one can looping over the first `ndims` of the</span>
<span class="sd">    variable and using `scatter_update` on the subtensors that result of slicing</span>
<span class="sd">    the first dimension. This is a valid option for `ndims = 1`, but less</span>
<span class="sd">    efficient than this implementation.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_delta: `tf.IndexedSlices` to be assigned to this variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered assignment has completed.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `sparse_delta` is not an `IndexedSlices`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="k">def</span> <span class="nf">scatter_nd_sub</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies sparse subtraction to individual values or slices in a Variable.</span>

<span class="sd">    Assuming the variable has rank `P` and `indices` is a `Tensor` of rank `Q`.</span>

<span class="sd">    `indices` must be integer tensor, containing indices into self.</span>
<span class="sd">    It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 &lt; K &lt;= P`.</span>

<span class="sd">    The innermost dimension of `indices` (with length `K`) corresponds to</span>
<span class="sd">    indices into elements (if `K = P`) or slices (if `K &lt; P`) along the `K`th</span>
<span class="sd">    dimension of self.</span>

<span class="sd">    `updates` is `Tensor` of rank `Q-1+P-K` with shape:</span>

<span class="sd">    ```</span>
<span class="sd">    [d_0, ..., d_{Q-2}, self.shape[K], ..., self.shape[P-1]].</span>
<span class="sd">    ```</span>

<span class="sd">    For example, say we want to add 4 scattered elements to a rank-1 tensor to</span>
<span class="sd">    8 elements. In Python, that update would look like this:</span>

<span class="sd">    ```python</span>
<span class="sd">        v = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])</span>
<span class="sd">        indices = tf.constant([[4], [3], [1] ,[7]])</span>
<span class="sd">        updates = tf.constant([9, 10, 11, 12])</span>
<span class="sd">        op = v.scatter_nd_sub(indices, updates)</span>
<span class="sd">        with tf.compat.v1.Session() as sess:</span>
<span class="sd">          print sess.run(op)</span>
<span class="sd">    ```</span>

<span class="sd">    The resulting update to v would look like this:</span>

<span class="sd">        [1, -9, 3, -6, -6, 6, 7, -4]</span>

<span class="sd">    See `tf.scatter_nd` for more details about how to make updates to</span>
<span class="sd">    slices.</span>

<span class="sd">    Args:</span>
<span class="sd">      indices: The indices to be used in the operation.</span>
<span class="sd">      updates: The values to be used in the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered subtraction has completed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="k">def</span> <span class="nf">scatter_nd_add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies sparse addition to individual values or slices in a Variable.</span>

<span class="sd">    The Variable has rank `P` and `indices` is a `Tensor` of rank `Q`.</span>

<span class="sd">    `indices` must be integer tensor, containing indices into self.</span>
<span class="sd">    It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 &lt; K &lt;= P`.</span>

<span class="sd">    The innermost dimension of `indices` (with length `K`) corresponds to</span>
<span class="sd">    indices into elements (if `K = P`) or slices (if `K &lt; P`) along the `K`th</span>
<span class="sd">    dimension of self.</span>

<span class="sd">    `updates` is `Tensor` of rank `Q-1+P-K` with shape:</span>

<span class="sd">    ```</span>
<span class="sd">    [d_0, ..., d_{Q-2}, self.shape[K], ..., self.shape[P-1]].</span>
<span class="sd">    ```</span>

<span class="sd">    For example, say we want to add 4 scattered elements to a rank-1 tensor to</span>
<span class="sd">    8 elements. In Python, that update would look like this:</span>

<span class="sd">    ```python</span>
<span class="sd">        v = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])</span>
<span class="sd">        indices = tf.constant([[4], [3], [1] ,[7]])</span>
<span class="sd">        updates = tf.constant([9, 10, 11, 12])</span>
<span class="sd">        add = v.scatter_nd_add(indices, updates)</span>
<span class="sd">        with tf.compat.v1.Session() as sess:</span>
<span class="sd">          print sess.run(add)</span>
<span class="sd">    ```</span>

<span class="sd">    The resulting update to v would look like this:</span>

<span class="sd">        [1, 13, 3, 14, 14, 6, 7, 20]</span>

<span class="sd">    See `tf.scatter_nd` for more details about how to make updates to</span>
<span class="sd">    slices.</span>

<span class="sd">    Args:</span>
<span class="sd">      indices: The indices to be used in the operation.</span>
<span class="sd">      updates: The values to be used in the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered addition has completed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="k">def</span> <span class="nf">scatter_nd_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies sparse assignment to individual values or slices in a Variable.</span>

<span class="sd">    The Variable has rank `P` and `indices` is a `Tensor` of rank `Q`.</span>

<span class="sd">    `indices` must be integer tensor, containing indices into self.</span>
<span class="sd">    It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 &lt; K &lt;= P`.</span>

<span class="sd">    The innermost dimension of `indices` (with length `K`) corresponds to</span>
<span class="sd">    indices into elements (if `K = P`) or slices (if `K &lt; P`) along the `K`th</span>
<span class="sd">    dimension of self.</span>

<span class="sd">    `updates` is `Tensor` of rank `Q-1+P-K` with shape:</span>

<span class="sd">    ```</span>
<span class="sd">    [d_0, ..., d_{Q-2}, self.shape[K], ..., self.shape[P-1]].</span>
<span class="sd">    ```</span>

<span class="sd">    For example, say we want to add 4 scattered elements to a rank-1 tensor to</span>
<span class="sd">    8 elements. In Python, that update would look like this:</span>

<span class="sd">    ```python</span>
<span class="sd">        v = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])</span>
<span class="sd">        indices = tf.constant([[4], [3], [1] ,[7]])</span>
<span class="sd">        updates = tf.constant([9, 10, 11, 12])</span>
<span class="sd">        op = v.scatter_nd_assign(indices, updates)</span>
<span class="sd">        with tf.compat.v1.Session() as sess:</span>
<span class="sd">          print sess.run(op)</span>
<span class="sd">    ```</span>

<span class="sd">    The resulting update to v would look like this:</span>

<span class="sd">        [1, 11, 3, 10, 9, 6, 7, 12]</span>

<span class="sd">    See `tf.scatter_nd` for more details about how to make updates to</span>
<span class="sd">    slices.</span>

<span class="sd">    Args:</span>
<span class="sd">      indices: The indices to be used in the operation.</span>
<span class="sd">      updates: The values to be used in the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered assignment has completed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="k">def</span> <span class="nf">sparse_read</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Gather slices from params axis axis according to indices.</span>

<span class="sd">    This function supports a subset of tf.gather, see tf.gather for details on</span>
<span class="sd">    usage.</span>

<span class="sd">    Args:</span>
<span class="sd">      indices: The index `Tensor`.  Must be one of the following types: `int32`,</span>
<span class="sd">        `int64`. Must be in range `[0, params.shape[axis])`.</span>
<span class="sd">      name: A name for the operation (optional).</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor`. Has the same type as `params`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">AttributeError</span>

  <span class="k">def</span> <span class="nf">gather_nd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Gather slices from `params` into a Tensor with shape specified by `indices`.</span>

<span class="sd">    See tf.gather_nd for details.</span>

<span class="sd">    Args:</span>
<span class="sd">      indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.</span>
<span class="sd">        Index tensor.</span>
<span class="sd">      name: A name for the operation (optional).</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor`. Has the same type as `params`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">AttributeError</span>

  <span class="nd">@deprecated</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Prefer Dataset.range instead.&quot;</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">count_up_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">limit</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Increments this variable until it reaches `limit`.</span>

<span class="sd">    When that Op is run it tries to increment the variable by `1`. If</span>
<span class="sd">    incrementing the variable would bring it above `limit` then the Op raises</span>
<span class="sd">    the exception `OutOfRangeError`.</span>

<span class="sd">    If no error is raised, the Op outputs the value of the variable before</span>
<span class="sd">    the increment.</span>

<span class="sd">    This is essentially a shortcut for `count_up_to(self, limit)`.</span>

<span class="sd">    Args:</span>
<span class="sd">      limit: value at which incrementing the variable raises an error.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the variable value before the increment. If no</span>
<span class="sd">      other Op modifies this variable, the values produced will all be</span>
<span class="sd">      distinct.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="nd">@deprecated</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span>
              <span class="s2">&quot;Prefer Variable.assign which has equivalent behavior in 2.X.&quot;</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">session</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Load new value into this variable.</span>

<span class="sd">    Writes new value to variable&#39;s memory. Doesn&#39;t add ops to the graph.</span>

<span class="sd">    This convenience method requires a session where the graph</span>
<span class="sd">    containing this variable has been launched. If no session is</span>
<span class="sd">    passed, the default session is used.  See `tf.compat.v1.Session` for more</span>
<span class="sd">    information on launching a graph and on sessions.</span>

<span class="sd">    ```python</span>
<span class="sd">    v = tf.Variable([1, 2])</span>
<span class="sd">    init = tf.compat.v1.global_variables_initializer()</span>

<span class="sd">    with tf.compat.v1.Session() as sess:</span>
<span class="sd">        sess.run(init)</span>
<span class="sd">        # Usage passing the session explicitly.</span>
<span class="sd">        v.load([2, 3], sess)</span>
<span class="sd">        print(v.eval(sess)) # prints [2 3]</span>
<span class="sd">        # Usage with the default session.  The &#39;with&#39; block</span>
<span class="sd">        # above makes &#39;sess&#39; the default session.</span>
<span class="sd">        v.load([3, 4], sess)</span>
<span class="sd">        print(v.eval()) # prints [3 4]</span>
<span class="sd">    ```</span>

<span class="sd">    Args:</span>
<span class="sd">        value: New variable value</span>
<span class="sd">        session: The session to use to evaluate this variable. If none, the</span>
<span class="sd">          default session is used.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: Session is not passed and no default session</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">session</span> <span class="o">=</span> <span class="n">session</span> <span class="ow">or</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_session</span><span class="p">()</span>
      <span class="k">if</span> <span class="n">session</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Either session argument should be provided or default session &quot;</span>
            <span class="s2">&quot;should be established&quot;</span><span class="p">)</span>
      <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">initializer</span><span class="p">,</span> <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">initializer</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span> <span class="n">value</span><span class="p">})</span>

  <span class="c1"># Conversion to tensor.</span>
  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">_TensorConversionFunction</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
    <span class="sd">&quot;&quot;&quot;Utility function for converting a Variable to a Tensor.&quot;&quot;&quot;</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">name</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Incompatible type conversion requested to type &#39;</span><span class="si">%s</span><span class="s2">&#39; for variable &quot;</span>
          <span class="s2">&quot;of type &#39;</span><span class="si">%s</span><span class="s2">&#39;&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">as_ref</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">v</span><span class="o">.</span><span class="n">_ref</span><span class="p">()</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">v</span><span class="o">.</span><span class="n">value</span><span class="p">()</span>

  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">_OverloadAllOperators</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
    <span class="sd">&quot;&quot;&quot;Register overloads for all operators.&quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">operator</span> <span class="ow">in</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">OVERLOADABLE_OPERATORS</span><span class="p">:</span>
      <span class="bp">cls</span><span class="o">.</span><span class="n">_OverloadOperator</span><span class="p">(</span><span class="n">operator</span><span class="p">)</span>
    <span class="c1"># For slicing, bind getitem differently than a tensor (use SliceHelperVar</span>
    <span class="c1"># instead)</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="nb">setattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&quot;__getitem__&quot;</span><span class="p">,</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">_SliceHelperVar</span><span class="p">)</span>

  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">_OverloadOperator</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">operator</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
    <span class="sd">&quot;&quot;&quot;Defer an operator overload to `ops.Tensor`.</span>

<span class="sd">    We pull the operator out of ops.Tensor dynamically to avoid ordering issues.</span>

<span class="sd">    Args:</span>
<span class="sd">      operator: string. The operator name.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># We can&#39;t use the overload mechanism on __eq__ &amp; __ne__ since __eq__ is</span>
    <span class="c1"># called when adding a variable to sets. As a result we call a.value() which</span>
    <span class="c1"># causes infinite recursion when operating within a GradientTape</span>
    <span class="c1"># TODO(gjn): Consider removing this</span>
    <span class="k">if</span> <span class="n">operator</span> <span class="o">==</span> <span class="s2">&quot;__eq__&quot;</span> <span class="ow">or</span> <span class="n">operator</span> <span class="o">==</span> <span class="s2">&quot;__ne__&quot;</span><span class="p">:</span>
      <span class="k">return</span>

    <span class="n">tensor_oper</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">operator</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_run_op</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
      <span class="c1"># pylint: disable=protected-access</span>
      <span class="k">return</span> <span class="n">tensor_oper</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">value</span><span class="p">(),</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="n">functools</span><span class="o">.</span><span class="n">update_wrapper</span><span class="p">(</span><span class="n">_run_op</span><span class="p">,</span> <span class="n">tensor_oper</span><span class="p">)</span>
    <span class="nb">setattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">operator</span><span class="p">,</span> <span class="n">_run_op</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__hash__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_USE_EQUALITY</span> <span class="ow">and</span> <span class="n">ops</span><span class="o">.</span><span class="n">executing_eagerly_outside_functions</span><span class="p">():</span>  <span class="c1"># pylint: disable=protected-access</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Variable is unhashable if Tensor equality is enabled. &quot;</span>
                      <span class="s2">&quot;Instead, use tensor.experimental_ref() as the key.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

  <span class="c1"># TODO(gjn): duplicate of math_ops.tensor_equals, consider removing</span>
  <span class="k">def</span> <span class="fm">__eq__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compares two variables element-wise for equality.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_USE_EQUALITY</span> <span class="ow">and</span> <span class="n">ops</span><span class="o">.</span><span class="n">executing_eagerly_outside_functions</span><span class="p">():</span>  <span class="c1"># pylint: disable=protected-access</span>
      <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="n">incompatible_shape_error</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># In legacy graph mode, tensor equality is object equality</span>
      <span class="k">return</span> <span class="bp">self</span> <span class="ow">is</span> <span class="n">other</span>

  <span class="c1"># TODO(gjn): duplicate of math_ops.tensor_not_equals, consider removing</span>
  <span class="k">def</span> <span class="fm">__ne__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compares two variables element-wise for equality.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_USE_EQUALITY</span> <span class="ow">and</span> <span class="n">ops</span><span class="o">.</span><span class="n">executing_eagerly_outside_functions</span><span class="p">():</span>  <span class="c1"># pylint: disable=protected-access</span>
      <span class="k">return</span> <span class="n">gen_math_ops</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="n">incompatible_shape_error</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># In legacy graph mode, tensor equality is object equality</span>
      <span class="k">return</span> <span class="bp">self</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">other</span>

  <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Dummy method to prevent iteration.</span>

<span class="sd">    Do not call.</span>

<span class="sd">    NOTE(mrry): If we register __getitem__ as an overloaded operator,</span>
<span class="sd">    Python will valiantly attempt to iterate over the variable&#39;s Tensor from 0</span>
<span class="sd">    to infinity.  Declaring this method prevents this unintended behavior.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: when invoked.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;&#39;Variable&#39; object is not iterable.&quot;</span><span class="p">)</span>

  <span class="c1"># NOTE(mrry): This enables the Variable&#39;s overloaded &quot;right&quot; binary</span>
  <span class="c1"># operators to run when the left operand is an ndarray, because it</span>
  <span class="c1"># accords the Variable class higher priority than an ndarray, or a</span>
  <span class="c1"># numpy matrix.</span>
  <span class="c1"># TODO(mrry): Convert this to using numpy&#39;s __numpy_ufunc__</span>
  <span class="c1"># mechanism, which allows more control over how Variables interact</span>
  <span class="c1"># with ndarrays.</span>
  <span class="n">__array_priority__</span> <span class="o">=</span> <span class="mi">100</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The name of this variable.&quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_shared_name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The shared name of the variable.</span>

<span class="sd">      Unlike name(), shared_name doesn&#39;t have &quot;:0&quot; suffix. It is user-specified</span>
<span class="sd">      name with name scope prefix.</span>

<span class="sd">    Returns:</span>
<span class="sd">      variable name.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;:&quot;</span><span class="p">)]</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">initializer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The initializer operation for this variable.&quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The device of this variable.&quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The `DType` of this variable.&quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">op</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The `Operation` of this variable.&quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">graph</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The `Graph` of this variable.&quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The `TensorShape` of this variable.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `TensorShape`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="k">def</span> <span class="nf">get_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Alias of `Variable.shape`.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span>

  <span class="k">def</span> <span class="nf">_gather_saveables_for_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;For implementing `Trackable`. This object is saveable on its own.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">{</span><span class="n">trackable</span><span class="o">.</span><span class="n">VARIABLE_VALUE_KEY</span><span class="p">:</span> <span class="bp">self</span><span class="p">}</span>

  <span class="k">def</span> <span class="nf">to_proto</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">export_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Converts a `Variable` to a `VariableDef` protocol buffer.</span>

<span class="sd">    Args:</span>
<span class="sd">      export_scope: Optional `string`. Name scope to remove.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `VariableDef` protocol buffer, or `None` if the `Variable` is not</span>
<span class="sd">      in the specified name scope.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">from_proto</span><span class="p">(</span><span class="n">variable_def</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a `Variable` object created from `variable_def`.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">RefVariable</span><span class="p">(</span><span class="n">variable_def</span><span class="o">=</span><span class="n">variable_def</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_set_save_slice_info</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_slice_info</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sets the slice info for this `Variable`.</span>

<span class="sd">    Args:</span>
<span class="sd">      save_slice_info: A `Variable.SaveSliceInfo` object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_save_slice_info</span> <span class="o">=</span> <span class="n">save_slice_info</span>

  <span class="k">def</span> <span class="nf">_get_save_slice_info</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_save_slice_info</span>

  <span class="k">def</span> <span class="nf">experimental_ref</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># tf.Tensor also has the same experimental_ref() API.  If you update the</span>
    <span class="c1"># documenation here, please update tf.Tensor.experimental_ref() as well.</span>
    <span class="sd">&quot;&quot;&quot;Returns a hashable reference object to this Variable.</span>

<span class="sd">    Warning: Experimental API that could be changed or removed.</span>

<span class="sd">    The primary usecase for this API is to put variables in a set/dictionary.</span>
<span class="sd">    We can&#39;t put variables in a set/dictionary as `variable.__hash__()` is no</span>
<span class="sd">    longer available starting Tensorflow 2.0.</span>

<span class="sd">    ```python</span>
<span class="sd">    import tensorflow as tf</span>

<span class="sd">    x = tf.Variable(5)</span>
<span class="sd">    y = tf.Variable(10)</span>
<span class="sd">    z = tf.Variable(10)</span>

<span class="sd">    # The followings will raise an exception starting 2.0</span>
<span class="sd">    # TypeError: Variable is unhashable if Variable equality is enabled.</span>
<span class="sd">    variable_set = {x, y, z}</span>
<span class="sd">    variable_dict = {x: &#39;five&#39;, y: &#39;ten&#39;}</span>
<span class="sd">    ```</span>

<span class="sd">    Instead, we can use `variable.experimental_ref()`.</span>

<span class="sd">    ```python</span>
<span class="sd">    variable_set = {x.experimental_ref(),</span>
<span class="sd">                    y.experimental_ref(),</span>
<span class="sd">                    z.experimental_ref()}</span>

<span class="sd">    print(x.experimental_ref() in variable_set)</span>
<span class="sd">    ==&gt; True</span>

<span class="sd">    variable_dict = {x.experimental_ref(): &#39;five&#39;,</span>
<span class="sd">                     y.experimental_ref(): &#39;ten&#39;,</span>
<span class="sd">                     z.experimental_ref(): &#39;ten&#39;}</span>

<span class="sd">    print(variable_dict[y.experimental_ref()])</span>
<span class="sd">    ==&gt; ten</span>
<span class="sd">    ```</span>

<span class="sd">    Also, the reference object provides `.deref()` function that returns the</span>
<span class="sd">    original Variable.</span>

<span class="sd">    ```python</span>
<span class="sd">    x = tf.Variable(5)</span>
<span class="sd">    print(x.experimental_ref().deref())</span>
<span class="sd">    ==&gt; &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=int32, numpy=5&gt;</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">object_identity</span><span class="o">.</span><span class="n">Reference</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

  <span class="k">class</span> <span class="nc">SaveSliceInfo</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Information on how to save this Variable as a slice.</span>

<span class="sd">    Provides internal support for saving variables as slices of a larger</span>
<span class="sd">    variable.  This API is not public and is subject to change.</span>

<span class="sd">    Available properties:</span>

<span class="sd">    * full_name</span>
<span class="sd">    * full_shape</span>
<span class="sd">    * var_offset</span>
<span class="sd">    * var_shape</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">full_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">full_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">var_offset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">var_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">save_slice_info_def</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">import_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Create a `SaveSliceInfo`.</span>

<span class="sd">      Args:</span>
<span class="sd">        full_name: Name of the full variable of which this `Variable` is a</span>
<span class="sd">          slice.</span>
<span class="sd">        full_shape: Shape of the full variable, as a list of int.</span>
<span class="sd">        var_offset: Offset of this `Variable` into the full variable, as a list</span>
<span class="sd">          of int.</span>
<span class="sd">        var_shape: Shape of this `Variable`, as a list of int.</span>
<span class="sd">        save_slice_info_def: `SaveSliceInfoDef` protocol buffer. If not `None`,</span>
<span class="sd">          recreates the SaveSliceInfo object its contents. `save_slice_info_def`</span>
<span class="sd">          and other arguments are mutually exclusive.</span>
<span class="sd">        import_scope: Optional `string`. Name scope to add. Only used when</span>
<span class="sd">          initializing from protocol buffer.</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="k">if</span> <span class="n">save_slice_info_def</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">save_slice_info_def</span><span class="p">,</span> <span class="n">variable_pb2</span><span class="o">.</span><span class="n">SaveSliceInfoDef</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">full_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">prepend_name_scope</span><span class="p">(</span>
            <span class="n">save_slice_info_def</span><span class="o">.</span><span class="n">full_name</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">full_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">save_slice_info_def</span><span class="o">.</span><span class="n">full_shape</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">var_offset</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">save_slice_info_def</span><span class="o">.</span><span class="n">var_offset</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">var_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">save_slice_info_def</span><span class="o">.</span><span class="n">var_shape</span><span class="p">]</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">full_name</span> <span class="o">=</span> <span class="n">full_name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">full_shape</span> <span class="o">=</span> <span class="n">full_shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">var_offset</span> <span class="o">=</span> <span class="n">var_offset</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">var_shape</span> <span class="o">=</span> <span class="n">var_shape</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">spec</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Computes the spec string used for saving.&quot;&quot;&quot;</span>
      <span class="n">full_shape_str</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="s2">&quot;</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">full_shape</span><span class="p">])</span> <span class="o">+</span> <span class="s2">&quot; &quot;</span>
      <span class="n">sl_spec</span> <span class="o">=</span> <span class="s2">&quot;:&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
          <span class="p">[</span><span class="s2">&quot;</span><span class="si">%d</span><span class="s2">,</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">o</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">var_offset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_shape</span><span class="p">)])</span>
      <span class="k">return</span> <span class="n">full_shape_str</span> <span class="o">+</span> <span class="n">sl_spec</span>

    <span class="k">def</span> <span class="nf">to_proto</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">export_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Returns a SaveSliceInfoDef() proto.</span>

<span class="sd">      Args:</span>
<span class="sd">        export_scope: Optional `string`. Name scope to remove.</span>

<span class="sd">      Returns:</span>
<span class="sd">        A `SaveSliceInfoDef` protocol buffer, or None if the `Variable` is not</span>
<span class="sd">        in the specified name scope.</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">export_scope</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">full_name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">export_scope</span><span class="p">)):</span>
        <span class="n">save_slice_info_def</span> <span class="o">=</span> <span class="n">variable_pb2</span><span class="o">.</span><span class="n">SaveSliceInfoDef</span><span class="p">()</span>
        <span class="n">save_slice_info_def</span><span class="o">.</span><span class="n">full_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">strip_name_scope</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">full_name</span><span class="p">,</span> <span class="n">export_scope</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">full_shape</span><span class="p">:</span>
          <span class="n">save_slice_info_def</span><span class="o">.</span><span class="n">full_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_offset</span><span class="p">:</span>
          <span class="n">save_slice_info_def</span><span class="o">.</span><span class="n">var_offset</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_shape</span><span class="p">:</span>
          <span class="n">save_slice_info_def</span><span class="o">.</span><span class="n">var_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">save_slice_info_def</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>


<span class="n">Variable</span><span class="o">.</span><span class="n">_OverloadAllOperators</span><span class="p">()</span>  <span class="c1"># pylint: disable=protected-access</span>
<span class="n">_pywrap_utils</span><span class="o">.</span><span class="n">RegisterType</span><span class="p">(</span><span class="s2">&quot;Variable&quot;</span><span class="p">,</span> <span class="n">Variable</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Variable&quot;</span><span class="p">])</span>
<span class="k">class</span> <span class="nc">VariableV1</span><span class="p">(</span><span class="n">Variable</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;See the [Variables Guide](https://tensorflow.org/guide/variables).</span>

<span class="sd">  A variable maintains state in the graph across calls to `run()`. You add a</span>
<span class="sd">  variable to the graph by constructing an instance of the class `Variable`.</span>

<span class="sd">  The `Variable()` constructor requires an initial value for the variable,</span>
<span class="sd">  which can be a `Tensor` of any type and shape. The initial value defines the</span>
<span class="sd">  type and shape of the variable. After construction, the type and shape of</span>
<span class="sd">  the variable are fixed. The value can be changed using one of the assign</span>
<span class="sd">  methods.</span>

<span class="sd">  If you want to change the shape of a variable later you have to use an</span>
<span class="sd">  `assign` Op with `validate_shape=False`.</span>

<span class="sd">  Just like any `Tensor`, variables created with `Variable()` can be used as</span>
<span class="sd">  inputs for other Ops in the graph. Additionally, all the operators</span>
<span class="sd">  overloaded for the `Tensor` class are carried over to variables, so you can</span>
<span class="sd">  also add nodes to the graph by just doing arithmetic on variables.</span>

<span class="sd">  ```python</span>
<span class="sd">  import tensorflow as tf</span>

<span class="sd">  # Create a variable.</span>
<span class="sd">  w = tf.Variable(&lt;initial-value&gt;, name=&lt;optional-name&gt;)</span>

<span class="sd">  # Use the variable in the graph like any Tensor.</span>
<span class="sd">  y = tf.matmul(w, ...another variable or tensor...)</span>

<span class="sd">  # The overloaded operators are available too.</span>
<span class="sd">  z = tf.sigmoid(w + y)</span>

<span class="sd">  # Assign a new value to the variable with `assign()` or a related method.</span>
<span class="sd">  w.assign(w + 1.0)</span>
<span class="sd">  w.assign_add(1.0)</span>
<span class="sd">  ```</span>

<span class="sd">  When you launch the graph, variables have to be explicitly initialized before</span>
<span class="sd">  you can run Ops that use their value. You can initialize a variable by</span>
<span class="sd">  running its *initializer op*, restoring the variable from a save file, or</span>
<span class="sd">  simply running an `assign` Op that assigns a value to the variable. In fact,</span>
<span class="sd">  the variable *initializer op* is just an `assign` Op that assigns the</span>
<span class="sd">  variable&#39;s initial value to the variable itself.</span>

<span class="sd">  ```python</span>
<span class="sd">  # Launch the graph in a session.</span>
<span class="sd">  with tf.compat.v1.Session() as sess:</span>
<span class="sd">      # Run the variable initializer.</span>
<span class="sd">      sess.run(w.initializer)</span>
<span class="sd">      # ...you now can run ops that use the value of &#39;w&#39;...</span>
<span class="sd">  ```</span>

<span class="sd">  The most common initialization pattern is to use the convenience function</span>
<span class="sd">  `global_variables_initializer()` to add an Op to the graph that initializes</span>
<span class="sd">  all the variables. You then run that Op after launching the graph.</span>

<span class="sd">  ```python</span>
<span class="sd">  # Add an Op to initialize global variables.</span>
<span class="sd">  init_op = tf.compat.v1.global_variables_initializer()</span>

<span class="sd">  # Launch the graph in a session.</span>
<span class="sd">  with tf.compat.v1.Session() as sess:</span>
<span class="sd">      # Run the Op that initializes global variables.</span>
<span class="sd">      sess.run(init_op)</span>
<span class="sd">      # ...you can now run any Op that uses variable values...</span>
<span class="sd">  ```</span>

<span class="sd">  If you need to create a variable with an initial value dependent on another</span>
<span class="sd">  variable, use the other variable&#39;s `initialized_value()`. This ensures that</span>
<span class="sd">  variables are initialized in the right order.</span>

<span class="sd">  All variables are automatically collected in the graph where they are</span>
<span class="sd">  created. By default, the constructor adds the new variable to the graph</span>
<span class="sd">  collection `GraphKeys.GLOBAL_VARIABLES`. The convenience function</span>
<span class="sd">  `global_variables()` returns the contents of that collection.</span>

<span class="sd">  When building a machine learning model it is often convenient to distinguish</span>
<span class="sd">  between variables holding the trainable model parameters and other variables</span>
<span class="sd">  such as a `global step` variable used to count training steps. To make this</span>
<span class="sd">  easier, the variable constructor supports a `trainable=&lt;bool&gt;` parameter. If</span>
<span class="sd">  `True`, the new variable is also added to the graph collection</span>
<span class="sd">  `GraphKeys.TRAINABLE_VARIABLES`. The convenience function</span>
<span class="sd">  `trainable_variables()` returns the contents of this collection. The</span>
<span class="sd">  various `Optimizer` classes use this collection as the default list of</span>
<span class="sd">  variables to optimize.</span>

<span class="sd">  WARNING: tf.Variable objects by default have a non-intuitive memory model. A</span>
<span class="sd">  Variable is represented internally as a mutable Tensor which can</span>
<span class="sd">  non-deterministically alias other Tensors in a graph. The set of operations</span>
<span class="sd">  which consume a Variable and can lead to aliasing is undetermined and can</span>
<span class="sd">  change across TensorFlow versions. Avoid writing code which relies on the</span>
<span class="sd">  value of a Variable either changing or not changing as other operations</span>
<span class="sd">  happen. For example, using Variable objects or simple functions thereof as</span>
<span class="sd">  predicates in a `tf.cond` is dangerous and error-prone:</span>

<span class="sd">  ```</span>
<span class="sd">  v = tf.Variable(True)</span>
<span class="sd">  tf.cond(v, lambda: v.assign(False), my_false_fn)  # Note: this is broken.</span>
<span class="sd">  ```</span>

<span class="sd">  Here, adding `use_resource=True` when constructing the variable will</span>
<span class="sd">  fix any nondeterminism issues:</span>
<span class="sd">  ```</span>
<span class="sd">  v = tf.Variable(True, use_resource=True)</span>
<span class="sd">  tf.cond(v, lambda: v.assign(False), my_false_fn)</span>
<span class="sd">  ```</span>

<span class="sd">  To use the replacement for variables which does</span>
<span class="sd">  not have these issues:</span>

<span class="sd">  * Add `use_resource=True` when constructing `tf.Variable`;</span>
<span class="sd">  * Call `tf.compat.v1.get_variable_scope().set_use_resource(True)` inside a</span>
<span class="sd">    `tf.compat.v1.variable_scope` before the `tf.compat.v1.get_variable()` call.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
      <span class="bp">self</span><span class="p">,</span>  <span class="c1"># pylint: disable=super-init-not-called</span>
      <span class="n">initial_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">collections</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">validate_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
      <span class="n">caching_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">variable_def</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">expected_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">import_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">use_resource</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">synchronization</span><span class="o">=</span><span class="n">VariableSynchronization</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span>
      <span class="n">aggregation</span><span class="o">=</span><span class="n">VariableAggregation</span><span class="o">.</span><span class="n">NONE</span><span class="p">,</span>
      <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a new variable with value `initial_value`.</span>

<span class="sd">    The new variable is added to the graph collections listed in `collections`,</span>
<span class="sd">    which defaults to `[GraphKeys.GLOBAL_VARIABLES]`.</span>

<span class="sd">    If `trainable` is `True` the variable is also added to the graph collection</span>
<span class="sd">    `GraphKeys.TRAINABLE_VARIABLES`.</span>

<span class="sd">    This constructor creates both a `variable` Op and an `assign` Op to set the</span>
<span class="sd">    variable to its initial value.</span>

<span class="sd">    Args:</span>
<span class="sd">      initial_value: A `Tensor`, or Python object convertible to a `Tensor`,</span>
<span class="sd">        which is the initial value for the Variable. The initial value must have</span>
<span class="sd">        a shape specified unless `validate_shape` is set to False. Can also be a</span>
<span class="sd">        callable with no argument that returns the initial value when called. In</span>
<span class="sd">        that case, `dtype` must be specified. (Note that initializer functions</span>
<span class="sd">        from init_ops.py must first be bound to a shape before being used here.)</span>
<span class="sd">      trainable: If `True`, also adds the variable to the graph collection</span>
<span class="sd">        `GraphKeys.TRAINABLE_VARIABLES`. This collection is used as the default</span>
<span class="sd">        list of variables to use by the `Optimizer` classes. Defaults to `True`,</span>
<span class="sd">        unless `synchronization` is set to `ON_READ`, in which case it defaults</span>
<span class="sd">        to `False`.</span>
<span class="sd">      collections: List of graph collections keys. The new variable is added to</span>
<span class="sd">        these collections. Defaults to `[GraphKeys.GLOBAL_VARIABLES]`.</span>
<span class="sd">      validate_shape: If `False`, allows the variable to be initialized with a</span>
<span class="sd">        value of unknown shape. If `True`, the default, the shape of</span>
<span class="sd">        `initial_value` must be known.</span>
<span class="sd">      caching_device: Optional device string describing where the Variable</span>
<span class="sd">        should be cached for reading.  Defaults to the Variable&#39;s device. If not</span>
<span class="sd">        `None`, caches on another device.  Typical use is to cache on the device</span>
<span class="sd">        where the Ops using the Variable reside, to deduplicate copying through</span>
<span class="sd">        `Switch` and other conditional statements.</span>
<span class="sd">      name: Optional name for the variable. Defaults to `&#39;Variable&#39;` and gets</span>
<span class="sd">        uniquified automatically.</span>
<span class="sd">      variable_def: `VariableDef` protocol buffer. If not `None`, recreates the</span>
<span class="sd">        Variable object with its contents, referencing the variable&#39;s nodes in</span>
<span class="sd">        the graph, which must already exist. The graph is not changed.</span>
<span class="sd">        `variable_def` and the other arguments are mutually exclusive.</span>
<span class="sd">      dtype: If set, initial_value will be converted to the given type. If</span>
<span class="sd">        `None`, either the datatype will be kept (if `initial_value` is a</span>
<span class="sd">        Tensor), or `convert_to_tensor` will decide.</span>
<span class="sd">      expected_shape: A TensorShape. If set, initial_value is expected to have</span>
<span class="sd">        this shape.</span>
<span class="sd">      import_scope: Optional `string`. Name scope to add to the `Variable.` Only</span>
<span class="sd">        used when initializing from protocol buffer.</span>
<span class="sd">      constraint: An optional projection function to be applied to the variable</span>
<span class="sd">        after being updated by an `Optimizer` (e.g. used to implement norm</span>
<span class="sd">        constraints or value constraints for layer weights). The function must</span>
<span class="sd">        take as input the unprojected Tensor representing the value of the</span>
<span class="sd">        variable and return the Tensor for the projected value (which must have</span>
<span class="sd">        the same shape). Constraints are not safe to use when doing asynchronous</span>
<span class="sd">        distributed training.</span>
<span class="sd">      use_resource: whether to use resource variables.</span>
<span class="sd">      synchronization: Indicates when a distributed a variable will be</span>
<span class="sd">        aggregated. Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableSynchronization`. By default the synchronization is set to</span>
<span class="sd">        `AUTO` and the current `DistributionStrategy` chooses when to</span>
<span class="sd">        synchronize.</span>
<span class="sd">      aggregation: Indicates how a distributed variable will be aggregated.</span>
<span class="sd">        Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableAggregation`.</span>
<span class="sd">      shape: (optional) The shape of this variable. If None, the shape of</span>
<span class="sd">        `initial_value` will be used. When setting this argument to</span>
<span class="sd">        `tf.TensorShape(None)` (representing an unspecified shape), the variable</span>
<span class="sd">        can be assigned with values of different shapes.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If both `variable_def` and initial_value are specified.</span>
<span class="sd">      ValueError: If the initial value is not specified, or does not have a</span>
<span class="sd">        shape and `validate_shape` is `True`.</span>
<span class="sd">      RuntimeError: If eager execution is enabled.</span>
<span class="sd">    &quot;&quot;&quot;</span>

  <span class="n">SaveSliceInfo</span> <span class="o">=</span> <span class="n">Variable</span><span class="o">.</span><span class="n">SaveSliceInfo</span>


<span class="c1"># TODO(apassos): do not repeat all comments here</span>
<span class="k">class</span> <span class="nc">RefVariable</span><span class="p">(</span><span class="n">VariableV1</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Ref-based implementation of variables.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
      <span class="bp">self</span><span class="p">,</span>  <span class="c1"># pylint: disable=super-init-not-called</span>
      <span class="n">initial_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">collections</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">validate_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
      <span class="n">caching_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">variable_def</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">expected_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">import_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">synchronization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">aggregation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a new variable with value `initial_value`.</span>

<span class="sd">    The new variable is added to the graph collections listed in `collections`,</span>
<span class="sd">    which defaults to `[GraphKeys.GLOBAL_VARIABLES]`.</span>

<span class="sd">    If `trainable` is `True` the variable is also added to the graph collection</span>
<span class="sd">    `GraphKeys.TRAINABLE_VARIABLES`.</span>

<span class="sd">    This constructor creates both a `variable` Op and an `assign` Op to set the</span>
<span class="sd">    variable to its initial value.</span>

<span class="sd">    Args:</span>
<span class="sd">      initial_value: A `Tensor`, or Python object convertible to a `Tensor`,</span>
<span class="sd">        which is the initial value for the Variable. The initial value must have</span>
<span class="sd">        a shape specified unless `validate_shape` is set to False. Can also be a</span>
<span class="sd">        callable with no argument that returns the initial value when called. In</span>
<span class="sd">        that case, `dtype` must be specified. (Note that initializer functions</span>
<span class="sd">        from init_ops.py must first be bound to a shape before being used here.)</span>
<span class="sd">      trainable: If `True`, also adds the variable to the graph collection</span>
<span class="sd">        `GraphKeys.TRAINABLE_VARIABLES`. This collection is used as the default</span>
<span class="sd">        list of variables to use by the `Optimizer` classes. Defaults to `True`,</span>
<span class="sd">        unless `synchronization` is set to `ON_READ`, in which case it defaults</span>
<span class="sd">        to `False`.</span>
<span class="sd">      collections: List of graph collections keys. The new variable is added to</span>
<span class="sd">        these collections. Defaults to `[GraphKeys.GLOBAL_VARIABLES]`.</span>
<span class="sd">      validate_shape: If `False`, allows the variable to be initialized with a</span>
<span class="sd">        value of unknown shape. If `True`, the default, the shape of</span>
<span class="sd">        `initial_value` must be known.</span>
<span class="sd">      caching_device: Optional device string describing where the Variable</span>
<span class="sd">        should be cached for reading.  Defaults to the Variable&#39;s device. If not</span>
<span class="sd">        `None`, caches on another device.  Typical use is to cache on the device</span>
<span class="sd">        where the Ops using the Variable reside, to deduplicate copying through</span>
<span class="sd">        `Switch` and other conditional statements.</span>
<span class="sd">      name: Optional name for the variable. Defaults to `&#39;Variable&#39;` and gets</span>
<span class="sd">        uniquified automatically.</span>
<span class="sd">      variable_def: `VariableDef` protocol buffer. If not `None`, recreates the</span>
<span class="sd">        Variable object with its contents, referencing the variable&#39;s nodes in</span>
<span class="sd">        the graph, which must already exist. The graph is not changed.</span>
<span class="sd">        `variable_def` and the other arguments are mutually exclusive.</span>
<span class="sd">      dtype: If set, initial_value will be converted to the given type. If</span>
<span class="sd">        `None`, either the datatype will be kept (if `initial_value` is a</span>
<span class="sd">        Tensor), or `convert_to_tensor` will decide.</span>
<span class="sd">      expected_shape: A TensorShape. If set, initial_value is expected to have</span>
<span class="sd">        this shape.</span>
<span class="sd">      import_scope: Optional `string`. Name scope to add to the `Variable.` Only</span>
<span class="sd">        used when initializing from protocol buffer.</span>
<span class="sd">      constraint: An optional projection function to be applied to the variable</span>
<span class="sd">        after being updated by an `Optimizer` (e.g. used to implement norm</span>
<span class="sd">        constraints or value constraints for layer weights). The function must</span>
<span class="sd">        take as input the unprojected Tensor representing the value of the</span>
<span class="sd">        variable and return the Tensor for the projected value (which must have</span>
<span class="sd">        the same shape). Constraints are not safe to use when doing asynchronous</span>
<span class="sd">        distributed training.</span>
<span class="sd">      synchronization: Indicates when a distributed a variable will be</span>
<span class="sd">        aggregated. Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableSynchronization`. By default the synchronization is set to</span>
<span class="sd">        `AUTO` and the current `DistributionStrategy` chooses when to</span>
<span class="sd">        synchronize.</span>
<span class="sd">      aggregation: Indicates how a distributed variable will be aggregated.</span>
<span class="sd">        Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableAggregation`.</span>
<span class="sd">      shape: (optional) The shape of this variable. If None, the shape of</span>
<span class="sd">        `initial_value` will be used. When setting this argument to</span>
<span class="sd">        `tf.TensorShape(None)` (representing an unspecified shape), the variable</span>
<span class="sd">        can be assigned with values of different shapes.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If both `variable_def` and initial_value are specified.</span>
<span class="sd">      ValueError: If the initial value is not specified, or does not have a</span>
<span class="sd">        shape and `validate_shape` is `True`.</span>
<span class="sd">      RuntimeError: If eager execution is enabled.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">if</span> <span class="n">variable_def</span><span class="p">:</span>
      <span class="c1"># If variable_def is provided, recreates the variable from its fields.</span>
      <span class="k">if</span> <span class="n">initial_value</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;variable_def and initial_value are mutually &quot;</span>
                         <span class="s2">&quot;exclusive.&quot;</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_init_from_proto</span><span class="p">(</span><span class="n">variable_def</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># Create from initial_value.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_init_from_args</span><span class="p">(</span>
          <span class="n">initial_value</span><span class="o">=</span><span class="n">initial_value</span><span class="p">,</span>
          <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="n">collections</span><span class="p">,</span>
          <span class="n">validate_shape</span><span class="o">=</span><span class="n">validate_shape</span><span class="p">,</span>
          <span class="n">caching_device</span><span class="o">=</span><span class="n">caching_device</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">expected_shape</span><span class="o">=</span><span class="n">expected_shape</span><span class="p">,</span>
          <span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">,</span>
          <span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span>
          <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">,</span>
          <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span><span class="p">:</span>
      <span class="k">return</span> <span class="s2">&quot;&lt;tf.Variable &#39;</span><span class="si">%s</span><span class="s2">&#39; shape=</span><span class="si">%s</span><span class="s2"> dtype=</span><span class="si">%s</span><span class="s2">, numpy=</span><span class="si">%s</span><span class="s2">&gt;&quot;</span> <span class="o">%</span> <span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_shape</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
          <span class="n">ops</span><span class="o">.</span><span class="n">numpy_text</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">read_value</span><span class="p">(),</span> <span class="n">is_repr</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="s2">&quot;&lt;tf.Variable &#39;</span><span class="si">%s</span><span class="s2">&#39; shape=</span><span class="si">%s</span><span class="s2"> dtype=</span><span class="si">%s</span><span class="s2">&gt;&quot;</span> <span class="o">%</span> <span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_shape</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_init_from_args</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                      <span class="n">initial_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">collections</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">validate_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                      <span class="n">caching_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">expected_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">synchronization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">aggregation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a new variable from arguments.</span>

<span class="sd">    Args:</span>
<span class="sd">      initial_value: A `Tensor`, or Python object convertible to a `Tensor`,</span>
<span class="sd">        which is the initial value for the Variable. The initial value must have</span>
<span class="sd">        a shape specified unless `validate_shape` is set to False. Can also be a</span>
<span class="sd">        callable with no argument that returns the initial value when called.</span>
<span class="sd">        (Note that initializer functions from init_ops.py must first be bound to</span>
<span class="sd">        a shape before being used here.)</span>
<span class="sd">      trainable: If `True`, also adds the variable to the graph collection</span>
<span class="sd">        `GraphKeys.TRAINABLE_VARIABLES`. This collection is used as the default</span>
<span class="sd">        list of variables to use by the `Optimizer` classes. Defaults to `True`,</span>
<span class="sd">        unless `synchronization` is set to `ON_READ`, in which case it defaults</span>
<span class="sd">        to `False`.</span>
<span class="sd">      collections: List of graph collections keys. The new variable is added to</span>
<span class="sd">        these collections. Defaults to `[GraphKeys.GLOBAL_VARIABLES]`.</span>
<span class="sd">      validate_shape: If `False`, allows the variable to be initialized with a</span>
<span class="sd">        value of unknown shape. If `True`, the default, the shape of</span>
<span class="sd">        `initial_value` must be known.</span>
<span class="sd">      caching_device: Optional device string or function describing where the</span>
<span class="sd">        Variable should be cached for reading.  Defaults to the Variable&#39;s</span>
<span class="sd">        device.  If not `None`, caches on another device.  Typical use is to</span>
<span class="sd">        cache on the device where the Ops using the Variable reside, to</span>
<span class="sd">        deduplicate copying through `Switch` and other conditional statements.</span>
<span class="sd">      name: Optional name for the variable. Defaults to `&#39;Variable&#39;` and gets</span>
<span class="sd">        uniquified automatically.</span>
<span class="sd">      dtype: If set, initial_value will be converted to the given type. If None,</span>
<span class="sd">        either the datatype will be kept (if initial_value is a Tensor) or</span>
<span class="sd">        float32 will be used (if it is a Python object convertible to a Tensor).</span>
<span class="sd">      expected_shape: Deprecated. Ignored.</span>
<span class="sd">      constraint: An optional projection function to be applied to the variable</span>
<span class="sd">        after being updated by an `Optimizer` (e.g. used to implement norm</span>
<span class="sd">        constraints or value constraints for layer weights). The function must</span>
<span class="sd">        take as input the unprojected Tensor representing the value of the</span>
<span class="sd">        variable and return the Tensor for the projected value (which must have</span>
<span class="sd">        the same shape). Constraints are not safe to use when doing asynchronous</span>
<span class="sd">        distributed training.</span>
<span class="sd">      synchronization: Indicates when a distributed a variable will be</span>
<span class="sd">        aggregated. Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableSynchronization`. By default the synchronization is set to</span>
<span class="sd">        `AUTO` and the current `DistributionStrategy` chooses when to</span>
<span class="sd">        synchronize.</span>
<span class="sd">      aggregation: Indicates how a distributed variable will be aggregated.</span>
<span class="sd">        Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableAggregation`.</span>
<span class="sd">      shape: (optional) The shape of this variable. If None, the shape of</span>
<span class="sd">        `initial_value` will be used. When setting this argument to</span>
<span class="sd">        `tf.TensorShape(None)` (representing an unspecified shape), the variable</span>
<span class="sd">        can be assigned with values of different shapes.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If the initial value is not specified, or does not have a</span>
<span class="sd">        shape and `validate_shape` is `True`.</span>
<span class="sd">      RuntimeError: If lifted into the eager context.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">expected_shape</span>
    <span class="k">if</span> <span class="n">initial_value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;initial_value must be specified.&quot;</span><span class="p">)</span>
    <span class="n">init_from_fn</span> <span class="o">=</span> <span class="n">callable</span><span class="p">(</span><span class="n">initial_value</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">collections</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">collections</span> <span class="o">=</span> <span class="p">[</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">collections</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">set</span><span class="p">)):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;collections argument to Variable constructor must be a list, tuple, &quot;</span>
          <span class="s2">&quot;or set. Got </span><span class="si">%s</span><span class="s2"> of type </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">collections</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">collections</span><span class="p">)))</span>
    <span class="k">if</span> <span class="n">constraint</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">constraint</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The `constraint` argument must be a callable.&quot;</span><span class="p">)</span>

    <span class="c1"># Store the graph key so optimizers know how to only retrieve variables from</span>
    <span class="c1"># this graph.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_graph_key</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">_graph_key</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initial_value</span><span class="p">,</span> <span class="n">trackable</span><span class="o">.</span><span class="n">CheckpointInitialValue</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_initialize_trackable</span><span class="p">()</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_update_uid</span> <span class="o">=</span> <span class="n">initial_value</span><span class="o">.</span><span class="n">checkpoint_position</span><span class="o">.</span><span class="n">restore_uid</span>
      <span class="n">initial_value</span> <span class="o">=</span> <span class="n">initial_value</span><span class="o">.</span><span class="n">wrapped_value</span>

    <span class="n">synchronization</span><span class="p">,</span> <span class="n">aggregation</span><span class="p">,</span> <span class="n">trainable</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">validate_synchronization_aggregation_trainable</span><span class="p">(</span><span class="n">synchronization</span><span class="p">,</span>
                                                       <span class="n">aggregation</span><span class="p">,</span> <span class="n">trainable</span><span class="p">,</span>
                                                       <span class="n">name</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_synchronization</span> <span class="o">=</span> <span class="n">synchronization</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_aggregation</span> <span class="o">=</span> <span class="n">aggregation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_trainable</span> <span class="o">=</span> <span class="n">trainable</span>
    <span class="k">if</span> <span class="n">trainable</span> <span class="ow">and</span> <span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">collections</span><span class="p">:</span>
      <span class="n">collections</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">collections</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span><span class="p">]</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">init_scope</span><span class="p">():</span>
      <span class="c1"># Ensure that we weren&#39;t lifted into the eager context.</span>
      <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;RefVariable not supported when eager execution is enabled. &quot;</span><span class="p">)</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Variable&quot;</span><span class="p">,</span>
                          <span class="p">[]</span> <span class="k">if</span> <span class="n">init_from_fn</span> <span class="k">else</span> <span class="p">[</span><span class="n">initial_value</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>

        <span class="k">if</span> <span class="n">init_from_fn</span><span class="p">:</span>
          <span class="c1"># Use attr_scope and device(None) to simulate the behavior of</span>
          <span class="c1"># colocate_with when the variable we want to colocate with doesn&#39;t</span>
          <span class="c1"># yet exist.</span>
          <span class="n">true_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_from_scope_name</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
          <span class="n">attr</span> <span class="o">=</span> <span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">AttrValue</span><span class="p">(</span>
              <span class="nb">list</span><span class="o">=</span><span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">AttrValue</span><span class="o">.</span><span class="n">ListValue</span><span class="p">(</span>
                  <span class="n">s</span><span class="o">=</span><span class="p">[</span><span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="s2">&quot;loc:@</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">true_name</span><span class="p">)]))</span>
          <span class="c1"># pylint: disable=protected-access</span>
          <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">_attr_scope</span><span class="p">({</span><span class="s2">&quot;_class&quot;</span><span class="p">:</span> <span class="n">attr</span><span class="p">}):</span>
            <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;Initializer&quot;</span><span class="p">),</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="kc">None</span><span class="p">):</span>
              <span class="bp">self</span><span class="o">.</span><span class="n">_initial_value</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
                  <span class="n">initial_value</span><span class="p">(),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;initial_value&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
              <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_initial_value</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">validate_shape</span> <span class="k">else</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">unknown_shape</span><span class="p">())</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_variable</span> <span class="o">=</span> <span class="n">state_ops</span><span class="o">.</span><span class="n">variable_op_v2</span><span class="p">(</span>
                <span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initial_value</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
          <span class="c1"># pylint: enable=protected-access</span>

        <span class="c1"># Or get the initial value from a Tensor or Python object.</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_initial_value</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
              <span class="n">initial_value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;initial_value&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
          <span class="c1"># pylint: disable=protected-access</span>
          <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initial_value</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">_get_control_flow_context</span><span class="p">()</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Initializer for variable </span><span class="si">%s</span><span class="s2"> is from inside a control-flow &quot;</span>
                <span class="s2">&quot;construct, such as a loop or conditional. When creating a &quot;</span>
                <span class="s2">&quot;variable inside a loop or conditional, use a lambda as the &quot;</span>
                <span class="s2">&quot;initializer.&quot;</span> <span class="o">%</span> <span class="n">name</span><span class="p">)</span>
          <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># pylint: enable=protected-access</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_initial_value</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">validate_shape</span> <span class="k">else</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">unknown_shape</span><span class="p">())</span>
          <span class="c1"># In this case, the variable op can&#39;t be created until after the</span>
          <span class="c1"># initial_value has been converted to a Tensor with a known type.</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_variable</span> <span class="o">=</span> <span class="n">state_ops</span><span class="o">.</span><span class="n">variable_op_v2</span><span class="p">(</span>
              <span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initial_value</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

        <span class="c1"># Cache the name in `self`, because some APIs call `Variable.name` in a</span>
        <span class="c1"># tight loop, and this halves the cost.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_variable</span><span class="o">.</span><span class="n">name</span>

        <span class="c1"># Manually overrides the variable&#39;s shape with the initial value&#39;s.</span>
        <span class="k">if</span> <span class="n">validate_shape</span><span class="p">:</span>
          <span class="n">initial_value_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initial_value</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span>
          <span class="k">if</span> <span class="ow">not</span> <span class="n">initial_value_shape</span><span class="o">.</span><span class="n">is_fully_defined</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;initial_value must have a shape specified: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
                             <span class="bp">self</span><span class="o">.</span><span class="n">_initial_value</span><span class="p">)</span>

        <span class="c1"># If &#39;initial_value&#39; makes use of other variables, make sure we don&#39;t</span>
        <span class="c1"># have an issue if these other variables aren&#39;t initialized first by</span>
        <span class="c1"># using their initialized_value() method.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initializer_op</span> <span class="o">=</span> <span class="n">state_ops</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_variable</span><span class="p">,</span>
            <span class="n">_try_guard_against_uninitialized_dependencies</span><span class="p">(</span>
                <span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initial_value</span><span class="p">),</span>
            <span class="n">validate_shape</span><span class="o">=</span><span class="n">validate_shape</span><span class="p">)</span><span class="o">.</span><span class="n">op</span>

        <span class="c1"># TODO(vrv): Change this class to not take caching_device, but</span>
        <span class="c1"># to take the op to colocate the snapshot with, so we can use</span>
        <span class="c1"># colocation rather than devices.</span>
        <span class="k">if</span> <span class="n">caching_device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
          <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">caching_device</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_snapshot</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_variable</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;read&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">colocate_with</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_variable</span><span class="o">.</span><span class="n">op</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_snapshot</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_variable</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;read&quot;</span><span class="p">)</span>
      <span class="n">ops</span><span class="o">.</span><span class="n">add_to_collections</span><span class="p">(</span><span class="n">collections</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_caching_device</span> <span class="o">=</span> <span class="n">caching_device</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_save_slice_info</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_constraint</span> <span class="o">=</span> <span class="n">constraint</span>

  <span class="k">def</span> <span class="nf">_init_from_proto</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">variable_def</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Recreates the Variable object from a `VariableDef` protocol buffer.</span>

<span class="sd">    Args:</span>
<span class="sd">      variable_def: `VariableDef` protocol buffer, describing a variable whose</span>
<span class="sd">        nodes already exists in the graph.</span>
<span class="sd">      import_scope: Optional `string`. Name scope to add.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">variable_def</span><span class="p">,</span> <span class="n">variable_pb2</span><span class="o">.</span><span class="n">VariableDef</span><span class="p">)</span>
    <span class="c1"># Create from variable_def.</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_variable</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">as_graph_element</span><span class="p">(</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">prepend_name_scope</span><span class="p">(</span>
            <span class="n">variable_def</span><span class="o">.</span><span class="n">variable_name</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_variable</span><span class="o">.</span><span class="n">name</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initializer_op</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">as_graph_element</span><span class="p">(</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">prepend_name_scope</span><span class="p">(</span>
            <span class="n">variable_def</span><span class="o">.</span><span class="n">initializer_name</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">))</span>
    <span class="c1"># Tests whether initial_value_name exists first for backwards compatibility.</span>
    <span class="k">if</span> <span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">variable_def</span><span class="p">,</span> <span class="s2">&quot;initial_value_name&quot;</span><span class="p">)</span> <span class="ow">and</span>
        <span class="n">variable_def</span><span class="o">.</span><span class="n">initial_value_name</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initial_value</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">as_graph_element</span><span class="p">(</span>
          <span class="n">ops</span><span class="o">.</span><span class="n">prepend_name_scope</span><span class="p">(</span>
              <span class="n">variable_def</span><span class="o">.</span><span class="n">initial_value_name</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initial_value</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">synchronization</span><span class="p">,</span> <span class="n">aggregation</span><span class="p">,</span> <span class="n">trainable</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">validate_synchronization_aggregation_trainable</span><span class="p">(</span>
            <span class="n">variable_def</span><span class="o">.</span><span class="n">synchronization</span><span class="p">,</span> <span class="n">variable_def</span><span class="o">.</span><span class="n">aggregation</span><span class="p">,</span>
            <span class="n">variable_def</span><span class="o">.</span><span class="n">trainable</span><span class="p">,</span> <span class="n">variable_def</span><span class="o">.</span><span class="n">variable_name</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_synchronization</span> <span class="o">=</span> <span class="n">synchronization</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_aggregation</span> <span class="o">=</span> <span class="n">aggregation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_trainable</span> <span class="o">=</span> <span class="n">trainable</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_snapshot</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">as_graph_element</span><span class="p">(</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">prepend_name_scope</span><span class="p">(</span>
            <span class="n">variable_def</span><span class="o">.</span><span class="n">snapshot_name</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">variable_def</span><span class="o">.</span><span class="n">HasField</span><span class="p">(</span><span class="s2">&quot;save_slice_info_def&quot;</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_save_slice_info</span> <span class="o">=</span> <span class="n">Variable</span><span class="o">.</span><span class="n">SaveSliceInfo</span><span class="p">(</span>
          <span class="n">save_slice_info_def</span><span class="o">=</span><span class="n">variable_def</span><span class="o">.</span><span class="n">save_slice_info_def</span><span class="p">,</span>
          <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_save_slice_info</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_caching_device</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_constraint</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="k">def</span> <span class="nf">_as_graph_element</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Conversion function for Graph.as_graph_element().&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_variable</span>

  <span class="k">def</span> <span class="nf">value</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the last snapshot of this variable.</span>

<span class="sd">    You usually do not need to call this method as all ops that need the value</span>
<span class="sd">    of the variable call it automatically through a `convert_to_tensor()` call.</span>

<span class="sd">    Returns a `Tensor` which holds the value of the variable.  You can not</span>
<span class="sd">    assign a new value to this tensor as it is not a reference to the variable.</span>

<span class="sd">    To avoid copies, if the consumer of the returned value is on the same device</span>
<span class="sd">    as the variable, this actually returns the live value of the variable, not</span>
<span class="sd">    a copy.  Updates to the variable are seen by the consumer.  If the consumer</span>
<span class="sd">    is on a different device it will get a copy of the variable.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` containing the value of the variable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_snapshot</span>

  <span class="k">def</span> <span class="nf">read_value</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the value of this variable, read in the current context.</span>

<span class="sd">    Can be different from value() if it&#39;s on another device, with control</span>
<span class="sd">    dependencies, etc.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` containing the value of the variable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_variable</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;read&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_ref</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a reference to this variable.</span>

<span class="sd">    You usually do not need to call this method as all ops that need a reference</span>
<span class="sd">    to the variable call it automatically.</span>

<span class="sd">    Returns is a `Tensor` which holds a reference to the variable.  You can</span>
<span class="sd">    assign a new value to the variable by passing the tensor to an assign op.</span>
<span class="sd">    See `tf.Variable.value` if you want to get the value of the</span>
<span class="sd">    variable.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that is a reference to the variable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_variable</span>

  <span class="k">def</span> <span class="nf">set_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Overrides the shape for this variable.</span>

<span class="sd">    Args:</span>
<span class="sd">      shape: the `TensorShape` representing the overridden shape.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ref</span><span class="p">()</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">()</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">trainable</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainable</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">synchronization</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_synchronization</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">aggregation</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_aggregation</span>

  <span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">session</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;In a session, computes and returns the value of this variable.</span>

<span class="sd">    This is not a graph construction method, it does not add ops to the graph.</span>

<span class="sd">    This convenience method requires a session where the graph</span>
<span class="sd">    containing this variable has been launched. If no session is</span>
<span class="sd">    passed, the default session is used.  See `tf.compat.v1.Session` for more</span>
<span class="sd">    information on launching a graph and on sessions.</span>

<span class="sd">    ```python</span>
<span class="sd">    v = tf.Variable([1, 2])</span>
<span class="sd">    init = tf.compat.v1.global_variables_initializer()</span>

<span class="sd">    with tf.compat.v1.Session() as sess:</span>
<span class="sd">        sess.run(init)</span>
<span class="sd">        # Usage passing the session explicitly.</span>
<span class="sd">        print(v.eval(sess))</span>
<span class="sd">        # Usage with the default session.  The &#39;with&#39; block</span>
<span class="sd">        # above makes &#39;sess&#39; the default session.</span>
<span class="sd">        print(v.eval())</span>
<span class="sd">    ```</span>

<span class="sd">    Args:</span>
<span class="sd">      session: The session to use to evaluate this variable. If none, the</span>
<span class="sd">        default session is used.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A numpy `ndarray` with a copy of the value of this variable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_variable</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">session</span><span class="o">=</span><span class="n">session</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">initial_value</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the Tensor used as the initial value for the variable.</span>

<span class="sd">    Note that this is different from `initialized_value()` which runs</span>
<span class="sd">    the op that initializes the variable before returning its value.</span>
<span class="sd">    This method returns the tensor that is used by the op that initializes</span>
<span class="sd">    the variable.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initial_value</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">constraint</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the constraint function associated with this variable.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The constraint function that was passed to the variable constructor.</span>
<span class="sd">      Can be `None` if no constraint was passed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_constraint</span>

  <span class="k">def</span> <span class="nf">assign</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">read_value</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Assigns a new value to the variable.</span>

<span class="sd">    This is essentially a shortcut for `assign(self, value)`.</span>

<span class="sd">    Args:</span>
<span class="sd">      value: A `Tensor`. The new value for this variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the assignment.</span>
<span class="sd">      name: The name of the operation to be created</span>
<span class="sd">      read_value: if True, will return something which evaluates to the new</span>
<span class="sd">        value of the variable; if False will return the assign op.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the assignment has completed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">assign</span> <span class="o">=</span> <span class="n">state_ops</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_variable</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="n">use_locking</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">read_value</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">assign</span>
    <span class="k">return</span> <span class="n">assign</span><span class="o">.</span><span class="n">op</span>

  <span class="k">def</span> <span class="nf">assign_add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">read_value</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Adds a value to this variable.</span>

<span class="sd">     This is essentially a shortcut for `assign_add(self, delta)`.</span>

<span class="sd">    Args:</span>
<span class="sd">      delta: A `Tensor`. The value to add to this variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: The name of the operation to be created</span>
<span class="sd">      read_value: if True, will return something which evaluates to the new</span>
<span class="sd">        value of the variable; if False will return the assign op.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the addition has completed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">assign</span> <span class="o">=</span> <span class="n">state_ops</span><span class="o">.</span><span class="n">assign_add</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_variable</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="n">use_locking</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">read_value</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">assign</span>
    <span class="k">return</span> <span class="n">assign</span><span class="o">.</span><span class="n">op</span>

  <span class="k">def</span> <span class="nf">assign_sub</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">read_value</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Subtracts a value from this variable.</span>

<span class="sd">    This is essentially a shortcut for `assign_sub(self, delta)`.</span>

<span class="sd">    Args:</span>
<span class="sd">      delta: A `Tensor`. The value to subtract from this variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: The name of the operation to be created</span>
<span class="sd">      read_value: if True, will return something which evaluates to the new</span>
<span class="sd">        value of the variable; if False will return the assign op.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the subtraction has completed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">assign</span> <span class="o">=</span> <span class="n">state_ops</span><span class="o">.</span><span class="n">assign_sub</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_variable</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="n">use_locking</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">read_value</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">assign</span>
    <span class="k">return</span> <span class="n">assign</span><span class="o">.</span><span class="n">op</span>

  <span class="k">def</span> <span class="nf">scatter_sub</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Subtracts `tf.IndexedSlices` from this variable.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_delta: `tf.IndexedSlices` to be subtracted from this variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered subtraction has completed.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `sparse_delta` is not an `IndexedSlices`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sparse_delta</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;sparse_delta is not IndexedSlices: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">sparse_delta</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gen_state_ops</span><span class="o">.</span><span class="n">scatter_sub</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_variable</span><span class="p">,</span>
        <span class="n">sparse_delta</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
        <span class="n">sparse_delta</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
        <span class="n">use_locking</span><span class="o">=</span><span class="n">use_locking</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">scatter_add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Adds `tf.IndexedSlices` to this variable.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_delta: `tf.IndexedSlices` to be added to this variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered addition has completed.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `sparse_delta` is not an `IndexedSlices`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sparse_delta</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;sparse_delta is not IndexedSlices: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">sparse_delta</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gen_state_ops</span><span class="o">.</span><span class="n">scatter_add</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_variable</span><span class="p">,</span>
        <span class="n">sparse_delta</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
        <span class="n">sparse_delta</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
        <span class="n">use_locking</span><span class="o">=</span><span class="n">use_locking</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">scatter_max</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Updates this variable with the max of `tf.IndexedSlices` and itself.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_delta: `tf.IndexedSlices` to use as an argument of max with this</span>
<span class="sd">        variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered maximization has completed.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `sparse_delta` is not an `IndexedSlices`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sparse_delta</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;sparse_delta is not IndexedSlices: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">sparse_delta</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gen_state_ops</span><span class="o">.</span><span class="n">scatter_max</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_variable</span><span class="p">,</span>
        <span class="n">sparse_delta</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
        <span class="n">sparse_delta</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
        <span class="n">use_locking</span><span class="o">=</span><span class="n">use_locking</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">scatter_min</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Updates this variable with the min of `tf.IndexedSlices` and itself.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_delta: `tf.IndexedSlices` to use as an argument of min with this</span>
<span class="sd">        variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered minimization has completed.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `sparse_delta` is not an `IndexedSlices`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sparse_delta</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;sparse_delta is not IndexedSlices: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">sparse_delta</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gen_state_ops</span><span class="o">.</span><span class="n">scatter_min</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_variable</span><span class="p">,</span>
        <span class="n">sparse_delta</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
        <span class="n">sparse_delta</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
        <span class="n">use_locking</span><span class="o">=</span><span class="n">use_locking</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">scatter_mul</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Multiply this variable by `tf.IndexedSlices`.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_delta: `tf.IndexedSlices` to multiply this variable by.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered multiplication has completed.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `sparse_delta` is not an `IndexedSlices`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sparse_delta</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;sparse_delta is not IndexedSlices: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">sparse_delta</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gen_state_ops</span><span class="o">.</span><span class="n">scatter_mul</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_variable</span><span class="p">,</span>
        <span class="n">sparse_delta</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
        <span class="n">sparse_delta</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
        <span class="n">use_locking</span><span class="o">=</span><span class="n">use_locking</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">scatter_div</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Divide this variable by `tf.IndexedSlices`.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_delta: `tf.IndexedSlices` to divide this variable by.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered division has completed.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `sparse_delta` is not an `IndexedSlices`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sparse_delta</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;sparse_delta is not IndexedSlices: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">sparse_delta</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gen_state_ops</span><span class="o">.</span><span class="n">scatter_div</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_variable</span><span class="p">,</span>
        <span class="n">sparse_delta</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
        <span class="n">sparse_delta</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
        <span class="n">use_locking</span><span class="o">=</span><span class="n">use_locking</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">scatter_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Assigns `tf.IndexedSlices` to this variable.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_delta: `tf.IndexedSlices` to be assigned to this variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered assignment has completed.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `sparse_delta` is not an `IndexedSlices`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sparse_delta</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;sparse_delta is not IndexedSlices: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">sparse_delta</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gen_state_ops</span><span class="o">.</span><span class="n">scatter_update</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_variable</span><span class="p">,</span>
        <span class="n">sparse_delta</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
        <span class="n">sparse_delta</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
        <span class="n">use_locking</span><span class="o">=</span><span class="n">use_locking</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">batch_scatter_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Assigns `tf.IndexedSlices` to this variable batch-wise.</span>

<span class="sd">    Analogous to `batch_gather`. This assumes that this variable and the</span>
<span class="sd">    sparse_delta IndexedSlices have a series of leading dimensions that are the</span>
<span class="sd">    same for all of them, and the updates are performed on the last dimension of</span>
<span class="sd">    indices. In other words, the dimensions should be the following:</span>

<span class="sd">    `num_prefix_dims = sparse_delta.indices.ndims - 1`</span>
<span class="sd">    `batch_dim = num_prefix_dims + 1`</span>
<span class="sd">    `sparse_delta.updates.shape = sparse_delta.indices.shape + var.shape[</span>
<span class="sd">         batch_dim:]`</span>

<span class="sd">    where</span>

<span class="sd">    `sparse_delta.updates.shape[:num_prefix_dims]`</span>
<span class="sd">    `== sparse_delta.indices.shape[:num_prefix_dims]`</span>
<span class="sd">    `== var.shape[:num_prefix_dims]`</span>

<span class="sd">    And the operation performed can be expressed as:</span>

<span class="sd">    `var[i_1, ..., i_n,</span>
<span class="sd">         sparse_delta.indices[i_1, ..., i_n, j]] = sparse_delta.updates[</span>
<span class="sd">            i_1, ..., i_n, j]`</span>

<span class="sd">    When sparse_delta.indices is a 1D tensor, this operation is equivalent to</span>
<span class="sd">    `scatter_update`.</span>

<span class="sd">    To avoid this operation one can looping over the first `ndims` of the</span>
<span class="sd">    variable and using `scatter_update` on the subtensors that result of slicing</span>
<span class="sd">    the first dimension. This is a valid option for `ndims = 1`, but less</span>
<span class="sd">    efficient than this implementation.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_delta: `tf.IndexedSlices` to be assigned to this variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered assignment has completed.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `sparse_delta` is not an `IndexedSlices`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">state_ops</span><span class="o">.</span><span class="n">batch_scatter_update</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sparse_delta</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
        <span class="n">sparse_delta</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
        <span class="n">use_locking</span><span class="o">=</span><span class="n">use_locking</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">scatter_nd_sub</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies sparse subtraction to individual values or slices in a Variable.</span>

<span class="sd">    `ref` is a `Tensor` with rank `P` and `indices` is a `Tensor` of rank `Q`.</span>

<span class="sd">    `indices` must be integer tensor, containing indices into `ref`.</span>
<span class="sd">    It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 &lt; K &lt;= P`.</span>

<span class="sd">    The innermost dimension of `indices` (with length `K`) corresponds to</span>
<span class="sd">    indices into elements (if `K = P`) or slices (if `K &lt; P`) along the `K`th</span>
<span class="sd">    dimension of `ref`.</span>

<span class="sd">    `updates` is `Tensor` of rank `Q-1+P-K` with shape:</span>

<span class="sd">    ```</span>
<span class="sd">    [d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]].</span>
<span class="sd">    ```</span>

<span class="sd">    For example, say we want to add 4 scattered elements to a rank-1 tensor to</span>
<span class="sd">    8 elements. In Python, that update would look like this:</span>

<span class="sd">    ```python</span>
<span class="sd">        ref = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])</span>
<span class="sd">        indices = tf.constant([[4], [3], [1] ,[7]])</span>
<span class="sd">        updates = tf.constant([9, 10, 11, 12])</span>
<span class="sd">        op = ref.scatter_nd_sub(indices, updates)</span>
<span class="sd">        with tf.compat.v1.Session() as sess:</span>
<span class="sd">          print sess.run(op)</span>
<span class="sd">    ```</span>

<span class="sd">    The resulting update to ref would look like this:</span>

<span class="sd">        [1, -9, 3, -6, -6, 6, 7, -4]</span>

<span class="sd">    See `tf.scatter_nd` for more details about how to make updates to</span>
<span class="sd">    slices.</span>

<span class="sd">    Args:</span>
<span class="sd">      indices: The indices to be used in the operation.</span>
<span class="sd">      updates: The values to be used in the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered subtraction has completed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">gen_state_ops</span><span class="o">.</span><span class="n">scatter_nd_sub</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_variable</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">scatter_nd_add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies sparse addition to individual values or slices in a Variable.</span>

<span class="sd">    `ref` is a `Tensor` with rank `P` and `indices` is a `Tensor` of rank `Q`.</span>

<span class="sd">    `indices` must be integer tensor, containing indices into `ref`.</span>
<span class="sd">    It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 &lt; K &lt;= P`.</span>

<span class="sd">    The innermost dimension of `indices` (with length `K`) corresponds to</span>
<span class="sd">    indices into elements (if `K = P`) or slices (if `K &lt; P`) along the `K`th</span>
<span class="sd">    dimension of `ref`.</span>

<span class="sd">    `updates` is `Tensor` of rank `Q-1+P-K` with shape:</span>

<span class="sd">    ```</span>
<span class="sd">    [d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]].</span>
<span class="sd">    ```</span>

<span class="sd">    For example, say we want to add 4 scattered elements to a rank-1 tensor to</span>
<span class="sd">    8 elements. In Python, that update would look like this:</span>

<span class="sd">    ```python</span>
<span class="sd">        ref = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])</span>
<span class="sd">        indices = tf.constant([[4], [3], [1] ,[7]])</span>
<span class="sd">        updates = tf.constant([9, 10, 11, 12])</span>
<span class="sd">        add = ref.scatter_nd_add(indices, updates)</span>
<span class="sd">        with tf.compat.v1.Session() as sess:</span>
<span class="sd">          print sess.run(add)</span>
<span class="sd">    ```</span>

<span class="sd">    The resulting update to ref would look like this:</span>

<span class="sd">        [1, 13, 3, 14, 14, 6, 7, 20]</span>

<span class="sd">    See `tf.scatter_nd` for more details about how to make updates to</span>
<span class="sd">    slices.</span>

<span class="sd">    Args:</span>
<span class="sd">      indices: The indices to be used in the operation.</span>
<span class="sd">      updates: The values to be used in the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered addition has completed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">gen_state_ops</span><span class="o">.</span><span class="n">scatter_nd_add</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_variable</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">scatter_nd_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies sparse assignment to individual values or slices in a Variable.</span>

<span class="sd">    `ref` is a `Tensor` with rank `P` and `indices` is a `Tensor` of rank `Q`.</span>

<span class="sd">    `indices` must be integer tensor, containing indices into `ref`.</span>
<span class="sd">    It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 &lt; K &lt;= P`.</span>

<span class="sd">    The innermost dimension of `indices` (with length `K`) corresponds to</span>
<span class="sd">    indices into elements (if `K = P`) or slices (if `K &lt; P`) along the `K`th</span>
<span class="sd">    dimension of `ref`.</span>

<span class="sd">    `updates` is `Tensor` of rank `Q-1+P-K` with shape:</span>

<span class="sd">    ```</span>
<span class="sd">    [d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]].</span>
<span class="sd">    ```</span>

<span class="sd">    For example, say we want to add 4 scattered elements to a rank-1 tensor to</span>
<span class="sd">    8 elements. In Python, that update would look like this:</span>

<span class="sd">    ```python</span>
<span class="sd">        ref = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])</span>
<span class="sd">        indices = tf.constant([[4], [3], [1] ,[7]])</span>
<span class="sd">        updates = tf.constant([9, 10, 11, 12])</span>
<span class="sd">        op = ref.scatter_nd_update(indices, updates)</span>
<span class="sd">        with tf.compat.v1.Session() as sess:</span>
<span class="sd">          print sess.run(op)</span>
<span class="sd">    ```</span>

<span class="sd">    The resulting update to ref would look like this:</span>

<span class="sd">        [1, 11, 3, 10, 9, 6, 7, 12]</span>

<span class="sd">    See `tf.scatter_nd` for more details about how to make updates to</span>
<span class="sd">    slices.</span>

<span class="sd">    Args:</span>
<span class="sd">      indices: The indices to be used in the operation.</span>
<span class="sd">      updates: The values to be used in the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered assignment has completed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">gen_state_ops</span><span class="o">.</span><span class="n">scatter_nd_update</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_variable</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_strided_slice_assign</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">begin_mask</span><span class="p">,</span>
                            <span class="n">end_mask</span><span class="p">,</span> <span class="n">ellipsis_mask</span><span class="p">,</span> <span class="n">new_axis_mask</span><span class="p">,</span>
                            <span class="n">shrink_axis_mask</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">strided_slice_assign</span><span class="p">(</span>
        <span class="n">ref</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_ref</span><span class="p">(),</span>
        <span class="n">begin</span><span class="o">=</span><span class="n">begin</span><span class="p">,</span>
        <span class="n">end</span><span class="o">=</span><span class="n">end</span><span class="p">,</span>
        <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">,</span>
        <span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">begin_mask</span><span class="o">=</span><span class="n">begin_mask</span><span class="p">,</span>
        <span class="n">end_mask</span><span class="o">=</span><span class="n">end_mask</span><span class="p">,</span>
        <span class="n">ellipsis_mask</span><span class="o">=</span><span class="n">ellipsis_mask</span><span class="p">,</span>
        <span class="n">new_axis_mask</span><span class="o">=</span><span class="n">new_axis_mask</span><span class="p">,</span>
        <span class="n">shrink_axis_mask</span><span class="o">=</span><span class="n">shrink_axis_mask</span><span class="p">)</span>

  <span class="nd">@deprecated</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Prefer Dataset.range instead.&quot;</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">count_up_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">limit</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Increments this variable until it reaches `limit`.</span>

<span class="sd">    When that Op is run it tries to increment the variable by `1`. If</span>
<span class="sd">    incrementing the variable would bring it above `limit` then the Op raises</span>
<span class="sd">    the exception `OutOfRangeError`.</span>

<span class="sd">    If no error is raised, the Op outputs the value of the variable before</span>
<span class="sd">    the increment.</span>

<span class="sd">    This is essentially a shortcut for `count_up_to(self, limit)`.</span>

<span class="sd">    Args:</span>
<span class="sd">      limit: value at which incrementing the variable raises an error.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the variable value before the increment. If no</span>
<span class="sd">      other Op modifies this variable, the values produced will all be</span>
<span class="sd">      distinct.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">state_ops</span><span class="o">.</span><span class="n">count_up_to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_variable</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="n">limit</span><span class="p">)</span>

  <span class="c1"># Conversion to tensor.</span>
  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">_TensorConversionFunction</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
    <span class="sd">&quot;&quot;&quot;Utility function for converting a Variable to a Tensor.&quot;&quot;&quot;</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">name</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Incompatible type conversion requested to type &#39;</span><span class="si">%s</span><span class="s2">&#39; for variable &quot;</span>
          <span class="s2">&quot;of type &#39;</span><span class="si">%s</span><span class="s2">&#39;&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">as_ref</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">v</span><span class="o">.</span><span class="n">_ref</span><span class="p">()</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">v</span><span class="o">.</span><span class="n">value</span><span class="p">()</span>

  <span class="c1"># NOTE(mrry): This enables the Variable&#39;s overloaded &quot;right&quot; binary</span>
  <span class="c1"># operators to run when the left operand is an ndarray, because it</span>
  <span class="c1"># accords the Variable class higher priority than an ndarray, or a</span>
  <span class="c1"># numpy matrix.</span>
  <span class="c1"># TODO(mrry): Convert this to using numpy&#39;s __numpy_ufunc__</span>
  <span class="c1"># mechanism, which allows more control over how Variables interact</span>
  <span class="c1"># with ndarrays.</span>
  <span class="n">__array_priority__</span> <span class="o">=</span> <span class="mi">100</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The name of this variable.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">initializer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The initializer operation for this variable.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializer_op</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The device of this variable.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_variable</span><span class="o">.</span><span class="n">device</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The `DType` of this variable.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_variable</span><span class="o">.</span><span class="n">dtype</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">op</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The `Operation` of this variable.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_variable</span><span class="o">.</span><span class="n">op</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">graph</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The `Graph` of this variable.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_variable</span><span class="o">.</span><span class="n">graph</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_distribute_strategy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The `tf.distribute.Strategy` that this variable was created under.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="kc">None</span>  <span class="c1"># Ref variables are never created inside a strategy.</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The `TensorShape` of this variable.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `TensorShape`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_variable</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">to_proto</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">export_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Converts a `Variable` to a `VariableDef` protocol buffer.</span>

<span class="sd">    Args:</span>
<span class="sd">      export_scope: Optional `string`. Name scope to remove.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `VariableDef` protocol buffer, or `None` if the `Variable` is not</span>
<span class="sd">      in the specified name scope.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">export_scope</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_variable</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">export_scope</span><span class="p">)):</span>
      <span class="n">var_def</span> <span class="o">=</span> <span class="n">variable_pb2</span><span class="o">.</span><span class="n">VariableDef</span><span class="p">()</span>
      <span class="n">var_def</span><span class="o">.</span><span class="n">variable_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">strip_name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_variable</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                                                   <span class="n">export_scope</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initial_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># For backwards compatibility.</span>
        <span class="n">var_def</span><span class="o">.</span><span class="n">initial_value_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">strip_name_scope</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_initial_value</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">export_scope</span><span class="p">)</span>
      <span class="n">var_def</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span>
      <span class="n">var_def</span><span class="o">.</span><span class="n">synchronization</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">synchronization</span><span class="o">.</span><span class="n">value</span>
      <span class="n">var_def</span><span class="o">.</span><span class="n">aggregation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">aggregation</span><span class="o">.</span><span class="n">value</span>
      <span class="n">var_def</span><span class="o">.</span><span class="n">initializer_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">strip_name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">initializer</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                                                      <span class="n">export_scope</span><span class="p">)</span>
      <span class="n">var_def</span><span class="o">.</span><span class="n">snapshot_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">strip_name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_snapshot</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                                                   <span class="n">export_scope</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_save_slice_info</span><span class="p">:</span>
        <span class="n">var_def</span><span class="o">.</span><span class="n">save_slice_info_def</span><span class="o">.</span><span class="n">MergeFrom</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_save_slice_info</span><span class="o">.</span><span class="n">to_proto</span><span class="p">(</span><span class="n">export_scope</span><span class="o">=</span><span class="n">export_scope</span><span class="p">))</span>
      <span class="k">return</span> <span class="n">var_def</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">None</span>

  <span class="k">def</span> <span class="fm">__iadd__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">log_first_n</span><span class="p">(</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">WARN</span><span class="p">,</span> <span class="s2">&quot;Variable += will be deprecated. Use variable.assign_add&quot;</span>
        <span class="s2">&quot; if you want assignment to the variable value or &#39;x = x + y&#39;&quot;</span>
        <span class="s2">&quot; if you want a new python Tensor object.&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span> <span class="o">+</span> <span class="n">other</span>

  <span class="k">def</span> <span class="fm">__isub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">log_first_n</span><span class="p">(</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">WARN</span><span class="p">,</span> <span class="s2">&quot;Variable -= will be deprecated. Use variable.assign_sub&quot;</span>
        <span class="s2">&quot; if you want assignment to the variable value or &#39;x = x - y&#39;&quot;</span>
        <span class="s2">&quot; if you want a new python Tensor object.&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span> <span class="o">-</span> <span class="n">other</span>

  <span class="k">def</span> <span class="fm">__imul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">log_first_n</span><span class="p">(</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">WARN</span><span class="p">,</span>
        <span class="s2">&quot;Variable *= will be deprecated. Use `var.assign(var * other)`&quot;</span>
        <span class="s2">&quot; if you want assignment to the variable value or `x = x * y`&quot;</span>
        <span class="s2">&quot; if you want a new python Tensor object.&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span> <span class="o">*</span> <span class="n">other</span>

  <span class="k">def</span> <span class="nf">__idiv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">log_first_n</span><span class="p">(</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">WARN</span><span class="p">,</span>
        <span class="s2">&quot;Variable /= will be deprecated. Use `var.assign(var / other)`&quot;</span>
        <span class="s2">&quot; if you want assignment to the variable value or `x = x / y`&quot;</span>
        <span class="s2">&quot; if you want a new python Tensor object.&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span> <span class="o">/</span> <span class="n">other</span>

  <span class="k">def</span> <span class="fm">__itruediv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">log_first_n</span><span class="p">(</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">WARN</span><span class="p">,</span>
        <span class="s2">&quot;Variable /= will be deprecated. Use `var.assign(var / other)`&quot;</span>
        <span class="s2">&quot; if you want assignment to the variable value or `x = x / y`&quot;</span>
        <span class="s2">&quot; if you want a new python Tensor object.&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span> <span class="o">/</span> <span class="n">other</span>

  <span class="k">def</span> <span class="nf">__irealdiv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">log_first_n</span><span class="p">(</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">WARN</span><span class="p">,</span>
        <span class="s2">&quot;Variable /= will be deprecated. Use `var.assign(var / other)`&quot;</span>
        <span class="s2">&quot; if you want assignment to the variable value or `x = x / y`&quot;</span>
        <span class="s2">&quot; if you want a new python Tensor object.&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span> <span class="o">/</span> <span class="n">other</span>

  <span class="k">def</span> <span class="fm">__ipow__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">log_first_n</span><span class="p">(</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">WARN</span><span class="p">,</span>
        <span class="s2">&quot;Variable **= will be deprecated. Use `var.assign(var ** other)`&quot;</span>
        <span class="s2">&quot; if you want assignment to the variable value or `x = x ** y`&quot;</span>
        <span class="s2">&quot; if you want a new python Tensor object.&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">**</span><span class="n">other</span>


<span class="k">def</span> <span class="nf">_try_guard_against_uninitialized_dependencies</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">initial_value</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Attempt to guard against dependencies on uninitialized variables.</span>

<span class="sd">  Replace references to variables in `initial_value` with references to the</span>
<span class="sd">  variable&#39;s initialized values. The initialized values are essentially</span>
<span class="sd">  conditional TensorFlow graphs that return a variable&#39;s value if it is</span>
<span class="sd">  initialized or its `initial_value` if it hasn&#39;t been initialized. This</span>
<span class="sd">  replacement is done on a best effort basis:</span>

<span class="sd">  - If the `initial_value` graph contains cycles, we don&#39;t do any</span>
<span class="sd">    replacements for that graph.</span>
<span class="sd">  - If the variables that `initial_value` depends on are not present in the</span>
<span class="sd">    `GLOBAL_VARIABLES` or `LOCAL_VARIABLES` we don&#39;t replace them.</span>

<span class="sd">  In these cases, it is up to the caller to ensure that the `initial_value`</span>
<span class="sd">  graph uses initialized variables or that they guard access to variables</span>
<span class="sd">  using their `initialized_value` method.</span>

<span class="sd">  Args:</span>
<span class="sd">    name: Variable name.</span>
<span class="sd">    initial_value: `Tensor`. The initial value.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` suitable to initialize a variable.</span>
<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If `initial_value` is not a `Tensor`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initial_value</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;initial_value needs to be a Tensor: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">initial_value</span><span class="p">)</span>

  <span class="c1"># Don&#39;t modify initial_value if it contains any cyclic dependencies.</span>
  <span class="k">if</span> <span class="n">_has_cycle</span><span class="p">(</span><span class="n">initial_value</span><span class="o">.</span><span class="n">op</span><span class="p">,</span> <span class="n">state</span><span class="o">=</span><span class="p">{}):</span>
    <span class="k">return</span> <span class="n">initial_value</span>
  <span class="k">return</span> <span class="n">_safe_initial_value_from_tensor</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">initial_value</span><span class="p">,</span> <span class="n">op_cache</span><span class="o">=</span><span class="p">{})</span>


<span class="n">_UNKNOWN</span><span class="p">,</span> <span class="n">_STARTED</span><span class="p">,</span> <span class="n">_FINISHED</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_has_cycle</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Detect cycles in the dependencies of `initial_value`.&quot;&quot;&quot;</span>
  <span class="n">op_state</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">_UNKNOWN</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">op_state</span> <span class="o">==</span> <span class="n">_STARTED</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">True</span>
  <span class="k">elif</span> <span class="n">op_state</span> <span class="o">==</span> <span class="n">_FINISHED</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">False</span>

  <span class="n">state</span><span class="p">[</span><span class="n">op</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">_STARTED</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">((</span><span class="n">i</span><span class="o">.</span><span class="n">op</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">),</span> <span class="n">op</span><span class="o">.</span><span class="n">control_inputs</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">_has_cycle</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
      <span class="k">return</span> <span class="kc">True</span>
  <span class="n">state</span><span class="p">[</span><span class="n">op</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">_FINISHED</span>
  <span class="k">return</span> <span class="kc">False</span>


<span class="k">def</span> <span class="nf">_safe_initial_value_from_tensor</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">op_cache</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Replace dependencies on variables with their initialized values.</span>

<span class="sd">  Args:</span>
<span class="sd">    name: Variable name.</span>
<span class="sd">    tensor: A `Tensor`. The tensor to replace.</span>
<span class="sd">    op_cache: A dict mapping operation names to `Operation`s. Used to memoize</span>
<span class="sd">      the results so as to avoid creating redundant operations.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` compatible with `tensor`. Any inputs that lead to variable</span>
<span class="sd">    values will be replaced with a corresponding graph that uses the</span>
<span class="sd">    variable&#39;s initialized values. This is done on a best-effort basis. If no</span>
<span class="sd">    modifications need to be made then `tensor` will be returned unchanged.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">op</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">op</span>
  <span class="n">new_op</span> <span class="o">=</span> <span class="n">op_cache</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">new_op</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">new_op</span> <span class="o">=</span> <span class="n">_safe_initial_value_from_op</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">op_cache</span><span class="p">)</span>
    <span class="n">op_cache</span><span class="p">[</span><span class="n">op</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_op</span>
  <span class="k">return</span> <span class="n">new_op</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="n">tensor</span><span class="o">.</span><span class="n">value_index</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">_safe_initial_value_from_op</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">op_cache</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Replace dependencies on variables with their initialized values.</span>

<span class="sd">  Args:</span>
<span class="sd">    name: Variable name.</span>
<span class="sd">    op: An `Operation`. The operation to replace.</span>
<span class="sd">    op_cache: A dict mapping operation names to `Operation`s. Used to memoize</span>
<span class="sd">      the results so as to avoid creating redundant operations.</span>

<span class="sd">  Returns:</span>
<span class="sd">    An `Operation` compatible with `op`. Any inputs that lead to variable</span>
<span class="sd">    values will be replaced with a corresponding graph that uses the</span>
<span class="sd">    variable&#39;s initialized values. This is done on a best-effort basis. If no</span>
<span class="sd">    modifications need to be made then `op` will be returned unchanged.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">op_type</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">node_def</span><span class="o">.</span><span class="n">op</span>
  <span class="k">if</span> <span class="n">op_type</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;IsVariableInitialized&quot;</span><span class="p">,</span> <span class="s2">&quot;VarIsInitializedOp&quot;</span><span class="p">,</span>
                 <span class="s2">&quot;ReadVariableOp&quot;</span><span class="p">,</span> <span class="s2">&quot;If&quot;</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">op</span>

  <span class="c1"># Attempt to find the initialized_value of any variable reference / handles.</span>
  <span class="c1"># TODO(b/70206927): Fix handling of ResourceVariables.</span>
  <span class="k">if</span> <span class="n">op_type</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;Variable&quot;</span><span class="p">,</span> <span class="s2">&quot;VariableV2&quot;</span><span class="p">,</span> <span class="s2">&quot;VarHandleOp&quot;</span><span class="p">):</span>
    <span class="n">initialized_value</span> <span class="o">=</span> <span class="n">_find_initialized_value_for_variable</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">op</span> <span class="k">if</span> <span class="n">initialized_value</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">initialized_value</span><span class="o">.</span><span class="n">op</span>

  <span class="c1"># Recursively build initializer expressions for inputs.</span>
  <span class="n">modified</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">new_op_inputs</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">op_input</span> <span class="ow">in</span> <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">:</span>
    <span class="n">new_op_input</span> <span class="o">=</span> <span class="n">_safe_initial_value_from_tensor</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">op_input</span><span class="p">,</span> <span class="n">op_cache</span><span class="p">)</span>
    <span class="n">new_op_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_op_input</span><span class="p">)</span>
    <span class="n">modified</span> <span class="o">=</span> <span class="n">modified</span> <span class="ow">or</span> <span class="p">(</span><span class="n">new_op_input</span> <span class="o">!=</span> <span class="n">op_input</span><span class="p">)</span>

  <span class="c1"># If at least one input was modified, replace the op.</span>
  <span class="k">if</span> <span class="n">modified</span><span class="p">:</span>
    <span class="n">new_op_type</span> <span class="o">=</span> <span class="n">op_type</span>
    <span class="k">if</span> <span class="n">new_op_type</span> <span class="o">==</span> <span class="s2">&quot;RefSwitch&quot;</span><span class="p">:</span>
      <span class="n">new_op_type</span> <span class="o">=</span> <span class="s2">&quot;Switch&quot;</span>
    <span class="n">new_op_name</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">node_def</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;_&quot;</span> <span class="o">+</span> <span class="n">name</span>
    <span class="n">new_op_name</span> <span class="o">=</span> <span class="n">new_op_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;:&quot;</span><span class="p">,</span> <span class="s2">&quot;_&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">op</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">create_op</span><span class="p">(</span>
        <span class="n">new_op_type</span><span class="p">,</span>
        <span class="n">new_op_inputs</span><span class="p">,</span>
        <span class="n">op</span><span class="o">.</span><span class="n">_output_types</span><span class="p">,</span>  <span class="c1"># pylint: disable=protected-access</span>
        <span class="n">name</span><span class="o">=</span><span class="n">new_op_name</span><span class="p">,</span>
        <span class="n">attrs</span><span class="o">=</span><span class="n">op</span><span class="o">.</span><span class="n">node_def</span><span class="o">.</span><span class="n">attr</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">op</span>


<span class="k">def</span> <span class="nf">_find_initialized_value_for_variable</span><span class="p">(</span><span class="n">variable_op</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Find the initialized value for a variable op.</span>

<span class="sd">  To do so, lookup the variable op in the variables collection.</span>

<span class="sd">  Args:</span>
<span class="sd">    variable_op: A variable `Operation`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` representing the initialized value for the variable or `None`</span>
<span class="sd">    if the initialized value could not be found.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">var_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">variable_op</span><span class="o">.</span><span class="n">node_def</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">variable_op</span><span class="o">.</span><span class="n">node_def</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;:0&quot;</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">collection_name</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">,</span>
                            <span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">LOCAL_VARIABLES</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">variable_op</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">collection_name</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">var</span><span class="o">.</span><span class="n">name</span> <span class="ow">in</span> <span class="n">var_names</span><span class="p">:</span>
          <span class="k">return</span> <span class="n">var</span><span class="o">.</span><span class="n">initialized_value</span><span class="p">()</span>
  <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
    <span class="c1"># Return None when an incomplete user-defined variable type was put in</span>
    <span class="c1"># the collection.</span>
    <span class="k">return</span> <span class="kc">None</span>
  <span class="k">return</span> <span class="kc">None</span>


<span class="k">class</span> <span class="nc">PartitionedVariable</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A container for partitioned `Variable` objects.</span>

<span class="sd">  @compatibility(eager) `tf.PartitionedVariable` is not compatible with</span>
<span class="sd">  eager execution.  Use `tf.Variable` instead which is compatible</span>
<span class="sd">  with both eager execution and graph construction.  See [the</span>
<span class="sd">  TensorFlow Eager Execution</span>
<span class="sd">  guide](https://www.tensorflow.org/guide/eager#variables_and_optimizers)</span>
<span class="sd">  for details on how variables work in eager execution.</span>
<span class="sd">  @end_compatibility</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">variable_list</span><span class="p">,</span> <span class="n">partitions</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a new partitioned variable wrapper.</span>

<span class="sd">    Variables passed via the variable_list must contain a save_slice_info</span>
<span class="sd">    field.  Concatenation and iteration is in lexicographic order according</span>
<span class="sd">    to the var_offset property of the save_slice_info.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: String. Overall name of the variables.</span>
<span class="sd">      shape: List of integers.  Overall shape of the variables.</span>
<span class="sd">      dtype: Type of the variables.</span>
<span class="sd">      variable_list: List of `Variable` that comprise this partitioned variable.</span>
<span class="sd">      partitions: List of integers.  Number of partitions for each dimension.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: If `variable_list` is not a list of `Variable` objects, or</span>
<span class="sd">        `partitions` is not a list.</span>
<span class="sd">      ValueError: If `variable_list` is empty, or the `Variable` shape</span>
<span class="sd">        information does not match `shape`, or `partitions` has invalid values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">variable_list</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;variable_list is not a list or tuple: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
                      <span class="n">variable_list</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">partitions</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;partitions is not a list or tuple: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">partitions</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">p</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">partitions</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;partition values must be positive: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">partitions</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">variable_list</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;variable_list may not be empty&quot;</span><span class="p">)</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">variable_list</span><span class="p">:</span>
      <span class="c1"># Sort the variable_list lexicographically according to var offset value.</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">_get_save_slice_info</span><span class="p">()</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">variable_list</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;All variables must have a save_slice_info available: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
            <span class="p">[</span><span class="n">v</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">variable_list</span><span class="p">])</span>
      <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">partitions</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;len(shape) != len(partitions): </span><span class="si">%s</span><span class="s2"> vs. </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
                         <span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">partitions</span><span class="p">))</span>
      <span class="k">if</span> <span class="n">v</span><span class="o">.</span><span class="n">_get_save_slice_info</span><span class="p">()</span><span class="o">.</span><span class="n">full_shape</span> <span class="o">!=</span> <span class="n">shape</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;All variables&#39; full shapes must match shape: </span><span class="si">%s</span><span class="s2">; &quot;</span>
                         <span class="s2">&quot;but full shapes were: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
                         <span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="nb">str</span><span class="p">([</span><span class="n">v</span><span class="o">.</span><span class="n">_get_save_slice_info</span><span class="p">()</span><span class="o">.</span><span class="n">full_shape</span><span class="p">])))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_variable_list</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span>
        <span class="n">variable_list</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">_get_save_slice_info</span><span class="p">()</span><span class="o">.</span><span class="n">var_offset</span><span class="p">)</span>
    <span class="c1"># pylint: enable=protected-access</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">name</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span> <span class="o">=</span> <span class="n">shape</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="o">=</span> <span class="n">dtype</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_partitions</span> <span class="o">=</span> <span class="n">partitions</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_as_tensor</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return an iterable for accessing the underlying partition Variables.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_variable_list</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">num_partition_axes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_partition_axes</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">num_partition_axes</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot get a length for </span><span class="si">%d</span><span class="s2"> &gt; 1 partition axes&quot;</span> <span class="o">%</span>
                       <span class="n">num_partition_axes</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_variable_list</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_partition_axes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">p</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partitions</span><span class="p">):</span>
      <span class="k">return</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitions</span><span class="p">)</span> <span class="k">if</span> <span class="n">p</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">]</span>

  <span class="k">def</span> <span class="nf">_concat</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the overall concatenated value as a `Tensor`.</span>

<span class="sd">    This is different from using the partitioned variable directly as a tensor</span>
<span class="sd">    (through tensor conversion and `as_tensor`) in that it creates a new set of</span>
<span class="sd">    operations that keeps the control dependencies from its scope.</span>

<span class="sd">    Returns:</span>
<span class="sd">      `Tensor` containing the concatenated value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_variable_list</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_variable_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">)</span>

    <span class="n">partition_axes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partition_axes</span><span class="p">()</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">partition_axes</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
          <span class="s2">&quot;Cannot concatenate along more than one dimension: </span><span class="si">%s</span><span class="s2">.  &quot;</span>
          <span class="s2">&quot;Multi-axis partition concat is not supported&quot;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">partition_axes</span><span class="p">))</span>
    <span class="n">partition_ix</span> <span class="o">=</span> <span class="n">partition_axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">+</span> <span class="s2">&quot;/ConcatPartitions/&quot;</span><span class="p">):</span>
      <span class="n">concatenated</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_variable_list</span><span class="p">,</span> <span class="n">partition_ix</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="kc">None</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">concatenated</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">as_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the overall concatenated value as a `Tensor`.</span>

<span class="sd">    The returned tensor will not inherit the control dependencies from the scope</span>
<span class="sd">    where the value is used, which is similar to getting the value of</span>
<span class="sd">    `Variable`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      `Tensor` containing the concatenated value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">(</span><span class="kc">None</span><span class="p">):</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_concat</span><span class="p">()</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">_TensorConversionFunction</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># pylint: disable=invalid-name</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">name</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Incompatible type conversion requested to type &#39;</span><span class="si">%s</span><span class="s2">&#39; for variable &quot;</span>
          <span class="s2">&quot;of type &#39;</span><span class="si">%s</span><span class="s2">&#39;&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">as_ref</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
          <span class="s2">&quot;PartitionedVariable doesn&#39;t support being used as a reference.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">v</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">()</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_distribute_strategy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The `tf.distribute.Strategy` that this variable was created under.&quot;&quot;&quot;</span>
    <span class="c1"># NOTE(yuefengz): Today, no partitioned variables in a distribute strategy.</span>
    <span class="k">return</span> <span class="kc">None</span>

  <span class="k">def</span> <span class="nf">get_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span>

  <span class="k">def</span> <span class="nf">_get_variable_list</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_variable_list</span>

  <span class="k">def</span> <span class="nf">_get_partitions</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partitions</span>

  <span class="k">def</span> <span class="nf">_apply_assign_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">assign_fn</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="n">partition_axes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partition_axes</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">partition_axes</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
          <span class="s2">&quot;Cannot do assign action along more than one dimension: </span><span class="si">%s</span><span class="s2">.  &quot;</span>
          <span class="s2">&quot;Multi-axis partition assign action is not supported &quot;</span> <span class="o">%</span>
          <span class="nb">str</span><span class="p">(</span><span class="n">partition_axes</span><span class="p">))</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
      <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_variable_list</span><span class="p">)</span>
      <span class="n">value_list</span> <span class="o">=</span> <span class="n">value</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">PartitionedVariable</span><span class="p">):</span>
      <span class="n">value_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">var_part</span> <span class="k">for</span> <span class="n">var_part</span> <span class="ow">in</span> <span class="n">value</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">partition_ix</span> <span class="o">=</span> <span class="n">partition_axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">size_splits_list</span> <span class="o">=</span> <span class="p">[</span>
          <span class="n">tensor_shape</span><span class="o">.</span><span class="n">dimension_value</span><span class="p">(</span><span class="n">var</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">partition_ix</span><span class="p">])</span>
          <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_variable_list</span>
      <span class="p">]</span>
      <span class="n">value_list</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">size_splits_list</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">partition_ix</span><span class="p">)</span>

    <span class="n">op_list</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">assign_fn</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">value_list</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">var</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_variable_list</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="n">op_list</span>

  <span class="k">def</span> <span class="nf">assign</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">read_value</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">assign_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">var</span><span class="p">,</span> <span class="n">r_value</span><span class="p">:</span> <span class="n">var</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span>
        <span class="n">r_value</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="n">use_locking</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">read_value</span><span class="o">=</span><span class="n">read_value</span><span class="p">)</span>
    <span class="n">assign_list</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_assign_fn</span><span class="p">(</span><span class="n">assign_fn</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">read_value</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">assign_list</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">assign</span><span class="o">.</span><span class="n">op</span> <span class="k">for</span> <span class="n">assign</span> <span class="ow">in</span> <span class="n">assign_list</span><span class="p">]</span>

  <span class="k">def</span> <span class="nf">assign_add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">read_value</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">assign_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">var</span><span class="p">,</span> <span class="n">r_value</span><span class="p">:</span> <span class="n">var</span><span class="o">.</span><span class="n">assign_add</span><span class="p">(</span>
        <span class="n">r_value</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="n">use_locking</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">read_value</span><span class="o">=</span><span class="n">read_value</span><span class="p">)</span>
    <span class="n">assign_list</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_assign_fn</span><span class="p">(</span><span class="n">assign_fn</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">read_value</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">assign_list</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">assign</span><span class="o">.</span><span class="n">op</span> <span class="k">for</span> <span class="n">assign</span> <span class="ow">in</span> <span class="n">assign_list</span><span class="p">]</span>

  <span class="k">def</span> <span class="nf">assign_sub</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">read_value</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">assign_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">var</span><span class="p">,</span> <span class="n">r_value</span><span class="p">:</span> <span class="n">var</span><span class="o">.</span><span class="n">assign_sub</span><span class="p">(</span>
        <span class="n">r_value</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="n">use_locking</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">read_value</span><span class="o">=</span><span class="n">read_value</span><span class="p">)</span>
    <span class="n">assign_list</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_assign_fn</span><span class="p">(</span><span class="n">assign_fn</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">read_value</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">assign_list</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">assign</span><span class="o">.</span><span class="n">op</span> <span class="k">for</span> <span class="n">assign</span> <span class="ow">in</span> <span class="n">assign_list</span><span class="p">]</span>


<span class="c1"># Register a conversion function which reads the value of the variable,</span>
<span class="c1"># allowing instances of the class to be used as tensors.</span>
<span class="n">ops</span><span class="o">.</span><span class="n">register_tensor_conversion_function</span><span class="p">(</span><span class="n">RefVariable</span><span class="p">,</span>
                                        <span class="n">RefVariable</span><span class="o">.</span><span class="n">_TensorConversionFunction</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
<span class="n">ops</span><span class="o">.</span><span class="n">register_dense_tensor_like_type</span><span class="p">(</span><span class="n">RefVariable</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;global_variables&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">global_variables</span><span class="p">(</span><span class="n">scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns global variables.</span>

<span class="sd">  Global variables are variables that are shared across machines in a</span>
<span class="sd">  distributed environment. The `Variable()` constructor or `get_variable()`</span>
<span class="sd">  automatically adds new variables to the graph collection</span>
<span class="sd">  `GraphKeys.GLOBAL_VARIABLES`.</span>
<span class="sd">  This convenience function returns the contents of that collection.</span>

<span class="sd">  An alternative to global variables are local variables. See</span>
<span class="sd">  `tf.compat.v1.local_variables`</span>

<span class="sd">  Args:</span>
<span class="sd">    scope: (Optional.) A string. If supplied, the resulting list is filtered to</span>
<span class="sd">      include only items whose `name` attribute matches `scope` using</span>
<span class="sd">      `re.match`. Items without a `name` attribute are never returned if a scope</span>
<span class="sd">      is supplied. The choice of `re.match` means that a `scope` without special</span>
<span class="sd">      tokens filters by prefix.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of `Variable` objects.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;all_variables&quot;</span><span class="p">])</span>
<span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;2017-03-02&quot;</span><span class="p">,</span> <span class="s2">&quot;Please use tf.global_variables instead.&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">all_variables</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Use `tf.compat.v1.global_variables` instead.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">global_variables</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_all_saveable_objects</span><span class="p">(</span><span class="n">scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns all variables and `SaveableObject`s that must be checkpointed.</span>

<span class="sd">  Args:</span>
<span class="sd">    scope: (Optional.) A string. If supplied, the resulting list is filtered to</span>
<span class="sd">      include only items whose `name` attribute matches `scope` using</span>
<span class="sd">      `re.match`. Items without a `name` attribute are never returned if a scope</span>
<span class="sd">      is supplied. The choice of `re.match` means that a `scope` without special</span>
<span class="sd">      tokens filters by prefix.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of `Variable` and `SaveableObject` to be checkpointed</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># TODO(andreasst): make this function public once things are settled.</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="p">)</span> <span class="o">+</span>
          <span class="n">ops</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">SAVEABLE_OBJECTS</span><span class="p">,</span> <span class="n">scope</span><span class="p">))</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;local_variables&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">local_variables</span><span class="p">(</span><span class="n">scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns local variables.</span>

<span class="sd">  Local variables - per process variables, usually not saved/restored to</span>
<span class="sd">  checkpoint and used for temporary or intermediate values.</span>
<span class="sd">  For example, they can be used as counters for metrics computation or</span>
<span class="sd">  number of epochs this machine has read data.</span>
<span class="sd">  The `tf.contrib.framework.local_variable()` function automatically adds the</span>
<span class="sd">  new variable to `GraphKeys.LOCAL_VARIABLES`.</span>
<span class="sd">  This convenience function returns the contents of that collection.</span>

<span class="sd">  An alternative to local variables are global variables. See</span>
<span class="sd">  `tf.compat.v1.global_variables`</span>

<span class="sd">  Args:</span>
<span class="sd">    scope: (Optional.) A string. If supplied, the resulting list is filtered to</span>
<span class="sd">      include only items whose `name` attribute matches `scope` using</span>
<span class="sd">      `re.match`. Items without a `name` attribute are never returned if a scope</span>
<span class="sd">      is supplied. The choice of `re.match` means that a `scope` without special</span>
<span class="sd">      tokens filters by prefix.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of local `Variable` objects.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">LOCAL_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;model_variables&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">model_variables</span><span class="p">(</span><span class="n">scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns all variables in the MODEL_VARIABLES collection.</span>

<span class="sd">  Args:</span>
<span class="sd">    scope: (Optional.) A string. If supplied, the resulting list is filtered to</span>
<span class="sd">      include only items whose `name` attribute matches `scope` using</span>
<span class="sd">      `re.match`. Items without a `name` attribute are never returned if a scope</span>
<span class="sd">      is supplied. The choice of `re.match` means that a `scope` without special</span>
<span class="sd">      tokens filters by prefix.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of local Variable objects.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">MODEL_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;trainable_variables&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">trainable_variables</span><span class="p">(</span><span class="n">scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns all variables created with `trainable=True`.</span>

<span class="sd">  When passed `trainable=True`, the `Variable()` constructor automatically</span>
<span class="sd">  adds new variables to the graph collection</span>
<span class="sd">  `GraphKeys.TRAINABLE_VARIABLES`. This convenience function returns the</span>
<span class="sd">  contents of that collection.</span>

<span class="sd">  Args:</span>
<span class="sd">    scope: (Optional.) A string. If supplied, the resulting list is filtered to</span>
<span class="sd">      include only items whose `name` attribute matches `scope` using</span>
<span class="sd">      `re.match`. Items without a `name` attribute are never returned if a scope</span>
<span class="sd">      is supplied. The choice of `re.match` means that a `scope` without special</span>
<span class="sd">      tokens filters by prefix.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of Variable objects.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;moving_average_variables&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">moving_average_variables</span><span class="p">(</span><span class="n">scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns all variables that maintain their moving averages.</span>

<span class="sd">  If an `ExponentialMovingAverage` object is created and the `apply()`</span>
<span class="sd">  method is called on a list of variables, these variables will</span>
<span class="sd">  be added to the `GraphKeys.MOVING_AVERAGE_VARIABLES` collection.</span>
<span class="sd">  This convenience function returns the contents of that collection.</span>

<span class="sd">  Args:</span>
<span class="sd">    scope: (Optional.) A string. If supplied, the resulting list is filtered to</span>
<span class="sd">      include only items whose `name` attribute matches `scope` using</span>
<span class="sd">      `re.match`. Items without a `name` attribute are never returned if a scope</span>
<span class="sd">      is supplied. The choice of `re.match` means that a `scope` without special</span>
<span class="sd">      tokens filters by prefix.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of Variable objects.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">MOVING_AVERAGE_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;initializers.variables&quot;</span><span class="p">,</span> <span class="s2">&quot;variables_initializer&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">variables_initializer</span><span class="p">(</span><span class="n">var_list</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;init&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns an Op that initializes a list of variables.</span>

<span class="sd">  After you launch the graph in a session, you can run the returned Op to</span>
<span class="sd">  initialize all the variables in `var_list`. This Op runs all the</span>
<span class="sd">  initializers of the variables in `var_list` in parallel.</span>

<span class="sd">  Calling `initialize_variables()` is equivalent to passing the list of</span>
<span class="sd">  initializers to `Group()`.</span>

<span class="sd">  If `var_list` is empty, however, the function still returns an Op that can</span>
<span class="sd">  be run. That Op just has no effect.</span>

<span class="sd">  Args:</span>
<span class="sd">    var_list: List of `Variable` objects to initialize.</span>
<span class="sd">    name: Optional name for the returned operation.</span>

<span class="sd">  Returns:</span>
<span class="sd">    An Op that run the initializers of all the specified variables.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">var_list</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">control_flow_ops</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">v</span><span class="o">.</span><span class="n">initializer</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">var_list</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">control_flow_ops</span><span class="o">.</span><span class="n">no_op</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;initialize_variables&quot;</span><span class="p">])</span>
<span class="nd">@tf_should_use</span><span class="o">.</span><span class="n">should_use_result</span>
<span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;2017-03-02&quot;</span><span class="p">,</span> <span class="s2">&quot;Use `tf.variables_initializer` instead.&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">initialize_variables</span><span class="p">(</span><span class="n">var_list</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;init&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;See `tf.compat.v1.variables_initializer`.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">variables_initializer</span><span class="p">(</span><span class="n">var_list</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;initializers.global_variables&quot;</span><span class="p">,</span> <span class="s2">&quot;global_variables_initializer&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">global_variables_initializer</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Returns an Op that initializes global variables.</span>

<span class="sd">  This is just a shortcut for `variables_initializer(global_variables())`</span>

<span class="sd">  Returns:</span>
<span class="sd">    An Op that initializes global variables in the graph.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">control_flow_ops</span><span class="o">.</span><span class="n">no_op</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;global_variables_initializer&quot;</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">variables_initializer</span><span class="p">(</span><span class="n">global_variables</span><span class="p">())</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;initialize_all_variables&quot;</span><span class="p">])</span>
<span class="nd">@tf_should_use</span><span class="o">.</span><span class="n">should_use_result</span>
<span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;2017-03-02&quot;</span><span class="p">,</span> <span class="s2">&quot;Use `tf.global_variables_initializer` instead.&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">initialize_all_variables</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;See `tf.compat.v1.global_variables_initializer`.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">global_variables_initializer</span><span class="p">()</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;initializers.local_variables&quot;</span><span class="p">,</span> <span class="s2">&quot;local_variables_initializer&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">local_variables_initializer</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Returns an Op that initializes all local variables.</span>

<span class="sd">  This is just a shortcut for `variables_initializer(local_variables())`</span>

<span class="sd">  Returns:</span>
<span class="sd">    An Op that initializes all local variables in the graph.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">control_flow_ops</span><span class="o">.</span><span class="n">no_op</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;local_variables_initializer&quot;</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">variables_initializer</span><span class="p">(</span><span class="n">local_variables</span><span class="p">())</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;initialize_local_variables&quot;</span><span class="p">])</span>
<span class="nd">@tf_should_use</span><span class="o">.</span><span class="n">should_use_result</span>
<span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;2017-03-02&quot;</span><span class="p">,</span> <span class="s2">&quot;Use `tf.local_variables_initializer` instead.&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">initialize_local_variables</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;See `tf.compat.v1.local_variables_initializer`.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">local_variables_initializer</span><span class="p">()</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;is_variable_initialized&quot;</span><span class="p">])</span>
<span class="nd">@tf_should_use</span><span class="o">.</span><span class="n">should_use_result</span>
<span class="k">def</span> <span class="nf">is_variable_initialized</span><span class="p">(</span><span class="n">variable</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Tests if a variable has been initialized.</span>

<span class="sd">  Args:</span>
<span class="sd">    variable: A `Variable`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Returns a scalar boolean Tensor, `True` if the variable has been</span>
<span class="sd">    initialized, `False` otherwise.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">state_ops</span><span class="o">.</span><span class="n">is_variable_initialized</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;assert_variables_initialized&quot;</span><span class="p">])</span>
<span class="nd">@tf_should_use</span><span class="o">.</span><span class="n">should_use_result</span>
<span class="k">def</span> <span class="nf">assert_variables_initialized</span><span class="p">(</span><span class="n">var_list</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns an Op to check if variables are initialized.</span>

<span class="sd">  NOTE: This function is obsolete and will be removed in 6 months.  Please</span>
<span class="sd">  change your implementation to use `report_uninitialized_variables()`.</span>

<span class="sd">  When run, the returned Op will raise the exception `FailedPreconditionError`</span>
<span class="sd">  if any of the variables has not yet been initialized.</span>

<span class="sd">  Note: This function is implemented by trying to fetch the values of the</span>
<span class="sd">  variables. If one of the variables is not initialized a message may be</span>
<span class="sd">  logged by the C++ runtime. This is expected.</span>

<span class="sd">  Args:</span>
<span class="sd">    var_list: List of `Variable` objects to check. Defaults to the value of</span>
<span class="sd">      `global_variables().`</span>

<span class="sd">  Returns:</span>
<span class="sd">    An Op, or None if there are no variables.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">var_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">var_list</span> <span class="o">=</span> <span class="n">global_variables</span><span class="p">()</span> <span class="o">+</span> <span class="n">local_variables</span><span class="p">()</span>
  <span class="c1"># Backwards compatibility for old-style variables. TODO(touts): remove.</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">var_list</span><span class="p">:</span>
    <span class="n">var_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_operations</span><span class="p">():</span>
      <span class="k">if</span> <span class="n">op</span><span class="o">.</span><span class="n">type</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;Variable&quot;</span><span class="p">,</span> <span class="s2">&quot;VariableV2&quot;</span><span class="p">,</span> <span class="s2">&quot;AutoReloadVariable&quot;</span><span class="p">]:</span>
        <span class="n">var_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">var_list</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">None</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">ranks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">var_list</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">colocate_with</span><span class="p">(</span><span class="n">var</span><span class="o">.</span><span class="n">op</span><span class="p">):</span>
        <span class="n">ranks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">array_ops</span><span class="o">.</span><span class="n">rank_internal</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">ranks</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">ranks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">ranks</span><span class="p">)</span>


<span class="nd">@tf_export</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;report_uninitialized_variables&quot;</span><span class="p">])</span>
<span class="nd">@tf_should_use</span><span class="o">.</span><span class="n">should_use_result</span>
<span class="k">def</span> <span class="nf">report_uninitialized_variables</span><span class="p">(</span><span class="n">var_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                   <span class="n">name</span><span class="o">=</span><span class="s2">&quot;report_uninitialized_variables&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Adds ops to list the names of uninitialized variables.</span>

<span class="sd">  When run, it returns a 1-D tensor containing the names of uninitialized</span>
<span class="sd">  variables if there are any, or an empty array if there are none.</span>

<span class="sd">  Args:</span>
<span class="sd">    var_list: List of `Variable` objects to check. Defaults to the value of</span>
<span class="sd">      `global_variables() + local_variables()`</span>
<span class="sd">    name: Optional name of the `Operation`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A 1-D tensor containing names of the uninitialized variables, or an empty</span>
<span class="sd">    1-D tensor if there are no variables or no uninitialized variables.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">var_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">var_list</span> <span class="o">=</span> <span class="n">global_variables</span><span class="p">()</span> <span class="o">+</span> <span class="n">local_variables</span><span class="p">()</span>
    <span class="c1"># Backwards compatibility for old-style variables. TODO(touts): remove.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">var_list</span><span class="p">:</span>
      <span class="n">var_list</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_operations</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">op</span><span class="o">.</span><span class="n">type</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;Variable&quot;</span><span class="p">,</span> <span class="s2">&quot;VariableV2&quot;</span><span class="p">,</span> <span class="s2">&quot;AutoReloadVariable&quot;</span><span class="p">]:</span>
          <span class="n">var_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="c1"># Run all operations on CPU</span>
    <span class="k">if</span> <span class="n">var_list</span><span class="p">:</span>
      <span class="n">init_vars</span> <span class="o">=</span> <span class="p">[</span><span class="n">state_ops</span><span class="o">.</span><span class="n">is_variable_initialized</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">var_list</span><span class="p">]</span>
    <span class="n">local_device</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
        <span class="s2">&quot;TF_DEVICE_FOR_UNINITIALIZED_VARIABLE_REPORTING&quot;</span><span class="p">,</span> <span class="s2">&quot;/cpu:0&quot;</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">local_device</span><span class="p">):</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">var_list</span><span class="p">:</span>
        <span class="c1"># Return an empty tensor so we only need to check for returned tensor</span>
        <span class="c1"># size being 0 as an indication of model ready.</span>
        <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">constant</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">string</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Get a 1-D boolean tensor listing whether each variable is initialized.</span>
        <span class="n">variables_mask</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">logical_not</span><span class="p">(</span><span class="n">array_ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">init_vars</span><span class="p">))</span>
        <span class="c1"># Get a 1-D string tensor containing all the variable names.</span>
        <span class="n">variable_names_tensor</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
            <span class="p">[</span><span class="n">s</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">var_list</span><span class="p">])</span>
        <span class="c1"># Return a 1-D tensor containing all the names of</span>
        <span class="c1"># uninitialized variables.</span>
        <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">boolean_mask</span><span class="p">(</span><span class="n">variable_names_tensor</span><span class="p">,</span> <span class="n">variables_mask</span><span class="p">)</span>


<span class="n">ops</span><span class="o">.</span><span class="n">register_tensor_conversion_function</span><span class="p">(</span>
    <span class="n">PartitionedVariable</span><span class="p">,</span> <span class="n">PartitionedVariable</span><span class="o">.</span><span class="n">_TensorConversionFunction</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
</pre></div>

    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
      
    </p>
    <p>
        &copy; Copyright Copyright 2018, zfit.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 2.4.3.<br/>
    </p>
  </div>
</footer>
  </body>
</html>